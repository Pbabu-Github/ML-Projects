[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this bloghshjs"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Classifying Palmer Penguins",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Predicting a penguin’s species based on its physical measurements is an interesting challenge in data analysis and machine learning. In this blog, we’ll explore the Palmer Penguins dataset, which was collected by Dr. Kristen Gorman and the Palmer Station, to analyze biological measurements of three penguin species in Antarctica. Through data visualization and feature selection, we’ll uncover key insights and build a classification model that achieves perfect accuracy using a carefully chosen set of features. Along the way, we’ll discuss our findings and ensure a reproducible and insightful approach to species classification. ### Loading Data We will now use the Pandas function to read the CSV and also take a look at the data and the different variables and columns we have in our dataset before we do and data explorations and analysis.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Overfitting, Overparameterization, and Double Descent\n\n\n\n\n\nA blog post about Overfitting, Overparameterization, and Double Descent\n\n\n\n\n\nApr 20, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Logistic Regression\n\n\n\n\n\nA blog post about Implementing Logistic Regression.\n\n\n\n\n\nApr 8, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing Perceptron\n\n\n\n\n\nA blog post about Implementing Perceptron and some experiments with it.\n\n\n\n\n\nApr 2, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nDissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nA replication study of Obermeyer et al. (2019), exploring racial bias in healthcare cost algorithms and its impact on resource allocation.\n\n\n\n\n\nMar 2, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nA blog post about Design and Impact of Automated Decision Systems\n\n\n\n\n\nFeb 27, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nA blog post on Limits of the Quantitative Approach to Bias and Fairness.\n\n\n\n\n\nFeb 20, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Kernel Machines\n\n\n\n\n\nA blog post about ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍implementing a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍sparse ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍kernelized ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍logistic ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍regression\n\n\n\n\n\nFeb 20, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nA blog post about classifying penguins with machine learning using logistic Regression.\n\n\n\n\n\nFeb 20, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias in Machine Learning Models\n\n\n\n\n\nA blog post about auditing bias in machine learning models.\n\n\n\n\n\nFeb 20, 2025\n\n\nPB\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html#data-cleaning",
    "href": "posts/example-blog-post/index.html#data-cleaning",
    "title": "Classifying Palmer Penguins",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nOur dataset contains numerous variables, including comments and region information, that are not relevant for training our model. Since we will be focusing on variables such as island, culmen length, culmen depth, flipper length, and body mass, we will remove unnecessary columns before proceeding with data exploration, visualization, and model training. Let’s start by loading the necessary packages for data cleaning, visualization, and model training. We will also convert categorical feature columns like Sex and Island into one-hot encoded 0-1( or true or false) columns using the pd.get_dummies function. Additionally, we can see that Species is a categorical variable, but instead of one-hot encoding, we will use label encoding to convert it into numerical labels: 0 for Adelie, 1 for Chinstrap, and 2 for Gentoo. This transformation will make data visualization and model training much easier.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode categorical features and label\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Stage\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    # One-hot encode categorical variables\n    df =(pd.get_dummies(df)).astype(int)\n\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40\n16\n187\n3200\n9\n-24\n0\n1\n0\n0\n1\n1\n0\n\n\n1\n49\n19\n210\n3950\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n2\n50\n15\n218\n5700\n8\n-25\n1\n0\n0\n0\n1\n0\n1\n\n\n3\n45\n14\n210\n4200\n7\n-25\n1\n0\n0\n0\n1\n1\n0\n\n\n4\n51\n18\n203\n4100\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n\n\n\n\n\n\nVisualizing the data\nThe bar chart below shows the number of penguins of each species on different islands, helping us identify patterns in species distribution. Interestingly, we observe: - Torgersen Island: Only Adelie penguins are present, with a population of about 30-40 individuals. - Dream Island: Has both Adelie and Chinstrap penguins, with Chinstrap being the dominant species. - Biscoe Island: Has both Adelie and Gentoo penguins, with Gentoo having the largest population overall.\nChinstrap penguins are only found on Dream Island, while Gentoo penguins are exclusive to Biscoe Island. Adelie penguins, on the other hand, are the most widespread, appearing on all three islands. Based on this distribution, the island where a penguin is found could be a useful feature for predicting its species.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n# Map species labels back to names\nspecies_map = {0: 'Adelie', 1: 'Chinstrap', 2: 'Gentoo'}\nspecies_island_counts = X_train.copy()\nspecies_island_counts[\"Species\"] = [species_map[label] for label in y_train]\n\n# Convert one-hot encoded islands back to a single column\nspecies_island_counts[\"Island\"] = species_island_counts[[\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]].idxmax(axis=1)\nspecies_island_counts[\"Island\"] = species_island_counts[\"Island\"].str.replace(\"Island_\", \"\")\n\n# Count number of penguins per species per island\nplot_data = species_island_counts.groupby([\"Island\", \"Species\"]).size().reset_index(name=\"Count\")\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.barplot(data=plot_data, x=\"Island\", y=\"Count\", hue=\"Species\", palette=\"mako\")\nplt.xlabel(\"Island\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Penguin Species Distribution Across Islands\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, we can see that by simply looking at the islands, we can somewhat predict the species of the penguin. This is a good example of a feature that could help predict the species. However, the model would be much more efficient if each island only hosted one species of penguin. Since some islands have more than one species, we might want to explore additional features. For example, if a penguin is on Biscoe Island and has a certain Culmen length and Culmen depth, can we make more accurate predictions based on those measurements? To explore this, we will create a graph to assess whether including Culmen lenght and Culmen depth is a useful feature for predicting the species on each island.\n\n\n# Plotting the relationship between Culmen Length and Culmen Depth for each species on each island\nsns.relplot(data =train, hue=\"Species\", y = 'Culmen Depth (mm)', x =  'Culmen Length (mm)',col = 'Island')\n\n&lt;seaborn.axisgrid.FacetGrid at 0x149d2fc10&gt;\n\n\n\n\n\n\n\n\n\nThe graph above looks at the relationship between Culmen Depth (mm) and Culmen Length (mm) for different penguin species, separated by island. Each species is color-coded to highlight patterns in their beak dimensions. From this graph, we can observe: - Gentoo penguins (orange) tend to have longer culmen lengths and shallower culmen depths. - Adelie penguins (green) have moderate culmen lengths and a wide range of culmen depths. - Chinstrap penguins (blue) have longer culmen lenghts compared to Adelie penguins but exhibit similar culmen Depths. ## Can Culmen Features Help in Classification? Looking at the separability of species by island: - Biscoe Island: Gentoo and Adelie penguins are linearly separable, meaning a simple model could classify them effectively based on culmen features. - Dream Island: Chinstrap and Adelie penguins overlap in culmen depth, making them harder to separate linearly. However, their culmen lengths show some distinction, which could aid classification. - Torgersen Island: Since only Adelie penguins are found here, culmen features are irrelevant for classification on this island.\nThis suggests that culmen depth and length can be strong features for classifying species, especially when combined with island location. ## Exploring Culmen Features in relation to Flipper Lenght\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Length')\nsns.relplot(data = train, y = 'Culmen Depth (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Depth')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first graph explores the relationship between flipper length and culmen length across penguin species. Gentoo penguins have both longer flipper lengths and culmen lengths compared to the other species, making them easily distinguishable. Adelie penguins, in contrast, have shorter values for both features, clustering in the lower-left section of the graph. Chinstrap penguins overlap with Adelie penguins in flipper length but tend to have slightly longer culmen lengths, creating some classification challenges.\nThe second graph examines the relationship between flipper length and culmen depth. Here, Gentoo penguins are clearly separable from the other two species due to their significantly shallower culmen depths, forming a distinct cluster in the lower section. Howeber the Adelie and Chinstrap penguins overlap a lot more, particularly in flipper length and culmen depth. While Gentoo penguins can be classified easily, distinguishing between Adelie and Chinstrap penguins would be really difficult because of this clustering.\n\ntrain.groupby('Species').agg({\n    'Flipper Length (mm)': 'mean',\n    'Culmen Length (mm)': 'mean',\n    'Culmen Depth (mm)': 'mean'\n}).reset_index()\n\n\n\n\n\n\n\n\nSpecies\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n\n\n2\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433\n\n\n\n\n\n\n\nThis code above calculates the mean values of flipper length, culmen length, and culmen depth for each penguin species using the .groupby() function. The table summarizes the average flipper length, culmen length, and culmen depth for each penguin species. Gentoo penguins have the longest flipper and culmen lengths but the shallowest culmen depth. Chinstrap penguins have longer culmen lengths than Adelie penguins but similar culmen depths. Adelie penguins have the shortest flipper and culmen lengths but slightly deeper culmens than Chinstrap. These differences highlight key features that can help classify penguin species."
  },
  {
    "objectID": "posts/example-blog-post/index.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "href": "posts/example-blog-post/index.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "title": "Classifying Palmer Penguins",
    "section": "Now we are going to chose feature to train our model",
    "text": "Now we are going to chose feature to train our model\nBy using the combinations function from the itertools package, we generate different combinations of categorical and continuous features. So we will pair categorical variables like Sex and Clutch Completion with continuous variables such as Culmen Length, Culmen Depth, and Flipper Length. This allows us to explore various feature combinations and evaluate which ones might be most useful for accurately classifying the penguin species.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = None\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]  # Ensure categorical features are one-hot encoded\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        \n        # Train a logistic regression model to evaluate feature combinations\n        model = LogisticRegression(max_iter=1000)  # Increased iterations for better convergence\n        scores = cross_val_score(model, X_train[cols], y_train, cv=5, scoring='accuracy')\n        \n        avg_score = np.mean(scores)\n        if avg_score &gt; best_score:\n            best_score = avg_score\n            best_features = cols\n\nprint(\"Best features:\", best_features)\n\nBest features: ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nAccording to itertools, the best features to use are ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’. However, we wanted to explore how our model would perform based on our exploration of the island features along with the Culmen features. Thus, we will now reassign these as the best features and use them to train our model.\n\n#Rearranging Best featrues so that the first two columns are the quantitative columns and the rest are the qualitative columns\nbest_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nbest_features\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']"
  },
  {
    "objectID": "posts/example-blog-post/index.html#let-us-now-prepare-our-test-dataset",
    "href": "posts/example-blog-post/index.html#let-us-now-prepare-our-test-dataset",
    "title": "Classifying Palmer Penguins",
    "section": "Let us now prepare our Test Dataset",
    "text": "Let us now prepare our Test Dataset\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Prepare test data\nX_test, y_test = prepare_data(test)"
  },
  {
    "objectID": "posts/example-blog-post/index.html#training-and-evaluating-our-model",
    "href": "posts/example-blog-post/index.html#training-and-evaluating-our-model",
    "title": "Classifying Palmer Penguins",
    "section": "Training and evaluating our Model",
    "text": "Training and evaluating our Model\nIn the code below, we use a Logistic Regression model and train it on the selected features: [‘Culmen Length (mm)’, ‘Culmen Depth (mm)’, ‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’]. We then evaluate the accuracy using the LR.score function. Our training accuracy was 99%, and our testing accuracy—measuring how well the model performed on unseen data—was 100%. The confusion matrix confirms that we correctly classified all penguins with no misclassifications. Specifically, our model correctly identified 31 Adelie, 11 Chinstrap, and 26 Gentoo penguins, achieving perfect classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(X_train[best_features], y_train)\n\n\n\n# Predict on test set\ny_pred = LR.predict(X_test[best_features])\n\n# Evaluate the model\nscore_train = LR.score(X_train[best_features], y_train)\nscore_test = LR.score(X_test[best_features], y_test)\n\nprint(\"Training Accuracy:\", score_train)\nprint(\"Test Accuracy:\", score_test)\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\nTraining Accuracy: 0.9921875\nTest Accuracy: 1.0\n\nConfusion Matrix:\n [[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]"
  },
  {
    "objectID": "posts/example-blog-post/index.html#plotting-our-results",
    "href": "posts/example-blog-post/index.html#plotting-our-results",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting our results",
    "text": "Plotting our results\nThis code below helps us visualize the decision regions of our classification model by plotting how different penguin species are separated based on culmen features and island locations. It creates a grid of values for the culmen length and depth, predicts species across the grid. Each subplot represents a different island, showing how the model’s decision boundaries change depending on the island feature. This helps us understand how the model differentiates between Adelie, Chinstrap, and Gentoo penguins on each island.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7 * len(qual_features), 3))  # doing this to fit three plots side by side\n    fig.suptitle(title, fontsize=14)\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    if len(qual_features) == 1:\n        axarr = [axarr] \n\n    for i, ax in enumerate(axarr):\n        XY = pd.DataFrame({\n            X.columns[0]: XX,\n            X.columns[1]: YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # Use contour plot to visualize the predictions\n        ax.contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n        ax.scatter(x0[ix], x1[ix], c=y[ix], cmap=\"jet\", vmin=0, vmax=2)\n        \n        ax.set(xlabel=X.columns[0], ylabel=X.columns[1], title=qual_features[i])\n\n    patches = [Patch(color=color, label=spec) for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"])]\n    fig.legend(handles=patches, title=\"Species\", loc=\"upper right\")\n\n    plt.tight_layout(rect=[0, 0, 0.9, 1])\n    plt.show()\n\n# Run function for training and test sets\nplot_regions(LR, X_train[best_features], y_train, title=\"Training Set\")\nplot_regions(LR, X_test[best_features], y_pred, title=\"Test Set\")"
  },
  {
    "objectID": "posts/example-blog-post/blog1.html",
    "href": "posts/example-blog-post/blog1.html",
    "title": "Goal of Project",
    "section": "",
    "text": "---\ntitle: Hello Blog\nauthor: PB\ndate: '2025-02-20'\nimage: \"image.jpg\"\ndescription: \"A Blog post about classifying penguins with machine learning.\"\nformat: html\n---\nPredicting a penguin’s species based on its physical measurements is an interesting challenge in data analysis and machine learning. In this blog, we’ll explore the Palmer Penguins dataset, which was collected by Dr. Kristen Gorman and the Palmer Station, to analyze biological measurements of three penguin species in Antarctica. Through data visualization and feature selection, we’ll uncover key insights and build a classification model that achieves perfect accuracy using a carefully chosen set of features. Along the way, we’ll discuss our findings and ensure a reproducible and insightful approach to species classification. ### Loading Data We will now use the Pandas function to read the CSV and also take a look at the data and the different variables and columns we have in our dataset before we do and data explorations and analysis.\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/example-blog-post/blog1.html#data-cleaning",
    "href": "posts/example-blog-post/blog1.html#data-cleaning",
    "title": "Goal of Project",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nOur dataset contains numerous variables, including comments and region information, that are not relevant for training our model. Since we will be focusing on variables such as island, culmen length, culmen depth, flipper length, and body mass, we will remove unnecessary columns before proceeding with data exploration, visualization, and model training. Let’s start by loading the necessary packages for data cleaning, visualization, and model training. We will also convert categorical feature columns like Sex and Island into one-hot encoded 0-1( or true or false) columns using the pd.get_dummies function. Additionally, we can see that Species is a categorical variable, but instead of one-hot encoding, we will use label encoding to convert it into numerical labels: 0 for Adelie, 1 for Chinstrap, and 2 for Gentoo. This transformation will make data visualization and model training much easier.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode categorical features and label\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Stage\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    # One-hot encode categorical variables\n    df =(pd.get_dummies(df)).astype(int)\n\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40\n16\n187\n3200\n9\n-24\n0\n1\n0\n0\n1\n1\n0\n\n\n1\n49\n19\n210\n3950\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n2\n50\n15\n218\n5700\n8\n-25\n1\n0\n0\n0\n1\n0\n1\n\n\n3\n45\n14\n210\n4200\n7\n-25\n1\n0\n0\n0\n1\n1\n0\n\n\n4\n51\n18\n203\n4100\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n\n\n\n\n\n\nVisualizing the data\nThe bar chart below shows the number of penguins of each species on different islands, helping us identify patterns in species distribution. Interestingly, we observe: - Torgersen Island: Only Adelie penguins are present, with a population of about 30-40 individuals. - Dream Island: Has both Adelie and Chinstrap penguins, with Chinstrap being the dominant species. - Biscoe Island: Has both Adelie and Gentoo penguins, with Gentoo having the largest population overall.\nChinstrap penguins are only found on Dream Island, while Gentoo penguins are exclusive to Biscoe Island. Adelie penguins, on the other hand, are the most widespread, appearing on all three islands. Based on this distribution, the island where a penguin is found could be a useful feature for predicting its species.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n# Map species labels back to names\nspecies_map = {0: 'Adelie', 1: 'Chinstrap', 2: 'Gentoo'}\nspecies_island_counts = X_train.copy()\nspecies_island_counts[\"Species\"] = [species_map[label] for label in y_train]\n\n# Convert one-hot encoded islands back to a single column\nspecies_island_counts[\"Island\"] = species_island_counts[[\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]].idxmax(axis=1)\nspecies_island_counts[\"Island\"] = species_island_counts[\"Island\"].str.replace(\"Island_\", \"\")\n\n# Count number of penguins per species per island\nplot_data = species_island_counts.groupby([\"Island\", \"Species\"]).size().reset_index(name=\"Count\")\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.barplot(data=plot_data, x=\"Island\", y=\"Count\", hue=\"Species\", palette=\"mako\")\nplt.xlabel(\"Island\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Penguin Species Distribution Across Islands\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, we can see that by simply looking at the islands, we can somewhat predict the species of the penguin. This is a good example of a feature that could help predict the species. However, the model would be much more efficient if each island only hosted one species of penguin. Since some islands have more than one species, we might want to explore additional features. For example, if a penguin is on Biscoe Island and has a certain Culmen length and Culmen depth, can we make more accurate predictions based on those measurements? To explore this, we will create a graph to assess whether including Culmen lenght and Culmen depth is a useful feature for predicting the species on each island.\n\n\n# Plotting the relationship between Culmen Length and Culmen Depth for each species on each island\nsns.relplot(data =train, hue=\"Species\", y = 'Culmen Depth (mm)', x =  'Culmen Length (mm)',col = 'Island')\n\n\n\n\n\n\n\n\nThe graph above looks at the relationship between Culmen Depth (mm) and Culmen Length (mm) for different penguin species, separated by island. Each species is color-coded to highlight patterns in their beak dimensions. From this graph, we can observe: - Gentoo penguins (orange) tend to have longer culmen lengths and shallower culmen depths. - Adelie penguins (green) have moderate culmen lengths and a wide range of culmen depths. - Chinstrap penguins (blue) have longer culmen lenghts compared to Adelie penguins but exhibit similar culmen Depths. ## Can Culmen Features Help in Classification? Looking at the separability of species by island: - Biscoe Island: Gentoo and Adelie penguins are linearly separable, meaning a simple model could classify them effectively based on culmen features. - Dream Island: Chinstrap and Adelie penguins overlap in culmen depth, making them harder to separate linearly. However, their culmen lengths show some distinction, which could aid classification. - Torgersen Island: Since only Adelie penguins are found here, culmen features are irrelevant for classification on this island.\nThis suggests that culmen depth and length can be strong features for classifying species, especially when combined with island location. ## Exploring Culmen Features in relation to Flipper Lenght\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Length')\nsns.relplot(data = train, y = 'Culmen Depth (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Depth')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first graph explores the relationship between flipper length and culmen length across penguin species. Gentoo penguins have both longer flipper lengths and culmen lengths compared to the other species, making them easily distinguishable. Adelie penguins, in contrast, have shorter values for both features, clustering in the lower-left section of the graph. Chinstrap penguins overlap with Adelie penguins in flipper length but tend to have slightly longer culmen lengths, creating some classification challenges.\nThe second graph examines the relationship between flipper length and culmen depth. Here, Gentoo penguins are clearly separable from the other two species due to their significantly shallower culmen depths, forming a distinct cluster in the lower section. Howeber the Adelie and Chinstrap penguins overlap a lot more, particularly in flipper length and culmen depth. While Gentoo penguins can be classified easily, distinguishing between Adelie and Chinstrap penguins would be really difficult because of this clustering.\n\ntrain.groupby('Species').agg({\n    'Flipper Length (mm)': 'mean',\n    'Culmen Length (mm)': 'mean',\n    'Culmen Depth (mm)': 'mean'\n}).reset_index()\n\n\n\n\n\n\n\n\nSpecies\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n\n\n2\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433\n\n\n\n\n\n\n\nThis code above calculates the mean values of flipper length, culmen length, and culmen depth for each penguin species using the .groupby() function. The table summarizes the average flipper length, culmen length, and culmen depth for each penguin species. Gentoo penguins have the longest flipper and culmen lengths but the shallowest culmen depth. Chinstrap penguins have longer culmen lengths than Adelie penguins but similar culmen depths. Adelie penguins have the shortest flipper and culmen lengths but slightly deeper culmens than Chinstrap. These differences highlight key features that can help classify penguin species."
  },
  {
    "objectID": "posts/example-blog-post/blog1.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "href": "posts/example-blog-post/blog1.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "title": "Goal of Project",
    "section": "Now we are going to chose feature to train our model",
    "text": "Now we are going to chose feature to train our model\nBy using the combinations function from the itertools package, we generate different combinations of categorical and continuous features. So we will pair categorical variables like Sex and Clutch Completion with continuous variables such as Culmen Length, Culmen Depth, and Flipper Length. This allows us to explore various feature combinations and evaluate which ones might be most useful for accurately classifying the penguin species.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = None\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]  # Ensure categorical features are one-hot encoded\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        \n        # Train a logistic regression model to evaluate feature combinations\n        model = LogisticRegression(max_iter=1000)  # Increased iterations for better convergence\n        scores = cross_val_score(model, X_train[cols], y_train, cv=5, scoring='accuracy')\n        \n        avg_score = np.mean(scores)\n        if avg_score &gt; best_score:\n            best_score = avg_score\n            best_features = cols\n\nprint(\"Best features:\", best_features)\n\nBest features: ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nAccording to itertools, the best features to use are ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’. However, we wanted to explore how our model would perform based on our exploration of the island features along with the Culmen features. Thus, we will now reassign these as the best features and use them to train our model.\n\n#Rearranging Best featrues so that the first two columns are the quantitative columns and the rest are the qualitative columns\nbest_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nbest_features\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']"
  },
  {
    "objectID": "posts/example-blog-post/blog1.html#let-us-now-prepare-our-test-dataset",
    "href": "posts/example-blog-post/blog1.html#let-us-now-prepare-our-test-dataset",
    "title": "Goal of Project",
    "section": "Let us now prepare our Test Dataset",
    "text": "Let us now prepare our Test Dataset\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Prepare test data\nX_test, y_test = prepare_data(test)"
  },
  {
    "objectID": "posts/example-blog-post/blog1.html#training-and-evaluating-our-model",
    "href": "posts/example-blog-post/blog1.html#training-and-evaluating-our-model",
    "title": "Goal of Project",
    "section": "Training and evaluating our Model",
    "text": "Training and evaluating our Model\nIn the code below, we use a Logistic Regression model and train it on the selected features: [‘Culmen Length (mm)’, ‘Culmen Depth (mm)’, ‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’]. We then evaluate the accuracy using the LR.score function. Our training accuracy was 99%, and our testing accuracy—measuring how well the model performed on unseen data—was 100%. The confusion matrix confirms that we correctly classified all penguins with no misclassifications. Specifically, our model correctly identified 31 Adelie, 11 Chinstrap, and 26 Gentoo penguins, achieving perfect classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(X_train[best_features], y_train)\n\n\n\n# Predict on test set\ny_pred = LR.predict(X_test[best_features])\n\n# Evaluate the model\nscore_train = LR.score(X_train[best_features], y_train)\nscore_test = LR.score(X_test[best_features], y_test)\n\nprint(\"Training Accuracy:\", score_train)\nprint(\"Test Accuracy:\", score_test)\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\nTraining Accuracy: 0.9921875\nTest Accuracy: 1.0\n\nConfusion Matrix:\n [[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]"
  },
  {
    "objectID": "posts/example-blog-post/blog1.html#plotting-our-results",
    "href": "posts/example-blog-post/blog1.html#plotting-our-results",
    "title": "Goal of Project",
    "section": "Plotting our results",
    "text": "Plotting our results\nThis code below helps us visualize the decision regions of our classification model by plotting how different penguin species are separated based on culmen features and island locations. It creates a grid of values for the culmen length and depth, predicts species across the grid. Each subplot represents a different island, showing how the model’s decision boundaries change depending on the island feature. This helps us understand how the model differentiates between Adelie, Chinstrap, and Gentoo penguins on each island.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7 * len(qual_features), 3))  # doing this to fit three plots side by side\n    fig.suptitle(title, fontsize=14)\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    if len(qual_features) == 1:\n        axarr = [axarr] \n\n    for i, ax in enumerate(axarr):\n        XY = pd.DataFrame({\n            X.columns[0]: XX,\n            X.columns[1]: YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # Use contour plot to visualize the predictions\n        ax.contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n        ax.scatter(x0[ix], x1[ix], c=y[ix], cmap=\"jet\", vmin=0, vmax=2)\n        \n        ax.set(xlabel=X.columns[0], ylabel=X.columns[1], title=qual_features[i])\n\n    patches = [Patch(color=color, label=spec) for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"])]\n    fig.legend(handles=patches, title=\"Species\", loc=\"upper right\")\n\n    plt.tight_layout(rect=[0, 0, 0.9, 1])\n    plt.show()\n\n# Run function for training and test sets\nplot_regions(LR, X_train[best_features], y_train, title=\"Training Set\")\nplot_regions(LR, X_test[best_features], y_pred, title=\"Test Set\")"
  },
  {
    "objectID": "posts/Penguins/index.html",
    "href": "posts/Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset, collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, provides physiological measurements for three species of penguins—Adélie, Chinstrap, and Gentoo—inhabiting the Palmer Archipelago. First published in 2014 and later made widely accessible to the data science community, this dataset serves as a valuable resource for exploring classification techniques and species differentiation. In this blog post, I analyze the dataset using machine learning models to classify penguin species based on features such as flipper length, body mass, and culmen dimensions. By analyzing physical measurements we identify key features that contribute to accurate species prediction. Through data visualization and feature selection, we uncover patterns that distinguish species based on island location and physical traits. Using logistic regression, we evaluate different feature combinations and determine an optimal set for classification and acheive a 99% training accuracy and a 100% testing accuracy. Our findings highlight the power of data-driven approaches in biological classification while ensuring a reproducible and insightful methodology."
  },
  {
    "objectID": "posts/Penguins/index.html#data-cleaning",
    "href": "posts/Penguins/index.html#data-cleaning",
    "title": "Classifying Palmer Penguins",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nOur dataset contains numerous variables, including comments and region information, that are not relevant for training our model. Since we will be focusing on variables such as island, culmen length, culmen depth, flipper length, and body mass, we will remove unnecessary columns before proceeding with data exploration, visualization, and model training. Let’s start by loading the necessary packages for data cleaning, visualization, and model training. We will also convert categorical feature columns like Sex and Island into one-hot encoded 0-1( or true or false) columns using the pd.get_dummies function. Additionally, we can see that Species is a categorical variable, but instead of one-hot encoding, we will use label encoding to convert it into numerical labels: 0 for Adelie, 1 for Chinstrap, and 2 for Gentoo. This transformation will make data visualization and model training much easier.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode categorical features and label\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Stage\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    # One-hot encode categorical variables\n    df =(pd.get_dummies(df)).astype(int)\n\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40\n16\n187\n3200\n9\n-24\n0\n1\n0\n0\n1\n1\n0\n\n\n1\n49\n19\n210\n3950\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n2\n50\n15\n218\n5700\n8\n-25\n1\n0\n0\n0\n1\n0\n1\n\n\n3\n45\n14\n210\n4200\n7\n-25\n1\n0\n0\n0\n1\n1\n0\n\n\n4\n51\n18\n203\n4100\n9\n-24\n0\n1\n0\n0\n1\n0\n1"
  },
  {
    "objectID": "posts/Penguins/index.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "href": "posts/Penguins/index.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "title": "Classifying Palmer Penguins",
    "section": "Now we are going to chose feature to train our model",
    "text": "Now we are going to chose feature to train our model\nBy using the combinations function from the itertools package, we generate different combinations of categorical and continuous features. So we will pair categorical variables like Sex and Clutch Completion with continuous variables such as Culmen Length, Culmen Depth, and Flipper Length. This allows us to explore various feature combinations and evaluate which ones might be most useful for accurately classifying the penguin species.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = None\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]  # Ensure categorical features are one-hot encoded\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        \n        # Train a logistic regression model to evaluate feature combinations\n        model = LogisticRegression(max_iter=1000)  # Increased iterations for better convergence\n        scores = cross_val_score(model, X_train[cols], y_train, cv=5, scoring='accuracy')\n        \n        avg_score = np.mean(scores)\n        if avg_score &gt; best_score:\n            best_score = avg_score\n            best_features = cols\n\nprint(\"Best features:\", best_features)\n\nBest features: ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nAccording to itertools, the best features to use are ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’. However, we wanted to explore how our model would perform based on our exploration of the island features along with the Culmen features. Thus, we will now reassign these as the best features and use them to train our model.\n\n#Rearranging Best featrues so that the first two columns are the quantitative columns and the rest are the qualitative columns\nbest_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nbest_features\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']"
  },
  {
    "objectID": "posts/Penguins/index.html#let-us-now-prepare-our-test-dataset",
    "href": "posts/Penguins/index.html#let-us-now-prepare-our-test-dataset",
    "title": "Classifying Palmer Penguins",
    "section": "Let us now prepare our Test Dataset",
    "text": "Let us now prepare our Test Dataset\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Prepare test data\nX_test, y_test = prepare_data(test)"
  },
  {
    "objectID": "posts/Penguins/index.html#training-and-evaluating-our-model",
    "href": "posts/Penguins/index.html#training-and-evaluating-our-model",
    "title": "Classifying Palmer Penguins",
    "section": "Training and evaluating our Model",
    "text": "Training and evaluating our Model\nIn the code below, we use a Logistic Regression model and train it on the selected features: [‘Culmen Length (mm)’, ‘Culmen Depth (mm)’, ‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’]. We then evaluate the accuracy using the LR.score function. Our training accuracy was 99%, and our testing accuracy—measuring how well the model performed on unseen data—was 100%. The confusion matrix confirms that we correctly classified all penguins with no misclassifications. Specifically, our model correctly identified 31 Adelie, 11 Chinstrap, and 26 Gentoo penguins, achieving perfect classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(X_train[best_features], y_train)\n\n\n\n# Predict on test set\ny_pred = LR.predict(X_test[best_features])\n\n# Evaluate the model\nscore_train = LR.score(X_train[best_features], y_train)\nscore_test = LR.score(X_test[best_features], y_test)\n\nprint(\"Training Accuracy:\", score_train)\nprint(\"Test Accuracy:\", score_test)\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\nTraining Accuracy: 0.9921875\nTest Accuracy: 1.0\n\nConfusion Matrix:\n [[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]"
  },
  {
    "objectID": "posts/Penguins/index.html#plotting-our-results",
    "href": "posts/Penguins/index.html#plotting-our-results",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting our results",
    "text": "Plotting our results\nThis code below helps us visualize the decision regions of our classification model by plotting how different penguin species are separated based on culmen features and island locations. It creates a grid of values for the culmen length and depth, predicts species across the grid. Each subplot represents a different island, showing how the model’s decision boundaries change depending on the island feature. This helps us understand how the model differentiates between Adelie, Chinstrap, and Gentoo penguins on each island.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7 * len(qual_features), 3))  # doing this to fit three plots side by side\n    fig.suptitle(title, fontsize=14)\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    if len(qual_features) == 1:\n        axarr = [axarr] \n\n    for i, ax in enumerate(axarr):\n        XY = pd.DataFrame({\n            X.columns[0]: XX,\n            X.columns[1]: YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # Use contour plot to visualize the predictions\n        ax.contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n        ax.scatter(x0[ix], x1[ix], c=y[ix], cmap=\"jet\", vmin=0, vmax=2)\n        \n        ax.set(xlabel=X.columns[0], ylabel=X.columns[1], title=qual_features[i])\n\n    patches = [Patch(color=color, label=spec) for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"])]\n    fig.legend(handles=patches, title=\"Species\", loc=\"upper right\")\n\n    plt.tight_layout(rect=[0, 0, 0.9, 1])\n    plt.show()\n\n# Run function for training and test sets\nplot_regions(LR, X_train[best_features], y_train, title=\"Training Set\")\nplot_regions(LR, X_test[best_features], y_pred, title=\"Test Set\")"
  },
  {
    "objectID": "posts/Penguins/index.html#can-culmen-features-help-in-classification",
    "href": "posts/Penguins/index.html#can-culmen-features-help-in-classification",
    "title": "Classifying Palmer Penguins",
    "section": "Can Culmen Features Help in Classification?",
    "text": "Can Culmen Features Help in Classification?\nLooking at the separability of species by island:\n\nBiscoe Island: Gentoo and Adelie penguins are linearly separable, meaning a simple model could classify them effectively based on culmen features.\nDream Island: Chinstrap and Adelie penguins overlap in culmen depth, making them harder to separate linearly. However, their culmen lengths show some distinction, which could aid classification.\nTorgersen Island: Since only Adelie penguins are found here, culmen features are irrelevant for classification on this island.\n\nThis suggests that culmen depth and length can be strong features for classifying species, especially when combined with island location."
  },
  {
    "objectID": "posts/Penguins/index.html#exploring-culmen-features-in-relation-to-flipper-lenght",
    "href": "posts/Penguins/index.html#exploring-culmen-features-in-relation-to-flipper-lenght",
    "title": "Classifying Palmer Penguins",
    "section": "Exploring Culmen Features in relation to Flipper Lenght",
    "text": "Exploring Culmen Features in relation to Flipper Lenght\nThe graph below explores the relationship between flipper length and culmen length across penguin species. Gentoo penguins have both longer flipper lengths and culmen lengths compared to the other species, making them easily distinguishable. Adelie penguins, in contrast, have shorter values for both features, clustering in the lower-left section of the graph. Chinstrap penguins overlap with Adelie penguins in flipper length but tend to have slightly longer culmen lengths, creating some classification challenges.\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Length')\n\n\n\n\n\n\n\n\n\n\nsns.relplot(data = train, y = 'Culmen Depth (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Depth')\n\n\n\n\n\n\n\n\nThe graph above examines the relationship between flipper length and culmen depth. Here, Gentoo penguins are clearly separable from the other two species due to their significantly shallower culmen depths, forming a distinct cluster in the lower section. Howeber the Adelie and Chinstrap penguins overlap a lot more, particularly in flipper length and culmen depth. While Gentoo penguins can be classified easily, distinguishing between Adelie and Chinstrap penguins would be really difficult because of this clustering."
  },
  {
    "objectID": "posts/Penguins/index.html#decision-region-plots",
    "href": "posts/Penguins/index.html#decision-region-plots",
    "title": "Classifying Palmer Penguins",
    "section": "Decision Region Plots",
    "text": "Decision Region Plots\nThis code below helps us visualize the decision regions of our classification model by plotting how different penguin species are separated based on culmen features and island locations. It creates a grid of values for the culmen length and depth, predicts species across the grid. Each subplot represents a different island, showing how the model’s decision boundaries change depending on the island feature. This helps us understand how the model differentiates between Adelie, Chinstrap, and Gentoo penguins on each island.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    # Define a color mapping where Gentoo is green and Chinstrap is blue\n    species_colors = {0: 'red', 1: 'blue', 2: 'green'}\n    \n    # Adjust figure size for better visibility\n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(12 * len(qual_features), 10))  # Increase size\n    fig.suptitle(title, fontsize=40, y=1.05)  # Adjust title position and size\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    if len(qual_features) == 1:\n        axarr = [axarr] \n\n    for i, ax in enumerate(axarr):\n        XY = pd.DataFrame({\n            X.columns[0]: XX,\n            X.columns[1]: YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # Use contour plot to visualize the predictions\n        ax.contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n        # Convert y[ix] to a pandas Series and apply map to get colors\n        y_series = pd.Series(y[ix])  # Convert y[ix] to a Series\n        ax.scatter(x0[ix], x1[ix], c=y_series.map(species_colors), s=80)  # Larger scatter points\n\n        ax.set_xlabel(X.columns[0], fontsize=30)\n        ax.set_ylabel(X.columns[1], fontsize=30)\n        ax.set_title(qual_features[i], fontsize=35)\n\n        ax.tick_params(axis='both', labelsize=25)  # Larger tick labels\n\n    # Increase spacing between subplots and adjust title positioning\n    plt.subplots_adjust(wspace=0.4, hspace=0.3, top=0.85)  \n\n    # Create legend with specific colors for species\n    patches = [Patch(color=color, label=spec) for spec, color in zip([\"Adelie\", \"Chinstrap\", \"Gentoo\"], [\"red\", \"blue\", \"green\"])]\n    fig.legend(handles=patches, title=\"Species\", loc=\"upper right\", fontsize=30, title_fontsize=35, bbox_to_anchor=(1.03, 1)) \n\n    plt.show()\n\n# Run function for training and test sets\nplot_regions(LR, X_train[best_features], y_train, title=\"Training Set\")\nplot_regions(LR, X_test[best_features], y_pred, title=\"Test Set\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe decision region plots clearly show how our model classifies penguins based on culmen length and depth, with island location influencing the boundaries.\n\nIsland Biscoe: The model effectively separates Gentoo (green) from Adelie (red) penguins, as they occupy distinct regions.\nIsland Dream: Chinstrap (blue) and Adelie (red) penguins have overlapping regions, making classification slightly more challenging. However, longer culmen lengths help differentiate Chinstrap penguins.\nIsland Torgersen: Since only Adlie penguins are found here, the entire region is classified as red, confirming that island location alone can sometimes be a strong predictor.\n\nThe clean decision boundaries in most plots indicate that the selected features work really well for our classification."
  },
  {
    "objectID": "posts/Penguins/index.html#discussion-and-takeaways",
    "href": "posts/Penguins/index.html#discussion-and-takeaways",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion And Takeaways",
    "text": "Discussion And Takeaways\nThrough this analysis, I gained several important insights about both the dataset and machine learning model performance. One of the key takeaways was the importance of data preprocessing, including encoding categorical variables and handling missing data, to ensure the model could learn effectively. By visualizing relationships between features, I noticed that certain physical traits, like culmen length and culmen depth, played a crucial role in distinguishing penguin species. Additionally, island location proved to be a strong predictor, as some species were exclusive to certain islands.\nTraining a logistic regression model with carefully selected features led to a 99% accuracy on the training set and 100% accuracy on the test set, showing that the model generalized well to unseen data. The decision region plots clearly illustrated how the model separated species based on their culmen features, with some overlap between Adelie and Chinstrap penguins making classification slightly more challenging. Overall, this project demonstrated how feature selection, visualization, and a well-trained model can achieve highly accurate classifications, reinforcing the power of machine learning in biological research."
  },
  {
    "objectID": "posts/Penguins/index.html#goal-of-project",
    "href": "posts/Penguins/index.html#goal-of-project",
    "title": "Classifying Palmer Penguins",
    "section": "Goal of Project",
    "text": "Goal of Project\nPredicting a penguin’s species based on its physical measurements is an interesting challenge in data analysis and machine learning. In this blog, we’ll explore the Palmer Penguins dataset, which was collected by Dr. Kristen Gorman and the Palmer Station, to analyze biological measurements of three penguin species in Antarctica. Through data visualization and feature selection, we’ll uncover key insights and build a classification model that achieves perfect accuracy using a carefully chosen set of features. Along the way, we’ll discuss our findings and ensure a reproducible and insightful approach to species classification."
  },
  {
    "objectID": "posts/Penguins/index.html#loading-data",
    "href": "posts/Penguins/index.html#loading-data",
    "title": "Classifying Palmer Penguins",
    "section": "Loading Data",
    "text": "Loading Data\nWe will now use the Pandas function to read the CSV and also take a look at the data and the different variables and columns we have in our dataset before we do and data explorations and analysis.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/Penguins/index.html#abstract",
    "href": "posts/Penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset, collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, provides physiological measurements for three species of penguins—Adélie, Chinstrap, and Gentoo—inhabiting the Palmer Archipelago. First published in 2014 and later made widely accessible to the data science community, this dataset serves as a valuable resource for exploring classification techniques and species differentiation. In this blog post, I analyze the dataset using machine learning models to classify penguin species based on features such as flipper length, body mass, and culmen dimensions. By analyzing physical measurements we identify key features that contribute to accurate species prediction. Through data visualization and feature selection, we uncover patterns that distinguish species based on island location and physical traits. Using logistic regression, we evaluate different feature combinations and determine an optimal set for classification and acheive a 99% training accuracy and a 100% testing accuracy. Our findings highlight the power of data-driven approaches in biological classification while ensuring a reproducible and insightful methodology."
  },
  {
    "objectID": "posts/Penguins/index.html#visualizing-the-data",
    "href": "posts/Penguins/index.html#visualizing-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizing the data",
    "text": "Visualizing the data\nThe bar chart below shows the number of penguins of each species on different islands, helping us identify patterns in species distribution. Interestingly, we observe:\n\nTorgersen Island: Only Adelie penguins are present, with a population of about 30-40 individuals.\nDream Island: Has both Adelie and Chinstrap penguins, with Chinstrap being the dominant species.\nBiscoe Island: Has both Adelie and Gentoo penguins, with Gentoo having the largest population overall.\n\nChinstrap penguins are only found on Dream Island, while Gentoo penguins are exclusive to Biscoe Island. Adelie penguins, on the other hand, are the most widespread, appearing on all three islands. Based on this distribution, the island where a penguin is found could be a useful feature for predicting its species.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n# Map species labels back to names\nspecies_map = {0: 'Adelie', 1: 'Chinstrap', 2: 'Gentoo'}\nspecies_island_counts = X_train.copy()\nspecies_island_counts[\"Species\"] = [species_map[label] for label in y_train]\n\n# Convert one-hot encoded islands back to a single column\nspecies_island_counts[\"Island\"] = species_island_counts[[\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]].idxmax(axis=1)\nspecies_island_counts[\"Island\"] = species_island_counts[\"Island\"].str.replace(\"Island_\", \"\")\n\n# Count number of penguins per species per island\nplot_data = species_island_counts.groupby([\"Island\", \"Species\"]).size().reset_index(name=\"Count\")\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.barplot(data=plot_data, x=\"Island\", y=\"Count\", hue=\"Species\", palette=\"mako\")\nplt.xlabel(\"Island\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Penguin Species Distribution Across Islands\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, we can see that by simply looking at the islands, we can somewhat predict the species of the penguin. This is a good example of a feature that could help predict the species. However, the model would be much more efficient if each island only hosted one species of penguin. Since some islands have more than one species, we might want to explore additional features. For example, if a penguin is on Biscoe Island and has a certain Culmen length and Culmen depth, can we make more accurate predictions based on those measurements? To explore this, we will create a graph to assess whether including Culmen lenght and Culmen depth is a useful feature for predicting the species on each island.\n\n\n# Plotting the relationship between Culmen Length and Culmen Depth for each species on each island\ng = sns.relplot(data=train, hue=\"Species\", y=\"Culmen Depth (mm)\", x=\"Culmen Length (mm)\", col=\"Island\")\n\ng.set_titles(size=18)  # Increase subplot titles\ng.set_axis_labels(\"Culmen Length (mm)\", \"Culmen Depth (mm)\", fontsize=16)  # Increase axis labels font size\ng._legend.set_title(\"Species\", prop={'size': 16}) \n[g._legend.get_texts()[i].set_fontsize(14) for i in range(len(g._legend.get_texts()))]  # Increase legend text size\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe graph above looks at the relationship between Culmen Depth (mm) and Culmen Length (mm) for different penguin species, separated by island. Each species is color-coded to highlight patterns in their beak dimensions. From this graph, we can observe:\n\nGentoo penguins (orange) tend to have longer culmen lengths and shallower culmen depths.\nAdelie penguins (green) have moderate culmen lengths and a wide range of culmen depths.\nChinstrap penguins (blue) have longer culmen lenghts compared to Adelie penguins but exhibit similar culmen Depths."
  },
  {
    "objectID": "posts/Penguins/index.html#feature-analysis-flipper-and-culmen-measurements",
    "href": "posts/Penguins/index.html#feature-analysis-flipper-and-culmen-measurements",
    "title": "Classifying Palmer Penguins",
    "section": "Feature Analysis: Flipper and Culmen Measurements",
    "text": "Feature Analysis: Flipper and Culmen Measurements\nThe table below shows the mean values of flipper length, culmen length, and culmen depth for each penguin species using the .groupby() function. The table summarizes the average flipper length, culmen length, and culmen depth for each penguin species. Gentoo penguins have the longest flipper and culmen lengths but the shallowest culmen depth. Chinstrap penguins have longer culmen lengths than Adelie penguins but similar culmen depths. Adelie penguins have the shortest flipper and culmen lengths but slightly deeper culmens than Chinstrap. These differences highlight key features that can help classify penguin species.\n\ntrain.groupby('Species').agg({\n    'Flipper Length (mm)': 'mean',\n    'Culmen Length (mm)': 'mean',\n    'Culmen Depth (mm)': 'mean'\n}).reset_index()\n\n\n\n\n\n\n\n\nSpecies\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n\n\n2\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433"
  },
  {
    "objectID": "posts/Penguins/index.html#lets-now-prepare-our-test-dataset",
    "href": "posts/Penguins/index.html#lets-now-prepare-our-test-dataset",
    "title": "Classifying Palmer Penguins",
    "section": "Let’s now prepare our Test Dataset",
    "text": "Let’s now prepare our Test Dataset\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Prepare test data\nX_test, y_test = prepare_data(test)"
  },
  {
    "objectID": "posts/Penguins/index.html#now-we-are-going-to-chose-features-to-train-our-model",
    "href": "posts/Penguins/index.html#now-we-are-going-to-chose-features-to-train-our-model",
    "title": "Classifying Palmer Penguins",
    "section": "Now we are going to chose features to train our model",
    "text": "Now we are going to chose features to train our model\nBy using the combinations function from the itertools package, we generate different combinations of categorical and continuous features. So we will pair categorical variables like Sex and Clutch Completion with continuous variables such as Culmen Length, Culmen Depth, and Flipper Length. This allows us to explore various feature combinations and evaluate which ones might be most useful for accurately classifying the penguin species.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = None\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]  # Ensure categorical features are one-hot encoded\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        \n        # Train a logistic regression model to evaluate feature combinations\n        model = LogisticRegression(max_iter=1000)  # Increased iterations for better convergence\n        scores = cross_val_score(model, X_train[cols], y_train, cv=5, scoring='accuracy')\n        \n        avg_score = np.mean(scores)\n        if avg_score &gt; best_score:\n            best_score = avg_score\n            best_features = cols\n\nprint(\"Best features:\", best_features)\n\nBest features: ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nAccording to itertools, the best features to use are ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’. However, we wanted to explore how our model would perform based on our exploration of the island features along with the Culmen features. Thus, we will now reassign these as the best features and use them to train our model.\n\n#Rearranging Best featrues so that the first two columns are the quantitative columns and the rest are the qualitative columns\nbest_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nbest_features\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#loading-the-data",
    "href": "posts/new-new-test-post/index.html#loading-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Loading the data",
    "text": "Loading the data\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#abstract-and-methodologies",
    "href": "posts/new-new-test-post/index.html#abstract-and-methodologies",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#analysis-of-loan-intent-by-age-group",
    "href": "posts/new-new-test-post/index.html#analysis-of-loan-intent-by-age-group",
    "title": "Classifying Palmer Penguins",
    "section": "Analysis of Loan Intent by Age Group",
    "text": "Analysis of Loan Intent by Age Group\nThis bar chart below shows how different age groups use their loans for various purposes—such as venture, education, medical, home improvement, personal, and debt consolidation. We can see that borrowers aged 18–29 make up a large portion of total loans, often driven by education and personal loan needs. As age increases, the number of loans generally decreases, but certain categories—like debt consolidation—can become more common in older groups.\nOverall, this chart highlights that younger borrowers borrow a lot more money may be more focused on educational or personal financing, while older borrowers might shift their attention to consolidating debt or improving their homes. Understanding these patterns help us in understanding different patternns in who would default on a loan and not.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n# Create additional features for analysis:\n# 1. Age groups: we create bins to see how loan intent varies with age.\nage_bins = [18, 30, 40, 50, 60, 100]\nage_labels = ['18-29', '30-39', '40-49', '50-59', '60+']\ndf['age_group'] = pd.cut(df['person_age'], bins=age_bins, labels=age_labels)\n\n# ---------------------------\n# Visualization 1:\n# How does loan intent vary with age and home ownership status?\nplt.figure(figsize=(10, 6))\nsns.countplot(data=df, x='age_group', hue='loan_intent')\nplt.title('Loan Intent Distribution by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Count')\nplt.legend(title='Loan Intent')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "href": "posts/new-new-test-post/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "title": "Classifying Palmer Penguins",
    "section": "Analysis of Average Loan Amount by Credit History Length",
    "text": "Analysis of Average Loan Amount by Credit History Length\nThis bar chart shows how the average loan amount changes based on the number of years a borrower has had a credit history. In general, we see that some longer lengths of credit history are associated with higher average loan amounts than others, though the pattern isn’t strictly increasing or decreasing. This suggests that lenders may be willing to extend larger lines of credit to individuals with certain credit history profiles.\nFor our automated decision system, credit history length could be an important feature because it often reflects a borrower’s past experience with credit and repayment behavior. However, we must be mindful of fairness and potential biases: borrowers who are younger or newer to credit might be at a disadvantage if the model heavily weighs credit history length. Balancing profitability for the bank with equitable access to credit remains a key challenge in designing our scoring and thresholding methods.\n\n\n# 2. Employment length groups: useful for exploring patterns with job experience.\nemp_bins = [0, 2, 5, 10, df['person_emp_length'].max()]\nemp_labels = ['0-1 yrs', '2-4 yrs', '5-9 yrs', '10+ yrs']\ndf['emp_length_group'] = pd.cut(df['person_emp_length'], bins=emp_bins, labels=emp_labels)\n\n# ---------------------------\n# Visualization 2:\n# Which segments are offered different interest rates? Compare distributions by home ownership.\nplt.figure(figsize=(10, 6))\navg_loan = df.groupby('cb_person_cred_hist_length')['loan_amnt'].mean().reset_index()\nsns.barplot(data=avg_loan, x='cb_person_cred_hist_length', y='loan_amnt')\nplt.title('Average Loan Amount by Credit History Length')\nplt.xlabel('Credit History Length (Years)')\nplt.ylabel('Average Loan Amount')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "href": "posts/new-new-test-post/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "title": "Classifying Palmer Penguins",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership\nIn our scatter plot below, each dot represents a borrower, with the x-axis showing how large the loan is relative to their income (as a percentage) and the y-axis showing the absolute loan amount. The colors indicate different types of home ownership (RENT, MORTGAGE, OWN, OTHER).\nAs you can see, the data points overlap heavily, making the chart look cluttered. To get a clearer picture, I wo;; split these data into separate graphs for each home ownership category. This will help us see more nuanced patterns—like whether renters tend to have higher loan-to-income ratios compared to those who own or have a mortgage.\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=df, \n    x='loan_percent_income_pct', \n    y='loan_amnt', \n    hue='person_home_ownership',\n    alpha=0.7\n)\nplt.title('Loan Amount vs. Loan % of Income by Home Ownership')\nplt.xlabel('Loan as % of Person Income')\nplt.ylabel('Loan Amount')\nplt.xlim(0, 100)  # Focus on 0–100% if most loans fall in this range\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "href": "posts/new-new-test-post/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "title": "Classifying Palmer Penguins",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)\nBy splitting the data into four subplots (one for each home ownership category), we can see that:\n\nRenters typically have loan amounts averaging around $10,000–$12,000, with a wide spread of loan-to-income ratios (averaging around 15–20%). In addition to this we see the most clustering for this group which means, in our dataset most of the people taking our loans are from this group.\nMortgage holders often take out larger loans (averaging $16,000–$18,000) but may have lower loan-to-income ratios (closer to 10% on average).\nOwners (those who fully own their homes) tend to borrow moderate amounts ($12,000–$15,000) at ratios of around 12–15%.\nOthers (less common categories) show a broad mix but generally fall between these ranges.\n\nThese distinctions are important for our automated decision system, since each home ownership group presents a different risk and borrowing profile. When designing a score function and threshold to maximize the bank’s profit, it’s important for us to consider whether certain groups (like renters) might be unfairly penalized if they tend to have higher loan-to-income ratios. Ultimately, these separate plots help us fine-tune our model so that we balance profitability with equitable access to credit across different segments of borrowers.\n\ndf['loan_percent_income_pct'] = df['loan_percent_income'] * 100\n\n# Create a FacetGrid: one subplot per home ownership category\ng = sns.FacetGrid(df, col=\"person_home_ownership\", col_wrap=2, height=4)\ng.map(sns.scatterplot, \"loan_percent_income_pct\", \"loan_amnt\", alpha=0.7)\n\n# Set the x-axis limits and labels for clarity\ng.set(xlim=(0, 100))\ng.set_axis_labels(\"Loan as % of Income\", \"Loan Amount\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "href": "posts/new-new-test-post/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "title": "Classifying Palmer Penguins",
    "section": "Summary of Loan Intent and Home Ownership Segments",
    "text": "Summary of Loan Intent and Home Ownership Segments\nThis table shows how different combinations of loan intent (e.g., EDUCATION, MEDICAL, PERSONAL) and home ownership (MORTGAGE, OWN, RENT, OTHER) compare in terms of average interest rate, average loan amount, and count of borrowers. We’ve sorted the table by average loan amount in descending order to identify which segments receive the largest lines of credit.\n\nHighest Averages: Segments like PERSONAL–OTHER and MEDICAL–OTHER appear near the top, suggesting they receive higher loan amounts (over $12,000 on average), but also tend to have higher interest rates (11–12%).\nMortgage vs. Rent: Many MORTGAGE segments (e.g., DEBTCONSOLIDATION–MORTGAGE, EDUCATION–MORTGAGE) cluster in the middle, with average loan amounts around $10,000–$11,000 and interest rates near 10–10.6%. Renters often see slightly higher interest rates (11–12%) and somewhat lower loan amounts (around $8,000–$9,000).\nLow Counts: Some segments have very few borrowers (like DEBTCONSOLIDATION–OWN with a count of only 62), which may not be reliable for broad conclusions.\n\nFrom the perspective of building an automated decision system, these patterns hint at where the bank’s profit opportunities and risks might lie. For instance, segments with higher average loan amounts but also higher interest rates could be more profitable—but might also carry greater default risk. Tracking how many borrowers fall into each segment (the “count” column) helps ensure the model doesn’t overly focus on small, potentially unrepresentative groups.\n\nsummary_table = (\n    df\n    .groupby(['loan_intent', 'person_home_ownership'], as_index=False)\n    .agg(\n        avg_interest_rate=('loan_int_rate', 'mean'),\n        avg_loan_amount=('loan_amnt', 'mean'),\n        count=('loan_amnt', 'count')  # how many borrowers in each segment\n    )\n)\n\n# Sort by average loan amount (descending) to see which segments get the largest lines of credit\nsummary_table_sorted_by_amount = summary_table.sort_values('avg_loan_amount', ascending=False)\n\nsummary_table_sorted_by_amount\n\n\n\n\n\n\n\n\n\nloan_intent\nperson_home_ownership\navg_interest_rate\navg_loan_amount\ncount\n\n\n\n\n17\nPERSONAL\nOTHER\n11.675714\n12366.666667\n15\n\n\n13\nMEDICAL\nOTHER\n12.745000\n12200.000000\n13\n\n\n5\nEDUCATION\nOTHER\n12.400833\n12142.857143\n14\n\n\n9\nHOMEIMPROVEMENT\nOTHER\n11.683000\n10959.090909\n11\n\n\n8\nHOMEIMPROVEMENT\nMORTGAGE\n10.613916\n10764.017341\n1384\n\n\n20\nVENTURE\nMORTGAGE\n10.468000\n10606.281060\n1811\n\n\n0\nDEBTCONSOLIDATION\nMORTGAGE\n10.400489\n10588.756111\n1841\n\n\n4\nEDUCATION\nMORTGAGE\n10.554563\n10502.178076\n2089\n\n\n12\nMEDICAL\nMORTGAGE\n10.505553\n10485.867052\n1730\n\n\n16\nPERSONAL\nMORTGAGE\n10.426285\n10481.223233\n1868\n\n\n21\nVENTURE\nOTHER\n12.274211\n10367.500000\n20\n\n\n11\nHOMEIMPROVEMENT\nRENT\n11.812287\n10109.604633\n1252\n\n\n1\nDEBTCONSOLIDATION\nOTHER\n11.566667\n9783.333333\n15\n\n\n14\nMEDICAL\nOWN\n10.749097\n9367.755682\n352\n\n\n10\nHOMEIMPROVEMENT\nOWN\n10.922405\n9242.450980\n255\n\n\n6\nEDUCATION\nOWN\n10.797507\n8996.177184\n412\n\n\n18\nPERSONAL\nOWN\n10.867262\n8944.209040\n354\n\n\n3\nDEBTCONSOLIDATION\nRENT\n11.358223\n8882.754425\n2260\n\n\n19\nPERSONAL\nRENT\n11.535415\n8826.900046\n2171\n\n\n23\nVENTURE\nRENT\n11.438455\n8804.566745\n2135\n\n\n22\nVENTURE\nOWN\n10.560000\n8789.621914\n648\n\n\n7\nEDUCATION\nRENT\n11.315216\n8685.308193\n2612\n\n\n15\nMEDICAL\nRENT\n11.422580\n8426.925182\n2740\n\n\n2\nDEBTCONSOLIDATION\nOWN\n14.432909\n7749.193548\n62"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#training-and-evaluating-our-logistic-regression-model",
    "href": "posts/new-new-test-post/index.html#training-and-evaluating-our-logistic-regression-model",
    "title": "Classifying Palmer Penguins",
    "section": "Training and evaluating our Logistic Regression model",
    "text": "Training and evaluating our Logistic Regression model\nI used a logistic regression model to predict whether a prospective borrower will default on a loan. After preprocessing the data—by standardizing numerical features and one-hot encoding categorical variables. I removed rows with missing values, and split the dataset into training and test sets. The model achieved a test accuracy of about 84.2%.\nThe confusion matrix provides additional insight into the model’s performance:\n\nTrue Negatives (TN): 3,393 borrowers who did not default and were correctly predicted as non-default.\nFalse Positives (FP): 221 borrowers who did not default but were incorrectly flagged as defaults.\nFalse Negatives (FN): 501 borrowers who defaulted but were missed by the model.\nTrue Positives (TP): 467 borrowers who defaulted and were correctly identified.\n\nOur results suggest that while the model performs reasonably well overall, there is still a balance to be struck between avoiding false positives and false negatives. This is particularly important when designing an automated decision system for credit risk, because both profitability for the bank and equitable access to credit are critical. Further tuning of the threshold and exploration of additional features could help optimize the model even further for its intended purpose\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Drop rows with missing values\ndf_train = df_train.dropna(subset=numeric_features + categorical_features)\n\n\ntarget = 'loan_status'\nX = df_train.drop(columns=[target])\ny = df_train[target]\n\nnumeric_features = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', \n                      'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\ncategorical_features = ['person_home_ownership', 'loan_intent', 'cb_person_default_on_file']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n    ])\nX_transformed = preprocessor.fit_transform(X)\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, test_size=0.2, random_state=123\n)\n\n# Fit a logistic regression model using the preprocessed features\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(\"Test Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\nTest Accuracy: 0.8424268878219118\nConfusion Matrix:\n [[3393  221]\n [ 501  467]]"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#finding-weight-and-threshold",
    "href": "posts/new-new-test-post/index.html#finding-weight-and-threshold",
    "title": "Classifying Palmer Penguins",
    "section": "Finding weight and threshold",
    "text": "Finding weight and threshold\n\n# ------------------------------------------------------\n# 1. Define the profit formulas (vectorized)\n# ------------------------------------------------------\ndef profit_if_repaid(loan_amnt, loan_int_rate):\n    \"\"\"\n    If the loan is repaid in full, the bank's profit is:\n      loan_amnt * (1 + 0.25*loan_int_rate)^10 - loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 10 - loan_amnt\n\ndef profit_if_default(loan_amnt, loan_int_rate):\n    \"\"\"\n    If the borrower defaults, we assume default happens 3 years into the loan, \n    and the bank loses 70% of the principal:\n      loan_amnt*(1 + 0.25*loan_int_rate)^3 - 1.7*loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 3 - 1.7 * loan_amnt\n\nWe will now compute predicted probabilities (probability of default) for the training data\n\ny_prob_train = model.predict_proba(X_train)[:, 1]  # column 1 = probability of default\n\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)\nloan_amnt_array = X_train['loan_amnt'].to_numpy()\nloan_int_rate_array = X_train['loan_int_rate'].to_numpy()\ny_train_array = y_train.to_numpy()\nprofit_repaid = profit_if_repaid(loan_amnt_array, loan_int_rate_array)\nprofit_default = profit_if_default(loan_amnt_array, loan_int_rate_array)\n\nNow we are going to find a threshold and identify teh best threshold\n\n# 5. Sweep over thresholds to find the one that maximizes average profit\n\nthresholds = np.linspace(0, 1, 101)\navg_profits = []\n\nfor t in thresholds:\n    # Predict default if probability &gt;= t\n    predicted_default = (y_prob_train &gt;= t).astype(int)\n      # If we predict default, we do NOT give the loan =&gt; profit = 0\n    # If we predict no default, we DO give the loan =&gt; actual profit depends on y_train_array\n    #    - If actual y=0 (no default), profit = profit_repaid\n    #    - If actual y=1 (default), profit = profit_default\n    give_loan = 1 - predicted_default  # 1 = give loan, 0 = no loan\n    # total_profit[i] = give_loan[i] * [ (1 - y[i])*profit_repaid[i] + y[i]*profit_default[i] ]\n    total_profit = give_loan * ((1 - y_train_array) * profit_repaid + y_train_array * profit_default)\n    \n    # Compute average profit per borrower\n    avg_profit = total_profit.mean()\n    avg_profits.append(avg_profit)\n\navg_profits = np.array(avg_profits)\n\n# 6. Identify the best threshold\n\nbest_idx = np.argmax(avg_profits) # index of the best threshold\nbest_threshold = thresholds[best_idx]\n\n\nprint(f\"Best Threshold: {best_threshold:.3f}\")\n\nBest Threshold: 1.000\n\n\n\n# 7. Plot profit vs. threshold\n\nplt.figure(figsize=(8, 5))\nplt.plot(thresholds, avg_profits, label='Profit per Borrower')\nplt.scatter(best_threshold, best_profit, color='red', zorder=10, label='Optimal Threshold')\nplt.title('Profit per Borrower (Training Set) vs. Threshold')\nplt.xlabel('Threshold (Probability of Default)')\nplt.ylabel('Average Profit per Borrower')\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html",
    "href": "posts/discecting_racial_bias_algo/index.html",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "In this project, I aim to replicate and extend the findings of Obermeyer et al. (2019) by exploring how healthcare cost predictions relate to patients’ chronic illness burden and race. Using a randomized dataset, we first visualize the relationship between risk score percentiles, chronic conditions, and medical expenditures, revealing that White patients tend to generate higher costs than Black patients despite similar illness burdens. We then built a Ridge regression model with polynomial features to quantify the disparity in costs between Black and White patients. Our final model estimates that, holding illness burden constant, Black patients incur roughly 81% of the costs incurred by White patients, suggesting that the cost-based risk score underestimates the true care needs of Black patients."
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#exporing-the-data",
    "href": "posts/discecting_racial_bias_algo/index.html#exporing-the-data",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "Below we will accesss teh data clean it and explore the different variables and features.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n# df.dropna(inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\nrace_counts_bf_clean = df['race'].value_counts()\ndf.dropna(inplace=True)\nrace_counts = df['race'].value_counts()\nprint(\"Count of patients by race before cleaning data\",race_counts_bf_clean)\nprint(\"Count of patients by race after cleaning data\", race_counts)\n\nCount of patients by race before cleaning data race\nwhite    43202\nblack     5582\nName: count, dtype: int64\nCount of patients by race after cleaning data race\nwhite    5911\nblack    1000\nName: count, dtype: int64\n\n\n\nimport  matplotlib.pyplot as plt\n# Graph 1: Bar chart of race counts\nplt.figure(figsize=(6, 4))\nplt.bar(race_counts.index, race_counts.values, color=['blue', 'orange'])\nplt.xlabel('Race')\nplt.ylabel('Number of Patients')\nplt.title('Number of Black vs. White Patients')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x=\"risk_score_t\", y=\"cost_t\", hue=\"race\", alpha=0.7)\n\n# Labels and title\nplt.xlabel(\"Risk Score\")\nplt.ylabel(\"Cost\")\nplt.title(\"Risk Score vs Cost by Race\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html",
    "href": "posts/linear_score_function-Banks/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#abstract-and-methodologies",
    "href": "posts/linear_score_function-Banks/index.html#abstract-and-methodologies",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#loading-the-data",
    "href": "posts/linear_score_function-Banks/index.html#loading-the-data",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Loading the data",
    "text": "Loading the data\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#analysis-of-loan-intent-by-age-group",
    "href": "posts/linear_score_function-Banks/index.html#analysis-of-loan-intent-by-age-group",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Intent by Age Group",
    "text": "Analysis of Loan Intent by Age Group\nThis bar chart below shows how different age groups use their loans for various purposes—such as venture, education, medical, home improvement, personal, and debt consolidation. We can see that borrowers aged 18–29 make up a large portion of total loans, often driven by education and personal loan needs. As age increases, the number of loans generally decreases, but certain categories—like debt consolidation—can become more common in older groups.\nOverall, this chart highlights that younger borrowers borrow a lot more money may be more focused on educational or personal financing, while older borrowers might shift their attention to consolidating debt or improving their homes. Understanding these patterns help us in understanding different patternns in who would default on a loan and not.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n# Create additional features for analysis:\n# 1. Age groups: we create bins to see how loan intent varies with age.\nage_bins = [18, 30, 40, 50, 60, 100]\nage_labels = ['18-29', '30-39', '40-49', '50-59', '60+']\ndf['age_group'] = pd.cut(df['person_age'], bins=age_bins, labels=age_labels)\n\n# ---------------------------\n# Visualization 1:\n# How does loan intent vary with age and home ownership status?\nplt.figure(figsize=(10, 6))\nsns.countplot(data=df, x='age_group', hue='loan_intent')\nplt.title('Loan Intent Distribution by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Count')\nplt.legend(title='Loan Intent')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "href": "posts/linear_score_function-Banks/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Average Loan Amount by Credit History Length",
    "text": "Analysis of Average Loan Amount by Credit History Length\nThis bar chart shows how the average loan amount changes based on the number of years a borrower has had a credit history. In general, we see that some longer lengths of credit history are associated with higher average loan amounts than others, though the pattern isn’t strictly increasing or decreasing. This suggests that lenders may be willing to extend larger lines of credit to individuals with certain credit history profiles.\nFor our automated decision system, credit history length could be an important feature because it often reflects a borrower’s past experience with credit and repayment behavior. However, we must be mindful of fairness and potential biases: borrowers who are younger or newer to credit might be at a disadvantage if the model heavily weighs credit history length. Balancing profitability for the bank with equitable access to credit remains a key challenge in designing our scoring and thresholding methods.\n\n\n# 2. Employment length groups: useful for exploring patterns with job experience.\nemp_bins = [0, 2, 5, 10, df['person_emp_length'].max()]\nemp_labels = ['0-1 yrs', '2-4 yrs', '5-9 yrs', '10+ yrs']\ndf['emp_length_group'] = pd.cut(df['person_emp_length'], bins=emp_bins, labels=emp_labels)\n\n# ---------------------------\n# Visualization 2:\n# Which segments are offered different interest rates? Compare distributions by home ownership.\nplt.figure(figsize=(10, 6))\navg_loan = df.groupby('cb_person_cred_hist_length')['loan_amnt'].mean().reset_index()\nsns.barplot(data=avg_loan, x='cb_person_cred_hist_length', y='loan_amnt')\nplt.title('Average Loan Amount by Credit History Length')\nplt.xlabel('Credit History Length (Years)')\nplt.ylabel('Average Loan Amount')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "href": "posts/linear_score_function-Banks/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership\nIn our scatter plot below, each dot represents a borrower, with the x-axis showing how large the loan is relative to their income (as a percentage) and the y-axis showing the absolute loan amount. The colors indicate different types of home ownership (RENT, MORTGAGE, OWN, OTHER).\nAs you can see, the data points overlap heavily, making the chart look cluttered. To get a clearer picture, I wo;; split these data into separate graphs for each home ownership category. This will help us see more nuanced patterns—like whether renters tend to have higher loan-to-income ratios compared to those who own or have a mortgage.\n\ndf['loan_percent_income_pct'] = df['loan_percent_income'] * 100\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=df, \n    x='loan_percent_income_pct', \n    y='loan_amnt', \n    hue='person_home_ownership',\n    alpha=0.7\n)\nplt.title('Loan Amount vs. Loan % of Income by Home Ownership')\nplt.xlabel('Loan as % of Person Income')\nplt.ylabel('Loan Amount')\nplt.xlim(0, 100)  # Focus on 0–100% if most loans fall in this range\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "href": "posts/linear_score_function-Banks/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)\nBy splitting the data into four subplots (one for each home ownership category), we can see that:\n\nRenters typically have loan amounts averaging around $10,000–$12,000, with a wide spread of loan-to-income ratios (averaging around 15–20%). In addition to this we see the most clustering for this group which means, in our dataset most of the people taking our loans are from this group.\nMortgage holders often take out larger loans (averaging $16,000–$18,000) but may have lower loan-to-income ratios (closer to 10% on average).\nOwners (those who fully own their homes) tend to borrow moderate amounts ($12,000–$15,000) at ratios of around 12–15%.\nOthers (less common categories) show a broad mix but generally fall between these ranges.\n\nThese distinctions are important for our automated decision system, since each home ownership group presents a different risk and borrowing profile. When designing a score function and threshold to maximize the bank’s profit, it’s important for us to consider whether certain groups (like renters) might be unfairly penalized if they tend to have higher loan-to-income ratios. Ultimately, these separate plots help us fine-tune our model so that we balance profitability with equitable access to credit across different segments of borrowers.\n\n\n\n# Create a FacetGrid: one subplot per home ownership category\ng = sns.FacetGrid(df, col=\"person_home_ownership\", col_wrap=2, height=4)\ng.map(sns.scatterplot, \"loan_percent_income_pct\", \"loan_amnt\", alpha=0.7)\n\n# Set the x-axis limits and labels for clarity\ng.set(xlim=(0, 100))\ng.set_axis_labels(\"Loan as % of Income\", \"Loan Amount\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "href": "posts/linear_score_function-Banks/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Summary of Loan Intent and Home Ownership Segments",
    "text": "Summary of Loan Intent and Home Ownership Segments\nThis table shows how different combinations of loan intent (e.g., EDUCATION, MEDICAL, PERSONAL) and home ownership (MORTGAGE, OWN, RENT, OTHER) compare in terms of average interest rate, average loan amount, and count of borrowers. We’ve sorted the table by average loan amount in descending order to identify which segments receive the largest lines of credit.\n\nHighest Averages: Segments like PERSONAL–OTHER and MEDICAL–OTHER appear near the top, suggesting they receive higher loan amounts (over $12,000 on average), but also tend to have higher interest rates (11–12%).\nMortgage vs. Rent: Many MORTGAGE segments (e.g., DEBTCONSOLIDATION–MORTGAGE, EDUCATION–MORTGAGE) cluster in the middle, with average loan amounts around $10,000–$11,000 and interest rates near 10–10.6%. Renters often see slightly higher interest rates (11–12%) and somewhat lower loan amounts (around $8,000–$9,000).\nLow Counts: Some segments have very few borrowers (like DEBTCONSOLIDATION–OWN with a count of only 62), which may not be reliable for broad conclusions.\n\nFrom the perspective of building an automated decision system, these patterns hint at where the bank’s profit opportunities and risks might lie. For instance, segments with higher average loan amounts but also higher interest rates could be more profitable—but might also carry greater default risk. Tracking how many borrowers fall into each segment (the “count” column) helps ensure the model doesn’t overly focus on small, potentially unrepresentative groups.\n\nsummary_table = (\n    df\n    .groupby(['loan_intent', 'person_home_ownership'], as_index=False)\n    .agg(\n        avg_interest_rate=('loan_int_rate', 'mean'),\n        avg_loan_amount=('loan_amnt', 'mean'),\n        count=('loan_amnt', 'count')  # how many borrowers in each segment\n    )\n)\n\n# Sort by average loan amount (descending) to see which segments get the largest lines of credit\nsummary_table_sorted_by_amount = summary_table.sort_values('avg_loan_amount', ascending=False)\n\nsummary_table_sorted_by_amount\n\n\n\n\n\n\n\n\n\nloan_intent\nperson_home_ownership\navg_interest_rate\navg_loan_amount\ncount\n\n\n\n\n17\nPERSONAL\nOTHER\n11.675714\n12366.666667\n15\n\n\n13\nMEDICAL\nOTHER\n12.745000\n12200.000000\n13\n\n\n5\nEDUCATION\nOTHER\n12.400833\n12142.857143\n14\n\n\n9\nHOMEIMPROVEMENT\nOTHER\n11.683000\n10959.090909\n11\n\n\n8\nHOMEIMPROVEMENT\nMORTGAGE\n10.613916\n10764.017341\n1384\n\n\n20\nVENTURE\nMORTGAGE\n10.468000\n10606.281060\n1811\n\n\n0\nDEBTCONSOLIDATION\nMORTGAGE\n10.400489\n10588.756111\n1841\n\n\n4\nEDUCATION\nMORTGAGE\n10.554563\n10502.178076\n2089\n\n\n12\nMEDICAL\nMORTGAGE\n10.505553\n10485.867052\n1730\n\n\n16\nPERSONAL\nMORTGAGE\n10.426285\n10481.223233\n1868\n\n\n21\nVENTURE\nOTHER\n12.274211\n10367.500000\n20\n\n\n11\nHOMEIMPROVEMENT\nRENT\n11.812287\n10109.604633\n1252\n\n\n1\nDEBTCONSOLIDATION\nOTHER\n11.566667\n9783.333333\n15\n\n\n14\nMEDICAL\nOWN\n10.749097\n9367.755682\n352\n\n\n10\nHOMEIMPROVEMENT\nOWN\n10.922405\n9242.450980\n255\n\n\n6\nEDUCATION\nOWN\n10.797507\n8996.177184\n412\n\n\n18\nPERSONAL\nOWN\n10.867262\n8944.209040\n354\n\n\n3\nDEBTCONSOLIDATION\nRENT\n11.358223\n8882.754425\n2260\n\n\n19\nPERSONAL\nRENT\n11.535415\n8826.900046\n2171\n\n\n23\nVENTURE\nRENT\n11.438455\n8804.566745\n2135\n\n\n22\nVENTURE\nOWN\n10.560000\n8789.621914\n648\n\n\n7\nEDUCATION\nRENT\n11.315216\n8685.308193\n2612\n\n\n15\nMEDICAL\nRENT\n11.422580\n8426.925182\n2740\n\n\n2\nDEBTCONSOLIDATION\nOWN\n14.432909\n7749.193548\n62"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#training-and-evaluating-our-logistic-regression-model",
    "href": "posts/linear_score_function-Banks/index.html#training-and-evaluating-our-logistic-regression-model",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Training and evaluating our Logistic Regression model",
    "text": "Training and evaluating our Logistic Regression model\nI used a logistic regression model to predict whether a prospective borrower will default on a loan. After preprocessing the data—by standardizing numerical features and one-hot encoding categorical variables. I removed rows with missing values, and split the dataset into training and test sets. The model achieved a test accuracy of about 84.2%.\nThe confusion matrix provides additional insight into the model’s performance:\n\nTrue Negatives (TN): 3,393 borrowers who did not default and were correctly predicted as non-default.\nFalse Positives (FP): 221 borrowers who did not default but were incorrectly flagged as defaults.\nFalse Negatives (FN): 501 borrowers who defaulted but were missed by the model.\nTrue Positives (TP): 467 borrowers who defaulted and were correctly identified.\n\nOur results suggest that while the model performs reasonably well overall, there is still a balance to be struck between avoiding false positives and false negatives. This is particularly important when designing an automated decision system for credit risk, because both profitability for the bank and equitable access to credit are critical. Further tuning of the threshold and exploration of additional features could help optimize the model even further for its intended purpose\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Drop rows with missing values\ndf_train = df_train.dropna(subset=numeric_features + categorical_features)\n\n\n\n\ntarget = 'loan_status'\nX = df_train.drop(columns=[target])\ny = df_train[target]\n\nnumeric_features = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', \n                      'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\ncategorical_features = ['person_home_ownership', 'loan_intent', 'cb_person_default_on_file']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n    ])\nX_transformed = preprocessor.fit_transform(X)\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, test_size=0.2, random_state=123\n)\n\n# Fit a logistic regression model using the preprocessed features\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(\"Test Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\nTest Accuracy: 0.8424268878219118\nConfusion Matrix:\n [[3393  221]\n [ 501  467]]"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#finding-weight-and-threshold",
    "href": "posts/linear_score_function-Banks/index.html#finding-weight-and-threshold",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Finding weight and threshold",
    "text": "Finding weight and threshold\n\ndf['loan_int_rate_decimal'] = df['loan_int_rate'] / 100.0\n\ndef profit_if_repaid(loan_amnt, loan_int_rate_decimal):\n    \"\"\"\n    If the loan is repaid in full, the bank's profit is:\n      loan_amnt * (1 + 0.25*loan_int_rate)^10 - loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 10 - loan_amnt\n\ndef profit_if_default(loan_amnt, loan_int_rate_decimal):\n    \"\"\"\n    If the borrower defaults, we assume default happens 3 years into the loan, \n    and the bank loses 70% of the principal:\n      loan_amnt*(1 + 0.25*loan_int_rate)^3 - 1.7*loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 3 - 1.7 * loan_amnt\n\nWe will now compute predicted probabilities (probability of default) for the training data\n\ny_prob_train = model.predict_proba(X_train)[:, 1]  # column 1 = probability of default\n\n\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)\nloan_amnt_array = X_train['loan_amnt'].to_numpy()\nloan_int_rate_array = X_train['loan_int_rate_decimal'].to_numpy()\ny_train_array = y_train.to_numpy()\nprofit_repaid = profit_if_repaid(loan_amnt_array, loan_int_rate_array)\nprofit_default = profit_if_default(loan_amnt_array, loan_int_rate_array)\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'loan_int_rate_decimal'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\n/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb Cell 22 line 6\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; X_train, X_test, y_train, y_test = train_test_split(\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt;     X, y, test_size=0.2, random_state=123\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt; )\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt; loan_amnt_array = X_train['loan_amnt'].to_numpy()\n----&gt; &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt; loan_int_rate_array = X_train['loan_int_rate_decimal'].to_numpy()\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; y_train_array = y_train.to_numpy()\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt; profit_repaid = profit_if_repaid(loan_amnt_array, loan_int_rate_array)\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/frame.py:4102, in DataFrame.__getitem__(self, key)\n   4100 if self.columns.nlevels &gt; 1:\n   4101     return self._getitem_multilevel(key)\n-&gt; 4102 indexer = self.columns.get_loc(key)\n   4103 if is_integer(indexer):\n   4104     indexer = [indexer]\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-&gt; 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 'loan_int_rate_decimal'\n\n\n\nNow we are going to find a threshold and identify teh best threshold\n\n# 5. Sweep over thresholds to find the one that maximizes average profit\n\nthresholds = np.linspace(0, 1, 101)\navg_profits = []\n\nfor t in thresholds:\n    # Predict default if probability &gt;= t\n    predicted_default = (y_prob_train &gt;= t).astype(int)\n      # If we predict default, we do NOT give the loan =&gt; profit = 0\n    # If we predict no default, we DO give the loan =&gt; actual profit depends on y_train_array\n    #    - If actual y=0 (no default), profit = profit_repaid\n    #    - If actual y=1 (default), profit = profit_default\n    give_loan = 1 - predicted_default  # 1 = give loan, 0 = no loan\n    # total_profit[i] = give_loan[i] * [ (1 - y[i])*profit_repaid[i] + y[i]*profit_default[i] ]\n    total_profit = give_loan * ((1 - y_train_array) * profit_repaid + y_train_array * profit_default)\n    \n    # Compute average profit per borrower\n    avg_profit = total_profit.mean()\n    avg_profits.append(avg_profit)\n\navg_profits = np.array(avg_profits)\n\n# 6. Identify the best threshold\n\nbest_idx = np.argmax(avg_profits) # index of the best threshold\nbest_threshold = thresholds[best_idx]\n\n\nprint(f\"Best Threshold: {best_threshold:.3f}\")\n\nBest Threshold: 1.000\n\n\n\n# 7. Plot profit vs. threshold\n\nplt.figure(figsize=(8, 5))\nplt.plot(thresholds, avg_profits, label='Profit per Borrower')\nplt.scatter(best_threshold, best_profit, color='red', zorder=10, label='Optimal Threshold')\nplt.title('Profit per Borrower (Training Set) vs. Threshold')\nplt.xlabel('Threshold (Probability of Default)')\nplt.ylabel('Average Profit per Borrower')\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#plotting-risk-score-percentiles-against-mean-number-of-active-chronic-conditions-within-that-percentile",
    "href": "posts/discecting_racial_bias_algo/index.html#plotting-risk-score-percentiles-against-mean-number-of-active-chronic-conditions-within-that-percentile",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Plotting risk score percentiles against mean number of active chronic conditions within that percentile",
    "text": "Plotting risk score percentiles against mean number of active chronic conditions within that percentile\nOur code below shows us a plot explaining how the algorithm’s risk score percentile (y-axis) increases as the average number of chronic illnesses (x-axis) goes up, with points colored by race. Generally, patients with more chronic illnesses receive a higher risk score.\nHowever, if a Black patient and a White patient both have the same chronic illnesses, the Black patient often ends up with a lower risk score than the White patient. Because the care management program looks for patients with high risk scores, the Black patient with the same health conditions is less likely to be flagged for extra care—this is the key concern the Obermeyer et al. (2019) highlights.\n\nimport numpy as np\nimport seaborn as sns\n\ndf['risk_percentile'] = df['risk_score_t'].rank(pct=True) * 100\ndf['risk_p_bin'] = df['risk_percentile'].round() # Rounding it to the nearest integer\n\n# Group by race & risk percentile; compute mean chronic illnesses\ngrouped = (\n    df.groupby(['race', 'risk_p_bin'], as_index=False)\n      .agg(mean_chronic=('gagne_sum_t', 'mean'))\n)\n\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 7))\n\nsns.scatterplot(\n    data=grouped,\n    x='mean_chronic',     \n    y='risk_p_bin',       \n    hue='race',\n    alpha=0.8\n)\n\n\nplt.xlabel(\"Mean Number of Chronic Illnesses\")\nplt.ylabel(\"Risk Score Percentile (binned)\")\nplt.title(\"Risk Score Percentile vs. Mean Chronic Illnesses by Race\")\nplt.legend(title=\"Race\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#visualizing-the-relationship-between-risk-score-chronic-illness-and-healthcare-costs",
    "href": "posts/discecting_racial_bias_algo/index.html#visualizing-the-relationship-between-risk-score-chronic-illness-and-healthcare-costs",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Visualizing the Relationship Between Risk Score, Chronic Illness, and Healthcare Costs",
    "text": "Visualizing the Relationship Between Risk Score, Chronic Illness, and Healthcare Costs\nSomething we notice in both our graphs is that White patients (displayed in orange) tend to generate higher medical costs than Black patients (displayed as blue dots) at similar risk score percentiles or similar numbers of chronic illnesses. This finding aligns with the paper’s discussion of a “wedge” between needing care and receiving (or using) care: even when Black and White patients have the same burden of chronic illnesses, White patients often end up with higher total expenditures.\nIt’s also notable that the vast majority of patients have relatively few chronic conditions . Only a small subset of patients have 10 or more illnesses, yet these high-illness groups appear to have an influence on total costs. This illustrates how a minority of patients with many chronic conditions can account for a disproportionately large share of healthcare spending.\n\n# Group for percentile risk score\ngrouped_risk = (\n    df.groupby(['race', 'risk_p_bin'], as_index=False)\n      .agg(mean_cost=('cost_t', 'mean'))\n)\n# Group for numbe rof chronic illness\ngrouped_chronic = (\n    df.groupby(['race', 'gagne_sum_t'], as_index=False)\n      .agg(mean_cost=('cost_t', 'mean'))\n)\n\n# fitting the  subplots, side by side\nfig, axes = plt.subplots(ncols=2, figsize=(12, 5))\n\n# plotting the percentile risk score\nsns.scatterplot(\n    data=grouped_risk,\n    x='risk_p_bin',\n    y='mean_cost',\n    hue='race',\n    alpha=0.8,\n    ax=axes[0]\n)\naxes[0].set_title(\"Mean Expenditure vs. Risk Score Percentile\")\naxes[0].set_xlabel(\"Percentile Risk Score\")\naxes[0].set_ylabel(\"Total Medical Expenditure\")\naxes[0].set_xticks([0, 20, 40, 60, 80, 100])  # Custom x-ticks\n\n# using a log scale for the y-axis\naxes[0].set_yscale('log')\n\n# plotting the number of chronic illnesses\nsns.scatterplot(\n    data=grouped_chronic,\n    x='gagne_sum_t',\n    y='mean_cost',\n    hue='race',\n    alpha=0.8,\n    ax=axes[1]\n)\naxes[1].set_title(\"Mean Expenditure vs. Number of Chronic Illnesses\")\naxes[1].set_xlabel(\"Number of Chronic Illnesses\")\naxes[1].set_ylabel(\"Total Medical Expenditure\")\n\n\n# Custom x-ticks\naxes[1].set_xticks([0, 5, 10, 15])\n\n# using a log scale for the y-axis\naxes[1].set_yscale('log')\n\naxes[1].legend(title=\"Race\", loc=\"upper left\", bbox_to_anchor=(1, 1))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn our code below we calculate percentage of patients with 5 or fewer chronic conditions and see that about 90% of patients have five or fewer chronic conditions, so by focusing on this group, you’re still covering the vast majority of the dataset. That makes it a reasonable choice: it simplifies the analysis while still capturing most patients’ experiences.\n\n# 2. Remove patients with $0 medical costs because log(0) is undefined\ndf = df[df['cost_t'] &gt; 0]\n\n# Determine the percentage of patients with 5 or fewer chronic conditions\ntotal_patients = df.shape[0]\npatients_5_or_fewer = df[df['gagne_sum_t'] &lt;= 5].shape[0]\npercentage_5_or_fewer = (patients_5_or_fewer / total_patients) * 100\nprint(\"Percentage of patients with 5 or fewer chronic conditions: \", (percentage_5_or_fewer))\n\nPercentage of patients with 5 or fewer chronic conditions:  89.79413053000438"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#data-preparation-before-modeling",
    "href": "posts/discecting_racial_bias_algo/index.html#data-preparation-before-modeling",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Data preparation before modeling",
    "text": "Data preparation before modeling\n\n# 2. Remove patients with $0 medical costs because log(0) is undefined\ndf = df[df['cost_t'] &gt; 0]\n\n# Determine the percentage of patients with 5 or fewer chronic conditions\ntotal_patients = df.shape[0]\npatients_5_or_fewer = df[df['gagne_sum_t'] &lt;= 5].shape[0]\npercentage_5_or_fewer = (patients_5_or_fewer / total_patients) * 100\nprint(\"Percentage of patients with 5 or fewer chronic conditions: \", (percentage_5_or_fewer))\n\nPercentage of patients with 5 or fewer chronic conditions:  89.79413053000438\n\n\nWe see that about 90% of patients have five or fewer chronic conditions, so by focusing on this group, you’re still covering the vast majority of the dataset. That makes it a reasonable choice: it simplifies the analysis while still capturing most patients’ experiences."
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#modeling-our-data",
    "href": "posts/discecting_racial_bias_algo/index.html#modeling-our-data",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Modeling our Data",
    "text": "Modeling our Data\nIn our code belwo we perform three main steps to prepare our data for modeling:\n\nLog-Transforming Costs: We create a new column log_cost by taking the natural log of cost_t. This transformation helps manage the large range of healthcare expenses, making it easier for a regression model to handle.\nOne-Hot Encoding Race: We introduce a race_dummy variable where Black patients are assigned a value of 1 and White patients a value of 0. Turning the categorical race variable into a numeric format allows the model to incorporate race as a predictor.\nDefining Predictors and Target: Our predictor variables (X) are the dummy-coded race variable (race_dummy) and the total number of chronic conditions (gagne_sum_t). Our target variable (y) is the log-transformed cost (log_cost).\n\n\n#log-transform of the cost\ndf['log_cost'] = np.log(df['cost_t'])\n\n# one-hot encoding race\ndf['race_dummy'] = (df['race'] == 'black').astype(int)\n\n#Separate the data into predictor variables (X) and the target variable (y)\n#   For predictors, we use the onehot encoded race variavle and the number of chronic conditions.\nX = df[['race_dummy', 'gagne_sum_t']]\ny = df['log_cost']\n\nprint(X['race_dummy'].value_counts())\n\nprint(\"Predictor variables x\")\n\nprint(X.head()) \n\nprint(\"Target variable y\")\n\nprint(y.head())\n\nrace_dummy\n0    5851\n1     998\nName: count, dtype: int64\nPredictor variables x\n    race_dummy  gagne_sum_t\n1            0            3\n8            1            1\n15           0            1\n19           0            0\n21           0            2\nTarget variable y\n1     7.863267\n8     6.907755\n15    7.313220\n19    8.594154\n21    8.318742\nName: log_cost, dtype: float64"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#modeling-our-cost-disparity",
    "href": "posts/discecting_racial_bias_algo/index.html#modeling-our-cost-disparity",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Modeling our Cost Disparity",
    "text": "Modeling our Cost Disparity\nIn our code below we built our model to assess the cost disparity. So we do this by building a polynomial regression model to explore the relationship between the number of chronic illnesses, race, and healthcare costs. In order to this we had to do the following steps\n\nTransforming the Data: We applied a log transformation to the cost variable to handle its wide range of values. In addition to that We one-hot encoded race, where Black patients were assigned 1 and White patients were assigned 0.\nGenerating Polynomial Features: Since the relationship between chronic conditions and cost was not linear, we created polynomial features up to degree 11 to capture nonlinearity.\nHyperparameter Tuning: We tested different polynomial degrees (1 to 11) and regularization strengths (Ridge regression with \\(\\alpha = 10^k\\) for \\(k = -4, -3, \\dots, 3, 4\\)). Using cross-validation, we identified the combination that minimized the mean squared error (MSE).\nFitting the Final Model: Once we found the best degree (9) and regularization strength (\\(\\alpha = 10\\)), we trained our final Ridge regression model. We extracted the coefficient for race (\\(w_b\\)), which indicates the impact of being Black on predicted log-cost. We computed \\(e^{w_b}\\), which tells us the relative cost of Black patients compared to White patients with similar illness burdens.\n\n\nimport warnings\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import Ridge\n\ndef add_polynomial_features(X, degree):\n    \"\"\"\n    Adds polynomial terms for 'gagne_sum_t' up to the specified degree.\n    \"\"\"\n    X_ = X.copy()\n\n    for j in range(1, degree + 1):\n        X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"] ** j\n    return X_\n\n# create a hyperparameter grid for the polynomial degree\ndegrees = range(1, 12)               # 1 through 11\nalphas = [10**k for k in range(-4, 5)]  # 10^-4 through 10^4\n\n# Initialize the best score to positive infinity and the best parameters to None\n# cross-validation will update these variables\nbest_score = np.inf\nbest_params = (None, None)  # (degree, alpha)\n\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n\n    for deg in degrees:\n        # Create polynomial features up to 'deg'\n        X_poly = add_polynomial_features(X[['gagne_sum_t']], deg)\n        # we include the race dummy column as well\n        X_poly['race_dummy'] = X['race_dummy']\n        \n        for alpha in alphas:\n            # creating a Ridge model with the given alpha\n            model = Ridge(alpha=alpha)\n            \n            # 5-fold cross-validation using negative MSE (sklearn uses negative MSE by default)\n            scores = cross_val_score(model, X_poly, y, cv=5, scoring='neg_mean_squared_error')\n            mean_mse = -np.mean(scores)  # convert negative MSE to MSE\n\n            # Update if we find a better (lower) MSE\n            if mean_mse &lt; best_score:\n                best_score = mean_mse\n                best_params = (deg, alpha)\n\nprint(\"Best degree and alpha:\", best_params)\nprint(\"Cross Validated MSE:\", best_score)\n\n# final model\nbest_degree = best_params[0]\nbest_alpha = best_params[1]\n\n# Build the final model using the polynomial degree and alpha that gave the best MSE\nX_final = add_polynomial_features(X[['gagne_sum_t']], best_degree)\nX_final['race_dummy'] = X['race_dummy']\n\n# Fit the final model\nfinal_model = Ridge(alpha=best_alpha)\nfinal_model.fit(X_final, y)\n\ncoef_names = list(X_final.columns)\ncoefs = final_model.coef_\n\nrace_index = coef_names.index(\"race_dummy\")\nrace_coef = coefs[race_index]\n\n# Compute e^(wb)\nrace_factor = np.exp(race_coef)\n\nprint(\"Race factor (e^(wb)):\", race_factor)\n\nBest degree and alpha: (9, 10)\nCross Validated MSE: 1.206333568523416\nRace factor (e^(wb)): 0.8095004382057049\n\n\n/Users/prashanthbabu/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:215: LinAlgWarning: Ill-conditioned matrix (rcond=2.53171e-22): result may not be accurate.\n  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#interpretting-our-results",
    "href": "posts/discecting_racial_bias_algo/index.html#interpretting-our-results",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Interpretting our results:",
    "text": "Interpretting our results:\nThe best polynomial degree was 9, suggesting a complex, nonlinear relationship between chronic conditions and cost. The regularization strength \\(\\alpha = 10\\) helped control overfitting while preserving useful patterns.\nThe race coefficient (\\(w_b\\)) resulted in a computed value of \\(e^{w_b} \\approx 0.81\\), meaning Black patients incur only \\(\\sim 81\\%\\) of the healthcare costs of equally sick White patients. This result aligns with the argument in Obermeyer et al. (2019)—that healthcare cost data underestimates Black patients’ need for care. Since cost data is used to determine who gets extra healthcare resources, this disparity can lead to biased decision-making, where Black patients may be less likely to be enrolled in high-risk management programs despite having the same number of chronic conditions as White patients.\nIn summary, our model supports the claim that cost-based risk scores do not fully capture healthcare need, and the bias disproportionately affects Black patients."
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#abstract",
    "href": "posts/discecting_racial_bias_algo/index.html#abstract",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "In this project, I aim to replicate and extend the findings of Obermeyer et al. (2019) by exploring how healthcare cost predictions relate to patients’ chronic illness burden and race. Using a randomized dataset, we first visualize the relationship between risk score percentiles, chronic conditions, and medical expenditures, revealing that White patients tend to generate higher costs than Black patients despite similar illness burdens. We then built a Ridge regression model with polynomial features to quantify the disparity in costs between Black and White patients. Our final model estimates that, holding illness burden constant, Black patients incur roughly 81% of the costs incurred by White patients, suggesting that the cost-based risk score underestimates the true care needs of Black patients."
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#exploring-the-data",
    "href": "posts/discecting_racial_bias_algo/index.html#exploring-the-data",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nBelow we will accesss the data clean it and explore the different variables and features.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nOur code below checks the ratio of white patients to black patiens before dropping NA values and after dorpping them. There is clearly a big difference in the ratio of black to white people in our dataset, which is something to keep in mind.\n\nrace_counts_bf_clean = df['race'].value_counts()\ndf.dropna(inplace=True)\nrace_counts = df['race'].value_counts()\nprint(\"Count of patients by race before cleaning data\",race_counts_bf_clean)\nprint(\"Count of patients by race after cleaning data\", race_counts)\n\nCount of patients by race before cleaning data race\nwhite    43202\nblack     5582\nName: count, dtype: int64\nCount of patients by race after cleaning data race\nwhite    5911\nblack    1000\nName: count, dtype: int64\n\n\n\nimport  matplotlib.pyplot as plt\nimport seaborn as sns\n# Graph 1: Bar chart of race counts\nplt.figure(figsize=(6, 4))\nplt.bar(race_counts.index, race_counts.values, color=['blue', 'orange'])\nplt.xlabel('Race')\nplt.ylabel('Number of Patients')\nplt.title('Number of Black vs. White Patients')\nplt.show()\n\n\n\n\n\n\n\n\nBelow is a scatter plot to visualize the Risk Score vs the Cost by race and see a lot of clustering. So we will need to further clean our data to get better visualizations.\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x=\"risk_score_t\", y=\"cost_t\", hue=\"race\", alpha=0.7)\n\n# Labels and title\nplt.xlabel(\"Risk Score\")\nplt.ylabel(\"Cost\")\nplt.title(\"Risk Score vs Cost by Race\")\nplt.show()"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#discussion",
    "href": "posts/discecting_racial_bias_algo/index.html#discussion",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Discussion",
    "text": "Discussion\nOur modeling process used polynomial feature expansion and regularized linear regression which allowed us to capture the nonlinear relationship between chronic conditions and cost. The final model’s race coefficient, suggests that Black patients’ predicted expenditures are about 81% of those of White patients with the same number of chronic conditions. This finding supports the “wedge” hypothesis discussed by Obermeyer et al. (2019): even when Black and White patients are equally sick, Black patients tend to have lower healthcare costs, likely due to systemic inequities in access and utilization of care. In relation to the formal statistical discrimination criteria from Barocas, Hardt, and Narayanan (2023), the bias in this algorithm is best described by a failure of the separation criterion. In an ideal scenario, the classifier (or risk score) should yield equal error rates—that is, for any given risk score, the probability of high health need should be equal across racial groups. Our analysis shows that this is not the case: Black patients, at a given risk score, exhibit a higher burden of chronic illness, indicating that the algorithm’s misclassification rates differ by race. This violation of separation shows how using cost as a proxy for health can introduce systematic bias, ultimately disadvantaging Black patients. Overall, through this project I learned how to build a Ridge regression model using polynomial features, as well as how to tune hyperparameters effectively. This process revealed how these modeling techniques can uncover hidden biases in decision-making algorithms, underscoring the need for more equitable measures when predicting healthcare needs."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#abstract-and-methodologies",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#abstract-and-methodologies",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#loading-the-data",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#loading-the-data",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Loading the data",
    "text": "Loading the data\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-intent-by-age-group",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-intent-by-age-group",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Intent by Age Group",
    "text": "Analysis of Loan Intent by Age Group\nThis bar chart below shows how different age groups use their loans for various purposes—such as venture, education, medical, home improvement, personal, and debt consolidation. We can see that borrowers aged 18–29 make up a large portion of total loans, often driven by education and personal loan needs. As age increases, the number of loans generally decreases, but certain categories—like debt consolidation—can become more common in older groups.\nOverall, this chart highlights that younger borrowers borrow a lot more money may be more focused on educational or personal financing, while older borrowers might shift their attention to consolidating debt or improving their homes. Understanding these patterns help us in understanding different patternns in who would default on a loan and not.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n# Create additional features for analysis:\n# 1. Age groups: we create bins to see how loan intent varies with age.\nage_bins = [18, 30, 40, 50, 60, 100]\nage_labels = ['18-29', '30-39', '40-49', '50-59', '60+']\ndf['age_group'] = pd.cut(df['person_age'], bins=age_bins, labels=age_labels)\n\n# ---------------------------\n# Visualization 1:\n# How does loan intent vary with age and home ownership status?\nplt.figure(figsize=(10, 6))\nsns.countplot(data=df, x='age_group', hue='loan_intent')\nplt.title('Loan Intent Distribution by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Count')\nplt.legend(title='Loan Intent')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Average Loan Amount by Credit History Length",
    "text": "Analysis of Average Loan Amount by Credit History Length\nThis bar chart shows how the average loan amount changes based on the number of years a borrower has had a credit history. In general, we see that some longer lengths of credit history are associated with higher average loan amounts than others, though the pattern isn’t strictly increasing or decreasing. This suggests that lenders may be willing to extend larger lines of credit to individuals with certain credit history profiles.\nFor our automated decision system, credit history length could be an important feature because it often reflects a borrower’s past experience with credit and repayment behavior. However, we must be mindful of fairness and potential biases: borrowers who are younger or newer to credit might be at a disadvantage if the model heavily weighs credit history length. Balancing profitability for the bank with equitable access to credit remains a key challenge in designing our scoring and thresholding methods.\n\n\n# 2. Employment length groups: useful for exploring patterns with job experience.\nemp_bins = [0, 2, 5, 10, df['person_emp_length'].max()]\nemp_labels = ['0-1 yrs', '2-4 yrs', '5-9 yrs', '10+ yrs']\ndf['emp_length_group'] = pd.cut(df['person_emp_length'], bins=emp_bins, labels=emp_labels)\n\n# ---------------------------\n# Visualization 2:\n# Which segments are offered different interest rates? Compare distributions by home ownership.\nplt.figure(figsize=(10, 6))\navg_loan = df.groupby('cb_person_cred_hist_length')['loan_amnt'].mean().reset_index()\nsns.barplot(data=avg_loan, x='cb_person_cred_hist_length', y='loan_amnt')\nplt.title('Average Loan Amount by Credit History Length')\nplt.xlabel('Credit History Length (Years)')\nplt.ylabel('Average Loan Amount')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership\nIn our scatter plot below, each dot represents a borrower, with the x-axis showing how large the loan is relative to their income (as a percentage) and the y-axis showing the absolute loan amount. The colors indicate different types of home ownership (RENT, MORTGAGE, OWN, OTHER).\nAs you can see, the data points overlap heavily, making the chart look cluttered. To get a clearer picture, I wo;; split these data into separate graphs for each home ownership category. This will help us see more nuanced patterns—like whether renters tend to have higher loan-to-income ratios compared to those who own or have a mortgage.\n\ndf['loan_percent_income_pct'] = df['loan_percent_income'] * 100\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=df, \n    x='loan_percent_income_pct', \n    y='loan_amnt', \n    hue='person_home_ownership',\n    alpha=0.7\n)\nplt.title('Loan Amount vs. Loan % of Income by Home Ownership')\nplt.xlabel('Loan as % of Person Income')\nplt.ylabel('Loan Amount')\nplt.xlim(0, 100)  # Focus on 0–100% if most loans fall in this range\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)\nBy splitting the data into four subplots (one for each home ownership category), we can see that:\n\nRenters typically have loan amounts averaging around $10,000–$12,000, with a wide spread of loan-to-income ratios (averaging around 15–20%). In addition to this we see the most clustering for this group which means, in our dataset most of the people taking our loans are from this group.\nMortgage holders often take out larger loans (averaging $16,000–$18,000) but may have lower loan-to-income ratios (closer to 10% on average).\nOwners (those who fully own their homes) tend to borrow moderate amounts ($12,000–$15,000) at ratios of around 12–15%.\nOthers (less common categories) show a broad mix but generally fall between these ranges.\n\nThese distinctions are important for our automated decision system, since each home ownership group presents a different risk and borrowing profile. When designing a score function and threshold to maximize the bank’s profit, it’s important for us to consider whether certain groups (like renters) might be unfairly penalized if they tend to have higher loan-to-income ratios. Ultimately, these separate plots help us fine-tune our model so that we balance profitability with equitable access to credit across different segments of borrowers.\n\n\n\n# Create a FacetGrid: one subplot per home ownership category\ng = sns.FacetGrid(df, col=\"person_home_ownership\", col_wrap=2, height=4)\ng.map(sns.scatterplot, \"loan_percent_income_pct\", \"loan_amnt\", alpha=0.7)\n\n# Set the x-axis limits and labels for clarity\ng.set(xlim=(0, 100))\ng.set_axis_labels(\"Loan as % of Income\", \"Loan Amount\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Summary of Loan Intent and Home Ownership Segments",
    "text": "Summary of Loan Intent and Home Ownership Segments\nThis table shows how different combinations of loan intent (e.g., EDUCATION, MEDICAL, PERSONAL) and home ownership (MORTGAGE, OWN, RENT, OTHER) compare in terms of average interest rate, average loan amount, and count of borrowers. We’ve sorted the table by average loan amount in descending order to identify which segments receive the largest lines of credit.\n\nHighest Averages: Segments like PERSONAL–OTHER and MEDICAL–OTHER appear near the top, suggesting they receive higher loan amounts (over $12,000 on average), but also tend to have higher interest rates (11–12%).\nMortgage vs. Rent: Many MORTGAGE segments (e.g., DEBTCONSOLIDATION–MORTGAGE, EDUCATION–MORTGAGE) cluster in the middle, with average loan amounts around $10,000–$11,000 and interest rates near 10–10.6%. Renters often see slightly higher interest rates (11–12%) and somewhat lower loan amounts (around $8,000–$9,000).\nLow Counts: Some segments have very few borrowers (like DEBTCONSOLIDATION–OWN with a count of only 62), which may not be reliable for broad conclusions.\n\nFrom the perspective of building an automated decision system, these patterns hint at where the bank’s profit opportunities and risks might lie. For instance, segments with higher average loan amounts but also higher interest rates could be more profitable—but might also carry greater default risk. Tracking how many borrowers fall into each segment (the “count” column) helps ensure the model doesn’t overly focus on small, potentially unrepresentative groups.\n\nsummary_table = (\n    df\n    .groupby(['loan_intent', 'person_home_ownership'], as_index=False)\n    .agg(\n        avg_interest_rate=('loan_int_rate', 'mean'),\n        avg_loan_amount=('loan_amnt', 'mean'),\n        count=('loan_amnt', 'count')  # how many borrowers in each segment\n    )\n)\n\n# Sort by average loan amount (descending) to see which segments get the largest lines of credit\nsummary_table_sorted_by_amount = summary_table.sort_values('avg_loan_amount', ascending=False)\n\nsummary_table_sorted_by_amount\n\n\n\n\n\n\n\n\n\nloan_intent\nperson_home_ownership\navg_interest_rate\navg_loan_amount\ncount\n\n\n\n\n17\nPERSONAL\nOTHER\n11.675714\n12366.666667\n15\n\n\n13\nMEDICAL\nOTHER\n12.745000\n12200.000000\n13\n\n\n5\nEDUCATION\nOTHER\n12.400833\n12142.857143\n14\n\n\n9\nHOMEIMPROVEMENT\nOTHER\n11.683000\n10959.090909\n11\n\n\n8\nHOMEIMPROVEMENT\nMORTGAGE\n10.613916\n10764.017341\n1384\n\n\n20\nVENTURE\nMORTGAGE\n10.468000\n10606.281060\n1811\n\n\n0\nDEBTCONSOLIDATION\nMORTGAGE\n10.400489\n10588.756111\n1841\n\n\n4\nEDUCATION\nMORTGAGE\n10.554563\n10502.178076\n2089\n\n\n12\nMEDICAL\nMORTGAGE\n10.505553\n10485.867052\n1730\n\n\n16\nPERSONAL\nMORTGAGE\n10.426285\n10481.223233\n1868\n\n\n21\nVENTURE\nOTHER\n12.274211\n10367.500000\n20\n\n\n11\nHOMEIMPROVEMENT\nRENT\n11.812287\n10109.604633\n1252\n\n\n1\nDEBTCONSOLIDATION\nOTHER\n11.566667\n9783.333333\n15\n\n\n14\nMEDICAL\nOWN\n10.749097\n9367.755682\n352\n\n\n10\nHOMEIMPROVEMENT\nOWN\n10.922405\n9242.450980\n255\n\n\n6\nEDUCATION\nOWN\n10.797507\n8996.177184\n412\n\n\n18\nPERSONAL\nOWN\n10.867262\n8944.209040\n354\n\n\n3\nDEBTCONSOLIDATION\nRENT\n11.358223\n8882.754425\n2260\n\n\n19\nPERSONAL\nRENT\n11.535415\n8826.900046\n2171\n\n\n23\nVENTURE\nRENT\n11.438455\n8804.566745\n2135\n\n\n22\nVENTURE\nOWN\n10.560000\n8789.621914\n648\n\n\n7\nEDUCATION\nRENT\n11.315216\n8685.308193\n2612\n\n\n15\nMEDICAL\nRENT\n11.422580\n8426.925182\n2740\n\n\n2\nDEBTCONSOLIDATION\nOWN\n14.432909\n7749.193548\n62"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#training-and-evaluating-our-logistic-regression-model",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#training-and-evaluating-our-logistic-regression-model",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Training and evaluating our Logistic Regression model",
    "text": "Training and evaluating our Logistic Regression model\nI used a logistic regression model to predict whether a prospective borrower will default on a loan. After preprocessing the data—by standardizing numerical features and one-hot encoding categorical variables. I removed rows with missing values, and split the dataset into training and test sets. The model achieved a test accuracy of about 84.2%.\nThe confusion matrix provides additional insight into the model’s performance:\n\nTrue Negatives (TN): 3,393 borrowers who did not default and were correctly predicted as non-default.\nFalse Positives (FP): 221 borrowers who did not default but were incorrectly flagged as defaults.\nFalse Negatives (FN): 501 borrowers who defaulted but were missed by the model.\nTrue Positives (TP): 467 borrowers who defaulted and were correctly identified.\n\nOur results suggest that while the model performs reasonably well overall, there is still a balance to be struck between avoiding false positives and false negatives. This is particularly important when designing an automated decision system for credit risk, because both profitability for the bank and equitable access to credit are critical. Further tuning of the threshold and exploration of additional features could help optimize the model even further for its intended purpose\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Drop rows with missing values\ndf_train = df_train.dropna(subset=numeric_features + categorical_features)\n\n\n\n\ntarget = 'loan_status'\nX = df_train.drop(columns=[target])\ny = df_train[target]\n\nnumeric_features = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', \n                      'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\ncategorical_features = ['person_home_ownership', 'loan_intent', 'cb_person_default_on_file']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n    ])\nX_transformed = preprocessor.fit_transform(X)\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, test_size=0.2, random_state=123\n)\n\n# Fit a logistic regression model using the preprocessed features\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(\"Test Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\nTest Accuracy: 0.8424268878219118\nConfusion Matrix:\n [[3393  221]\n [ 501  467]]"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#finding-weight-and-threshold",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#finding-weight-and-threshold",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Finding weight and threshold",
    "text": "Finding weight and threshold\n\ndf['loan_int_rate_decimal'] = df['loan_int_rate'] / 100.0\n\ndef profit_if_repaid(loan_amnt, loan_int_rate_decimal):\n    \"\"\"\n    If the loan is repaid in full, the bank's profit is:\n      loan_amnt * (1 + 0.25*loan_int_rate)^10 - loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 10 - loan_amnt\n\ndef profit_if_default(loan_amnt, loan_int_rate_decimal):\n    \"\"\"\n    If the borrower defaults, we assume default happens 3 years into the loan, \n    and the bank loses 70% of the principal:\n      loan_amnt*(1 + 0.25*loan_int_rate)^3 - 1.7*loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 3 - 1.7 * loan_amnt\n\nWe will now compute predicted probabilities (probability of default) for the training data\n\ny_prob_train = model.predict_proba(X_train)[:, 1]  # column 1 = probability of default\n\n\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)\nloan_amnt_array = X_train['loan_amnt'].to_numpy()\nloan_int_rate_array = X_train['loan_int_rate_decimal'].to_numpy()\ny_train_array = y_train.to_numpy()\nprofit_repaid = profit_if_repaid(loan_amnt_array, loan_int_rate_array)\nprofit_default = profit_if_default(loan_amnt_array, loan_int_rate_array)\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'loan_int_rate_decimal'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\n/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb Cell 22 line 6\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; X_train, X_test, y_train, y_test = train_test_split(\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt;     X, y, test_size=0.2, random_state=123\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt; )\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt; loan_amnt_array = X_train['loan_amnt'].to_numpy()\n----&gt; &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt; loan_int_rate_array = X_train['loan_int_rate_decimal'].to_numpy()\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; y_train_array = y_train.to_numpy()\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt; profit_repaid = profit_if_repaid(loan_amnt_array, loan_int_rate_array)\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/frame.py:4102, in DataFrame.__getitem__(self, key)\n   4100 if self.columns.nlevels &gt; 1:\n   4101     return self._getitem_multilevel(key)\n-&gt; 4102 indexer = self.columns.get_loc(key)\n   4103 if is_integer(indexer):\n   4104     indexer = [indexer]\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-&gt; 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 'loan_int_rate_decimal'\n\n\n\nNow we are going to find a threshold and identify teh best threshold\n\n# 5. Sweep over thresholds to find the one that maximizes average profit\n\nthresholds = np.linspace(0, 1, 101)\navg_profits = []\n\nfor t in thresholds:\n    # Predict default if probability &gt;= t\n    predicted_default = (y_prob_train &gt;= t).astype(int)\n      # If we predict default, we do NOT give the loan =&gt; profit = 0\n    # If we predict no default, we DO give the loan =&gt; actual profit depends on y_train_array\n    #    - If actual y=0 (no default), profit = profit_repaid\n    #    - If actual y=1 (default), profit = profit_default\n    give_loan = 1 - predicted_default  # 1 = give loan, 0 = no loan\n    # total_profit[i] = give_loan[i] * [ (1 - y[i])*profit_repaid[i] + y[i]*profit_default[i] ]\n    total_profit = give_loan * ((1 - y_train_array) * profit_repaid + y_train_array * profit_default)\n    \n    # Compute average profit per borrower\n    avg_profit = total_profit.mean()\n    avg_profits.append(avg_profit)\n\navg_profits = np.array(avg_profits)\n\n# 6. Identify the best threshold\n\nbest_idx = np.argmax(avg_profits) # index of the best threshold\nbest_threshold = thresholds[best_idx]\n\n\nprint(f\"Best Threshold: {best_threshold:.3f}\")\n\nBest Threshold: 1.000\n\n\n\n# 7. Plot profit vs. threshold\n\nplt.figure(figsize=(8, 5))\nplt.plot(thresholds, avg_profits, label='Profit per Borrower')\nplt.scatter(best_threshold, best_profit, color='red', zorder=10, label='Optimal Threshold')\nplt.title('Profit per Borrower (Training Set) vs. Threshold')\nplt.xlabel('Threshold (Probability of Default)')\nplt.ylabel('Average Profit per Borrower')\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Auditing Bias/index.html",
    "href": "posts/Auditing Bias/index.html",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "",
    "text": "The ACS PUMS dataset, collected by the U.S. Census Bureau, provides detailed demographic and employment data for thousands of individuals across the United States. In this blog post, we build a machine learning classifier to predict whether an individual is employed using demographic features such as age, education, and marital status. While race is excluded from the predictive features, it is retained as a group label to enable a comprehensive bias audit. By leveraging logistic regression with polynomial feature expansion and tuning model complexity via grid search, our classifier achieved an overall accuracy of approximately 82%. Our analysis not only demonstrates the predictive power of data-driven approaches in employment forecasting but also reveals disparities in error rates and predicted outcomes across racial groups, underscoring the critical need for fairness auditing before deploying such models in commercial or governmental settings."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#abstract",
    "href": "posts/Auditing Bias/index.html#abstract",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "",
    "text": "The ACS PUMS dataset, collected by the U.S. Census Bureau, provides detailed demographic and employment data for thousands of individuals across the United States. In this blog post, we build a machine learning classifier to predict whether an individual is employed using demographic features such as age, education, and marital status. While race is excluded from the predictive features, it is retained as a group label to enable a comprehensive bias audit. By leveraging logistic regression with polynomial feature expansion and tuning model complexity via grid search, our classifier achieved an overall accuracy of approximately 82%. Our analysis not only demonstrates the predictive power of data-driven approaches in employment forecasting but also reveals disparities in error rates and predicted outcomes across racial groups, underscoring the critical need for fairness auditing before deploying such models in commercial or governmental settings."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#goal-of-project",
    "href": "posts/Auditing Bias/index.html#goal-of-project",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Goal of Project",
    "text": "Goal of Project\nIn this project, we are going to build a machine learning classifier that predicts whether an individual is employed using demographic data from the ACS PUMS dataset. We will exclude race from the features used for prediction while retaining it as a group label to later audit the model for racial bias. This approach helps us explore whether the classifier inadvertently displays disparate performance across racial groups. In addition to this, we’ll tune the model’s complexity (using polynomial features with logistic regression) and perform a bias audit, examining overall and subgroup error rates, and statistical parity. Our findings will audit the bias of predictive models when applied to real-world census data."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#loading-data",
    "href": "posts/Auditing Bias/index.html#loading-data",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Loading Data",
    "text": "Loading Data\nUsing the folktables package, we will download the ACS PUMS data, which provides demographic and employment information collected by the U.S. Census. For our project, we select data for Michigan from the 2018 1-Year survey. In our code snippet below we will load the data:\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MI\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\nDownloading data for 2018 1-Year person survey for MI...\n\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000064\n3\n1\n2907\n2\n26\n1013097\n8\n60\n...\n9\n0\n12\n9\n11\n9\n0\n9\n10\n12\n\n\n1\nP\n2018GQ0000154\n3\n1\n1200\n2\n26\n1013097\n92\n20\n...\n92\n91\n93\n95\n93\n173\n91\n15\n172\n172\n\n\n2\nP\n2018GQ0000158\n3\n1\n2903\n2\n26\n1013097\n26\n54\n...\n26\n52\n3\n25\n25\n28\n28\n50\n51\n25\n\n\n3\nP\n2018GQ0000174\n3\n1\n1801\n2\n26\n1013097\n86\n20\n...\n85\n12\n87\n12\n87\n85\n157\n86\n86\n86\n\n\n4\nP\n2018GQ0000212\n3\n1\n2600\n2\n26\n1013097\n99\n33\n...\n98\n96\n98\n95\n174\n175\n96\n95\n179\n97\n\n\n\n\n5 rows × 286 columns"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#feature-selection",
    "href": "posts/Auditing Bias/index.html#feature-selection",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Feature Selection",
    "text": "Feature Selection\nBelow we are going to pick a list of possible feautres that would be used for modeling. We Exclude Target & Group Features: Remove ESR (employment status, which is our target) and RAC1P (race, used later for bias auditing) from the features list. We then subset the data: By focusing on a smaller set of relevant features, we simplify the model and focus on the factors that are most informative for predicting employment.\n\npossible_features = ['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nprint(acs_data[possible_features].head())\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n   AGEP  SCHL  MAR  RELP  DIS  ESP  CIT  MIG  MIL  ANC  NATIVITY  DEAR  DEYE  \\\n0    60  15.0    5    17    1  NaN    1  1.0  4.0    1         1     2     2   \n1    20  19.0    5    17    2  NaN    1  1.0  4.0    2         1     2     2   \n2    54  18.0    3    16    1  NaN    1  1.0  4.0    4         1     2     2   \n3    20  18.0    5    17    2  NaN    1  1.0  4.0    4         1     2     2   \n4    33  18.0    5    16    2  NaN    1  3.0  4.0    2         1     2     2   \n\n   DREM  SEX  RAC1P  ESR  \n0   1.0    1      2  6.0  \n1   2.0    2      1  6.0  \n2   1.0    1      1  6.0  \n3   2.0    1      1  6.0  \n4   2.0    1      1  6.0"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#chosing-our-problem",
    "href": "posts/Auditing Bias/index.html#chosing-our-problem",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Chosing our Problem",
    "text": "Chosing our Problem\nProblem Choice: In this project we will be predicting whether an individual is employed (target variable ESR) using demographic features—such as age, education, marital status, and others—while deliberately excluding race (RAC1P) from the predictors. However, we keep the race information aside as the group label. This allows us to later perform a bias audit by comparing model performance across different racial groups. This setup enables us to build a classifier that does not directly use race during training, while still allowing us to measure if the predictions favor one racial group over another helping us assess the bias.\nThe code below doesteh following steps\n\nDefines the Prediction Task: We use the BasicProblem class to specify which features to use, the target variable, and how to transform the target (converting ESR values into a binary label).\nWe then Extract the Data: The data is converted into a feature matrix, a label vector, and a group vector.\nAfter that we do our Train-Test Split: The data is split into training and test sets so that we can train our model and later evaluate its performance.\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# 3. Define the Prediction Task using BasicProblem\n# Here, we aim to predict employment status. The target_transform converts the raw ESR value\n# to a binary label (1 for employed if ESR == 1, else 0).\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',          # Use race as the group label for bias analysis later.\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1)\n)\n\n# Extract the feature matrix, binary labels, and group indicator from the DataFrame.\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\nprint(\"Features shape:\", features.shape)\n# 4. Train-Test Split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\nprint(\"Training set size:\", X_train.shape[0])\nprint(\"Test set size:\", X_test.shape[0])\n\n\nFeatures shape: (99419, 15)\nTraining set size: 79535\nTest set size: 19884"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#descriptive-analysis",
    "href": "posts/Auditing Bias/index.html#descriptive-analysis",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Descriptive Analysis",
    "text": "Descriptive Analysis\nOur code below converts the training data into a DataFrame to calculate some basic statistics. We find that there are 79,535 individuals in total. Overall, about 44.3% of these individuals are employed. When we break the data down by the race group (held in the “group” column), we see that Group 1 has 67,415 individuals, Group 2 has 6,881 individuals, and the remaining groups have much smaller numbers. Looking at employment rates within each group, about 45.5% of Group 1 are employed, while Group 2 has a lower rate at around 34.6%. The other groups show employment proportions between roughly 33% and 49%. This simple analysis helps us understand the size and distribution of our data, which is essential for building a fair and well-informed mode\n\n# 5. Descriptive Analysis\n# Convert training data into a DataFrame for easy descriptive analysis.\n\ndf = pd.DataFrame(X_train, columns=features_to_use)\ndf[\"group\"] = group_train  # This is the race indicator\ndf[\"label\"] = y_train      # 1 if employed, 0 otherwise\n\n# Explore the different statistics\ntotal_individuals = df.shape[0]\npositive_proportion = df[\"label\"].mean()\ngroup_counts = df[\"group\"].value_counts()\ngroup_positive = df.groupby(\"group\")[\"label\"].mean()\n\nprint(\"Total Individuals:\", total_individuals)\nprint(\"Overall Proportion Employed:\", positive_proportion)\nprint(\"Number of Individuals by Group:\\n\", group_counts)\nprint(\"Employment Proportion by Group:\\n\", group_positive)\n\nTotal Individuals: 79535\nOverall Proportion Employed: 0.44297479097252784\nNumber of Individuals by Group:\n group\n1    67415\n2     6881\n6     2061\n9     1922\n8      670\n3      467\n5       95\n7       24\nName: count, dtype: int64\nEmployment Proportion by Group:\n group\n1    0.454825\n2    0.345880\n3    0.419700\n5    0.400000\n6    0.492479\n7    0.458333\n8    0.444776\n9    0.328824\nName: label, dtype: float64\n\n\nWe also the “SEX” attribute and calculate employment rates for each race-sex combination. For example, in Group 1, males are employed at about 45.6% and females at 45.4%, while in Group 2, rates are around 34.4% for males and 34.8% for females. This helps identify any intersectional disparities.\n\n# Intersectional analysis: using an additional sensitive attribute (e.g., SEX)\n# Assuming the original acs_data contains the SEX column:\ndf[\"sex\"] = acs_data[\"SEX\"]\nintersection = df.groupby([\"group\", \"sex\"])[\"label\"].mean().reset_index()\nprint(\"Intersectional Employment Proportions:\\n\", intersection)\n\nIntersectional Employment Proportions:\n     group  sex     label\n0       1    1  0.455874\n1       1    2  0.453794\n2       2    1  0.344051\n3       2    2  0.347688\n4       3    1  0.384615\n5       3    2  0.451220\n6       5    1  0.428571\n7       5    2  0.369565\n8       6    1  0.486829\n9       6    2  0.498069\n10      7    1  0.416667\n11      7    2  0.500000\n12      8    1  0.448753\n13      8    2  0.440129\n14      9    1  0.339112\n15      9    2  0.319319\n\n\nOur Graph below plots the proprtions of employment amongst all the different races. This bar chart shows the proportion of employed individuals broken down by race group and sex. Each pair of bars compares males (blue) and females (orange) within the same race group. By comparing the heights of the bars, we can quickly see people of white have higher employment rates compared to people of black race who contrarily have this graph aslo shows us whether there are notable differences between males and females in each group.\n\n# Map numeric group codes to more descriptive labels\ngroup_map = {\n    1: \"White Alone\",\n    2: \"Black/African American\",\n    3: \"American Indian alone\",\n    5: \"Other Group 5\",\n    6: \"Other Group 6\",\n    7: \"Other Group 7\",\n    8: \"Some Other Race alone\",\n    9: \"Two or More Races\"\n}\n\n# Map numeric sex codes to more descriptive labels\nsex_map = {\n    1: \"Male\",\n    2: \"Female\"\n}\n\n# Apply these mappings to the DataFrame used for plotting\nintersection[\"group_label\"] = intersection[\"group\"].map(group_map)\nintersection[\"sex_label\"] = intersection[\"sex\"].map(sex_map)\n\n# Plot the results using a bar chart\nsns.barplot(data=intersection, x=\"group_label\", y=\"label\", hue=\"sex_label\")\nplt.xlabel(\"Race Group\")\nplt.ylabel(\"Proportion Employed\")\nplt.title(\"Employment Proportion by Race Group and Sex\")\nplt.xticks(rotation=45)  # Rotate x labels if they're too long\nplt.legend(title=\"Sex\")\nplt.show()"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#training-our-model",
    "href": "posts/Auditing Bias/index.html#training-our-model",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Training our model",
    "text": "Training our model\nBelow, we begin training our model to predict whether an individual is employed based on their demographic features (excluding race). We set up a pipeline that performs three main steps:\n\nPolynomialFeatures: Expands the original features to include interaction terms and higher-order terms, allowing the model to capture more complex relationships.\nStandardScaler: Normalizes these expanded features so that no single feature dominates because of its scale.\nLogisticRegression: Fits a logistic regression classifier to predict employment (employed vs. not employed).\n\nWe then use a parameter grid (param_grid) to tune two key aspects:\n\npolynomialfeatures__degree controls how many polynomial and interaction terms we create (from no expansion at degree 1, to quadratic expansion at degree 2).\nlogisticregression__C controls the regularization strength of the logistic regression model, balancing the trade-off between overfitting and underfitting.\n\nThis setup, combined with grid search, systematically tests different combinations of polynomial degrees and regularization strengths, helping us figure out the best model configuration for our prediction task.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Create a pipeline with PolynomialFeatures, StandardScaler, and LogisticRegression.\n# PolynomialFeatures adds interaction terms and higher order terms, increasing model complexity.\npipeline = make_pipeline(\n    PolynomialFeatures(), \n    StandardScaler(), \n    LogisticRegression(max_iter=1000)\n)\n# Define a grid of parameters to tune:\n# - polynomialfeatures__degree: The degree of the polynomial features (1 = no expansion, 2 = quadratic, etc.)\n# - logisticregression__C: Regularization strength for logistic regression.\nparam_grid = {\n    'polynomialfeatures__degree': [1, 2],  # Limit the degree to reduce feature explosion\n    'logisticregression__C': [0.1, 1, 10]    # A small set of regularization values\n}\n\nWe use GridSearchCV to perform a 3-fold cross-validation test each combination of polynomial degree and regularization strength. The training set is split into 3 parts, and each model configuration is evaluated on each part. The best parameters are then chosen based on the highest average accuracy, and the final model is trained on the entire training set with those parameters taht we find.\n\n\n# Perform grid search with 3-fold cross-validation to speed up computation.\ngrid = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy')\ngrid.fit(X_train, y_train)\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('polynomialfeatures',\n                                        PolynomialFeatures()),\n                                       ('standardscaler', StandardScaler()),\n                                       ('logisticregression',\n                                        LogisticRegression(max_iter=1000))]),\n             param_grid={'logisticregression__C': [0.1, 1, 10],\n                         'polynomialfeatures__degree': [1, 2]},\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('polynomialfeatures',\n                                        PolynomialFeatures()),\n                                       ('standardscaler', StandardScaler()),\n                                       ('logisticregression',\n                                        LogisticRegression(max_iter=1000))]),\n             param_grid={'logisticregression__C': [0.1, 1, 10],\n                         'polynomialfeatures__degree': [1, 2]},\n             scoring='accuracy') best_estimator_: PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n                ('standardscaler', StandardScaler()),\n                ('logisticregression',\n                 LogisticRegression(C=10, max_iter=1000))]) PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures() StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression(C=10, max_iter=1000)"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#testing-our-accuracy",
    "href": "posts/Auditing Bias/index.html#testing-our-accuracy",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Testing our accuracy",
    "text": "Testing our accuracy\nIn this section, we test and evaluate our classifier. From our grid search, we found that the best parameters are a polynomial degree of 2 and a regularization strength (C) of 10. This helped us achieved a cross-validation accuracy of about 82.2%, and when we evaluated the model on the test set, we obtained an accuracy of roughly 82.5%. This means that our classifier correctly predicts whether an individual is employed for 82% of the cases in the test set.\n\n\n# Output the best parameters and cross-validation accuracy.\nprint(\"Best Parameters:\", grid.best_params_)\nprint(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n\n# Evaluate the best model on the test set.\ny_pred = grid.predict(X_test)\ntest_accuracy = (y_pred == y_test).mean()\nprint(\"Test Set Accuracy:\", test_accuracy)\n\nBest Parameters: {'logisticregression__C': 10, 'polynomialfeatures__degree': 2}\nBest Cross-Validation Accuracy: 0.8223675160030627\nTest Set Accuracy: 0.8245825789579562"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#auditing-our-model-overall-measures",
    "href": "posts/Auditing Bias/index.html#auditing-our-model-overall-measures",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Auditing Our Model (Overall Measures)",
    "text": "Auditing Our Model (Overall Measures)\nHere we evaluate our model’s performance on the test data. We calculate the confusion matrix, which includes true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP). The model’s overall accuracy is about 82.5%. Its precision (PPV) shows that around 78% of predicted employed cases were correct. The false negative rate (FNR) indicates that about 15.4% of employed individuals were missed, while the false positive rate (FPR) reveals that roughly 19.3% of non-employed individuals were incorrectly classified as employed. These metrics show us how effective our model is on unseen data.\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\nfrom sklearn.calibration import calibration_curve\n\n# --------------------------\n# 1. Overall Measures\n# --------------------------\n# Compute overall confusion matrix and metrics\n# y_test: true labels; y_pred: binary predictions from your best model\ncm = confusion_matrix(y_test, y_pred)\ntn, fp, fn, tp = cm.ravel()\n\noverall_accuracy = accuracy_score(y_test, y_pred)\noverall_ppv = precision_score(y_test, y_pred)  # PPV = precision\noverall_fnr = fn / (tp + fn) if (tp + fn) &gt; 0 else np.nan  # false negative rate\noverall_fpr = fp / (tn + fp) if (tn + fp) &gt; 0 else np.nan  # false positive rate\n\nprint(\"Overall Accuracy:\", overall_accuracy)\nprint(\"Overall PPV (Precision):\", overall_ppv)\nprint(\"Overall False Negative Rate:\", overall_fnr)\nprint(\"Overall False Positive Rate:\", overall_fpr)\n\nOverall Accuracy: 0.8245825789579562\nOverall PPV (Precision): 0.7794621534627765\nOverall FNR: 0.15375944087476046\nOverall FPR: 0.19286298011441025"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#auditing-our-model-testing-by-group-measures",
    "href": "posts/Auditing Bias/index.html#auditing-our-model-testing-by-group-measures",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Auditing our model (testing by group measures)",
    "text": "Auditing our model (testing by group measures)\nOur group metrics reveal how well our model performs across different subgroups. In our by-group analysis, Group 1 (White individuals) has an accuracy of 82.7% and a precision of 78.9%, with a false negative rate of 15.2% and a false positive rate of 19.2%. Group 2 (Black individuals) shows slightly lower performance, with an accuracy of 81.3% and a precision of 70.9%, alongside a higher false negative rate (17.2%). These differences suggest that our model may be less reliable for Black individuals, which is an important insight for our project focused on detecting and addressing racial bias in employment predictions.\n\n# By-Group Measures\n\n# Get the unique groups \ngroups = np.unique(group_test)\ngroup_metrics = []\n\n# Loop over each group to calculate the performance metrics\nfor g in groups:\n    idx = (group_test == g)\n    # Extract the true labels and predictions for this group\n    y_true_g = y_test[idx]\n    y_pred_g = y_pred[idx]\n\n    # here we check if the group contains both classes (0 and 1) for a reliable confusion matrix\n    if len(np.unique(y_true_g)) &lt; 2:  # Handle cases with only one class\n        tn_g = fp_g = fn_g = tp_g = np.nan\n    else: # Compute the confusion matrix for the current group\n        cm_g = confusion_matrix(y_true_g, y_pred_g)\n        try: # Unpack the confusion matrix into true negatives, false positives, false negatives, and true positives\n            tn_g, fp_g, fn_g, tp_g = cm_g.ravel()\n        except ValueError: # If there's an error (e.g., due to an unexpected shape), assign NaN\n            tn_g = fp_g = fn_g = tp_g = np.nan\n\n    # Calculate accuracy: proportion of correct predictions in this group\n    acc = accuracy_score(y_true_g, y_pred_g)\n    # Calculate precision (PPV): proportion of predicted positives that are actually positive\n    ppv = precision_score(y_true_g, y_pred_g, zero_division=0)\n    # Calculate false negative rate (FNR): proportion of actual positives that were missed\n    fnr = fn_g / (tp_g + fn_g) if (tp_g + fn_g) &gt; 0 else np.nan\n    # Calculate false positive rate (FPR): proportion of actual negatives that were incorrectly predicted as positive\n    fpr = fp_g / (tn_g + fp_g) if (tn_g + fp_g) &gt; 0 else np.nan\n\n    print(f\"\\nGroup {g}:\")\n    print(f\"  Count: {np.sum(idx)}\")\n    print(f\"  Accuracy: {acc:.3f}\")\n    print(f\"  PPV (Precision): {ppv:.3f}\")\n    print(f\"  FNR (False Negative Rate): {fnr:.3f}\")\n    print(f\"  FPR (False Positive Rate): {fpr:.3f}\")\n\n    group_metrics.append({\n        'group': g,\n        'count': np.sum(idx),\n        'accuracy': acc,\n        'ppv': ppv,\n        'fnr': fnr,\n        'fpr': fpr,\n    })\n\n\nGroup 1:\n  Count: 16815\n  Accuracy: 0.827\n  PPV (Precision): 0.789\n  FNR (False Negative Rate): 0.152\n  FPR (False Positive Rate): 0.192\n\nGroup 2:\n  Count: 1813\n  Accuracy: 0.813\n  PPV (Precision): 0.709\n  FNR (False Negative Rate): 0.172\n  FPR (False Positive Rate): 0.195\n\nGroup 3:\n  Count: 126\n  Accuracy: 0.754\n  PPV (Precision): 0.655\n  FNR (False Negative Rate): 0.250\n  FPR (False Positive Rate): 0.244\n\nGroup 5:\n  Count: 21\n  Accuracy: 0.714\n  PPV (Precision): 0.556\n  FNR (False Negative Rate): 0.286\n  FPR (False Positive Rate): 0.286\n\nGroup 6:\n  Count: 474\n  Accuracy: 0.800\n  PPV (Precision): 0.731\n  FNR (False Negative Rate): 0.111\n  FPR (False Positive Rate): 0.276\n\nGroup 7:\n  Count: 3\n  Accuracy: 0.667\n  PPV (Precision): 0.000\n  FNR (False Negative Rate): nan\n  FPR (False Positive Rate): nan\n\nGroup 8:\n  Count: 162\n  Accuracy: 0.809\n  PPV (Precision): 0.815\n  FNR (False Negative Rate): 0.195\n  FPR (False Positive Rate): 0.188\n\nGroup 9:\n  Count: 470\n  Accuracy: 0.847\n  PPV (Precision): 0.726\n  FNR (False Negative Rate): 0.185\n  FPR (False Positive Rate): 0.139"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#calculating-bias-measures",
    "href": "posts/Auditing Bias/index.html#calculating-bias-measures",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Calculating Bias Measures",
    "text": "Calculating Bias Measures\nCalibration Curve Analysis:\n\nWe can see in our calibration curve, which shows how well our model’s predicted probabilities align with the actual outcomes. The blue line (our model’s predictions) is very close to the orange diagonal (perfect calibration), indicating that when our model predicts a probability ( p ) of being employed, roughly ( p% ) of those individuals are employed. From our graph, we can say that our model is well-calibrated overall, which means it does a good job matching predicted probabilities to real-world outcomes. However, calibration alone does not guarantee fairness, so we still need to check subgroup performance to ensure the model treats different groups equitably.\n\n\n# So we need to first calculate our y_prob: predicted probability for the positive class\ny_prob = grid.predict_proba(X_test)[:, 1]\n\n# Overall calibration curve\nprob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\nplt.figure(figsize=(6, 6))\nplt.plot(prob_pred, prob_true, label=\"Overall\")\nplt.plot([0, 1], [0, 1], label=\"Perfect Calibration\")\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve: Overall\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCalibration Curve by Group:\n\nOur graph below shows our calibration curve broken down by race group. Each colored line corresponds to a different group’s predicted probabilities versus their actual outcomes.We can see in our graph that Group 1 (White) appears relatively close to the diagonal for most probability ranges, suggesting good calibration. Group 2 (Black/African American) shows more fluctuation, especially in the mid-probability range, indicating that the model’s predicted probabilities for this group deviate more from perfect calibration. Groups with fewer samples (such as Groups 5, 7, and 9) have jagged lines, which likely stems from limited data. Looking at these deviations is important for understanding and addressing potential fairness issues across all groups.\n\n\ngroups = np.unique(group_test)\nplt.figure(figsize=(6, 6))\nfor g in groups:\n    idx = (group_test == g)\n    # If there are very few samples for this group, skip\n    if np.sum(idx) &lt; 10:\n        continue\n    prob_true_g, prob_pred_g = calibration_curve(y_test[idx], y_prob[idx], n_bins=10)\n    plt.plot(prob_pred_g, prob_true_g,  label=f\"Group {g}\")\nplt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve by Group\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nError Rate Balance\n\nOur error rate balance shows that overall, our model misses about 15.4% of employed individuals (FNR) and incorrectly predicts employment for 19.3% of unemployed individuals (FPR). For Group 1 (White individuals), the rates are very similar—15.2% FNR and 19.2% FPR. In contrast, Group 2 (Black individuals) has slightly higher error rates, with an FNR of 17.2% and an FPR of 19.5%. Other groups, particularly those with fewer samples, exhibit more variability: for example, Group 3 shows a 25% FNR and 24% FPR, while Group 6 (Asian individuals) has a low FNR (11.1%) but a higher FPR (27.6%). These differences indicate that our model does not achieve perfect error rate balance across all groups, highlighting areas where bias may be present in predicting employment status.\n\n\ndef get_fnr_fpr(y_true, y_pred):\n    \"\"\"Compute FNR and FPR for given true and predicted labels.\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    fnr = fn / (fn + tp) if (fn + tp) &gt; 0 else np.nan\n    fpr = fp / (fp + tn) if (fp + tn) &gt; 0 else np.nan\n    return fnr, fpr\n\n# Overall FNR/FPR\noverall_fnr, overall_fpr = get_fnr_fpr(y_test, grid.predict(X_test))\nprint(f\"Overall FNR: {overall_fnr:.3f}\")\nprint(f\"Overall FPR: {overall_fpr:.3f}\")\n\n# By-group FNR/FPR\nfor g in groups:\n    idx = (group_test == g)\n    y_true_g = y_test[idx]\n    y_pred_g = grid.predict(X_test[idx])\n    fnr_g, fpr_g = get_fnr_fpr(y_true_g, y_pred_g)\n    print(f\"Group {g} FNR: {fnr_g:.3f}, FPR: {fpr_g:.3f}\")\n\nOverall FNR: 0.154\nOverall FPR: 0.193\nGroup 1 FNR: 0.152, FPR: 0.192\nGroup 2 FNR: 0.172, FPR: 0.195\nGroup 3 FNR: 0.250, FPR: 0.244\nGroup 5 FNR: 0.286, FPR: 0.286\nGroup 6 FNR: 0.111, FPR: 0.276\nGroup 7 FNR: nan, FPR: 0.333\nGroup 8 FNR: 0.195, FPR: 0.188\nGroup 9 FNR: 0.185, FPR: 0.139\n\n\nStatistical Parity Test\n\nOur statistical parity analysis shows that, overall, our model predicts employment for about 48.4% of individuals. However, when we break it down by race, we see notable differences that raise fairness concerns. For example, Group 1 (White individuals) has a predicted employment rate of 49.3%, whereas Group 2 (Black individuals) is lower at 42.6%. Other groups vary as well—Group 6 (Asian individuals) has a higher rate of 55.7%, while Groups 7 and 9 have much lower rates (33.3% and 34.9%, respectively). These disparities in predicted positive rates suggest that our model may favor some groups over others, which is a critical issue in auditing fairness.\n\n\ny_pred = grid.predict(X_test)\n\n# Overall predicted positive rate\noverall_positive_rate = np.mean(y_pred)\nprint(f\"\\nOverall Predicted Positive Rate: {overall_positive_rate:.3f}\")\n\n# By-group predicted positive rate\nfor g in groups:\n    idx = (group_test == g)\n    group_positive_rate = np.mean(y_pred[idx])\n    print(f\"Group {g} Predicted Positive Rate: {group_positive_rate:.3f}\")\n\n\nOverall Predicted Positive Rate: 0.484\nGroup 1 Predicted Positive Rate: 0.493\nGroup 2 Predicted Positive Rate: 0.426\nGroup 3 Predicted Positive Rate: 0.437\nGroup 5 Predicted Positive Rate: 0.429\nGroup 6 Predicted Positive Rate: 0.557\nGroup 7 Predicted Positive Rate: 0.333\nGroup 8 Predicted Positive Rate: 0.500\nGroup 9 Predicted Positive Rate: 0.349\n\n\nFeasible FNR and FPR rates interpretation\nIn our graph each colored line shows all (FNR, FPR) combinations that would achieve a fixed positive predictive value (PPV) for a given group, based on that group’s prevalence. The circular markers indicate our model’s actual (FNR, FPR).\n\nGroup 1 (White): Its feasible line illustrates how changing the threshold could shift its false negative rate (FNR) and false positive rate (FPR).\nGroup 2 (Black): Has its own feasible line, which typically differs because of a higher prevalence (more individuals in the group are actually employed) and/or different PPV requirements.\n\nIf we tried to make FPR the same for both groups (say, moving Group 2’s marker to Group 1’s FPR), we would generally need to increase Group 2’s FNR. That means more Black individuals who are actually employed would be missed by the model—an important trade-off for fairness. Essentially, achieving equal FPR across groups often comes at the cost of raising FNR for at least one group, which can exacerbate disparities in missed positives. This plot helps us visualize how much we’d have to adjust each group’s threshold to meet that fairness goal, and how it could impact different groups in our dataset.\nIn the figure below, each colored line shows the combinations of false negative rate (FNR) and false positive rate (FPR) that would achieve the same positive predictive value (PPV) for a particular group, given its prevalence. The colored dots represent our model’s actual (FNR, FPR) for each group. Notably, some groups’ actual points lie close to their feasible lines, meaning only minor threshold adjustments would be required to change their FPR or FNR. However, for groups where the actual point is far from the feasible line (e.g., Group 2), forcing a lower FPR would require a significant increase in FNR. In practical terms, that means we would miss many more truly employed individuals from that group. This highlights the inherent trade-off between equalizing FPR across groups and maintaining a reasonable FNR, especially when group prevalences differ.\n\ncolor_map = {\n    1: \"blue\",\n    2: \"green\",\n    3: \"red\",\n    5: \"purple\",\n    6: \"orange\",\n    7: \"brown\",\n    8: \"pink\",\n    9: \"gray\"\n}\n\nfpr_lines = {}\nfpr_actual = {}\nfor g in groups:\n    idx = (group_test == g)\n    # Prevalence: proportion of true positives in group g\n    p = np.mean(y_test[idx])\n    fnr_range = np.linspace(0, 1, 100)\n    PPV_desired = overall_ppv  # fixed desired PPV\n    # Compute feasible FPR using Eq. (2.6)\n    fpr_line = np.where((1 - p) &gt; 0, ((1 - fnr_range) * p * (1/PPV_desired - 1)) / (1 - p), np.nan)\n    fpr_lines[g] = (fnr_range, fpr_line)\n    \n    # Get actual FNR and FPR from group_metrics\n    for metric in group_metrics:\n        if metric['group'] == g:\n            fpr_actual[g] = (metric['fnr'], metric['fpr'])\n            break\n\n# Plot the feasible FNR-FPR lines and actual points with consistent colors\nplt.figure(figsize=(8, 6))\nfor g, (fnr_range, fpr_line) in fpr_lines.items():\n    plt.plot(fnr_range, fpr_line, linestyle='--', linewidth=2,\n             label=f\"Feasible for Group {g}\", color=color_map.get(g, \"black\"))\n    if g in fpr_actual:\n        actual_fnr, actual_fpr = fpr_actual[g]\n        plt.plot(actual_fnr, actual_fpr, \"o\", markersize=10, \n                 markeredgecolor='black', markeredgewidth=1.5,\n                 color=color_map.get(g, \"black\"), \n                 label=f\"Actual Group {g}\")\n        \nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible FNR vs. FPR Rates (Chouldechova 2017 Eq. 2.6)\")\nplt.legend()\nplt.xlim([0, 1])\nplt.ylim([0, 0.3])  # Limit the y-axis to 0.3 for better visibility\nplt.show()"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#interpreting-fnr-changes-to-equalize-fpr",
    "href": "posts/Auditing Bias/index.html#interpreting-fnr-changes-to-equalize-fpr",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Interpreting FNR Changes to Equalize FPR:",
    "text": "Interpreting FNR Changes to Equalize FPR:\nFrom the feasible lines, we see how each group’s false negative rate (FNR) would have to shift if we wanted all groups to share the same false positive rate (FPR). For example, if Group 2’s actual FPR is higher than Group 1’s, lowering it to match Group 1’s level would require moving Group 2’s marker along its feasible line toward a lower FPR. This shift, however, pushes the marker toward a much higher FNR—often much higher than the group’s current FNR. In other words, this means that, to reduce Group 2’s false positives, we would end up missing many more truly employed individuals (i.e., a spike in FNR). Each group’s line tells a similar story: matching another group’s FPR typically demands a noticeable trade-off in FNR, underscoring the difficulty of balancing error rates across groups when their underlying prevalence differs."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#concluding-discussion",
    "href": "posts/Auditing Bias/index.html#concluding-discussion",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOur project aimed to predict whether an individual is employed using demographic data from the ACS PUMS dataset. Systems like this could benefit various stakeholders. For example, companies in human resources or financial services might use such a model to assess employment status quickly and efficiently. Government agencies could also deploy this system to better understand labor market trends or to guide social policy. Essentially, any organization that needs insights into employment patterns could find value in our approach.\nHowever, our bias audit revealed important fairness challenges. Although the model is overall well-calibrated—meaning its predicted probabilities match real-world outcomes—it shows differences in error rates across racial groups. Group 1 (White individuals) and Group 2 (Black individuals) have similar overall accuracy, but Black individuals experience a higher false negative rate, indicating that more truly employed individuals are being missed by the model. This disparity in error rates is critical; deploying such a model at scale might inadvertently disadvantage Black individuals, leading to biased outcomes in decision-making processes.\nBeyond bias in error rates, there are other potential concerns. Even if calibration is good, uneven predicted positive rates suggest that the model may favor one group over another, undermining fairness. Moreover, issues such as data quality, changes over time, and the potential misuse of the model in sensitive contexts (e.g., employment or credit scoring) could further compound these problems. To address these concerns, it is important to continue refining the model, incorporate more diverse and representative data, and establish robust monitoring and adjustment procedures when the model is deployed in real-world settings."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html",
    "href": "posts/limits of approach of quantitative bias/index.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In today’s day and age where data-driven decisions increasingly affect everyday lives—from credit scoring to hiring practices—quantitative methods have become central to assessing discrimination and bias. In his 2022 speech, Arvind Narayanan argues that “currently quantitative methods are primarily used to justify the status quo” Narayanan (2022). He challenges the prevalent view that numbers provide an objective measure of fairness, suggesting instead that quantitative approaches may inadvertently reinforce existing social hierarchies. This essay examines Narayanan’s position, discusses the dual nature of quantitative methods in both uncovering and perpetuating discrimination, and reviews a case study where quantitative techniques have been beneficial in illuminating fairness issues in decision-making processes."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#introduction",
    "href": "posts/limits of approach of quantitative bias/index.html#introduction",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In today’s day and age where data-driven decisions increasingly affect everyday lives—from credit scoring to hiring practices—quantitative methods have become central to assessing discrimination and bias. In his 2022 speech, Arvind Narayanan argues that “currently quantitative methods are primarily used to justify the status quo” Narayanan (2022). He challenges the prevalent view that numbers provide an objective measure of fairness, suggesting instead that quantitative approaches may inadvertently reinforce existing social hierarchies. This essay examines Narayanan’s position, discusses the dual nature of quantitative methods in both uncovering and perpetuating discrimination, and reviews a case study where quantitative techniques have been beneficial in illuminating fairness issues in decision-making processes."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#narayanans-position-on-quantitative-methods",
    "href": "posts/limits of approach of quantitative bias/index.html#narayanans-position-on-quantitative-methods",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Narayanan’s Position on Quantitative Methods",
    "text": "Narayanan’s Position on Quantitative Methods\nNarayanan’s speech critiques the heavy reliance on quantitative analyses in the study of discrimination. He argues that quantitative methods assume no discrimination unless proven otherwise, placing the burden of proof on those claiming bias. He asserts that this default assumption upholds existing power structures rather than challenging them. When disparities appear in metrics like false-positive rates in criminal risk assessments, they are often explained as statistical noise rather than evidence of systemic injustice Narayanan (2022).\nFurthermore, Narayanan emphasizes that quantitative methods, by their very nature, reduce complex social phenomena to just numbers. By doing this, they risk overlooking the real experiences of those affected by discrimination. While significance tests and confusion matrices show differences in outcomes between racial or gender groups, these metrics alone cannot fully capture the complexity of systemic oppression. This critique challenges scholars and practitioners to consider whether the technical accuracy of quantitative methods always aligns with their social impact."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#the-benefits-of-quantitative-methods",
    "href": "posts/limits of approach of quantitative bias/index.html#the-benefits-of-quantitative-methods",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "The Benefits of Quantitative Methods",
    "text": "The Benefits of Quantitative Methods\nDespite Narayanan’s reservations, quantitative methods have undeniable strengths that have contributed to significant advancements in understanding discrimination. Some of their benefits include:\n\nPrecision and Replicability: Quantitative techniques, such as formal mathematical definitions of bias and fairness, provide a framework for rigorously measuring disparities. For example, fairness metrics like statistical parity, equality of opportunity, and calibration enable researchers to compare outcomes across groups in a replicable manner Barocas, Hardt, and Narayanan (2023). These methods are especially useful in contexts where decisions must be justified to regulatory bodies or stakeholders.\nScalability: Large datasets from various sectors (e.g., criminal justice, hiring, lending) allow quantitative methods to detect subtle patterns of bias that might escape qualitative analyses. Collecting data systematically and analyzing it statistically can uncover trends—such as differences in false-positive rates between demographic groups—that highlight broader systemic issues. These findings can then guide policy decisions to address such disparities.\nPolicy Relevance: Quantitative analyses have played a critical role in policy-making. A notable example is the ProPublica investigation into criminal risk prediction tools, which used statistical tests to show that Black defendants were more likely to be misclassified as high risk compared to white defendants. Although Narayanan critiques the limitations of such studies, it is undeniable that they have driven significant public and policy debates about algorithmic fairness Narayanan (2022). Quantitative evidence, when appropriately contextualized, can help mobilize reform and promote accountability."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#an-example-of-a-beneficial-quantitative-study",
    "href": "posts/limits of approach of quantitative bias/index.html#an-example-of-a-beneficial-quantitative-study",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "An Example of a Beneficial Quantitative Study",
    "text": "An Example of a Beneficial Quantitative Study\nOne beneficial example of using quantitative methods in the study of fairness is the analysis of credit-scoring algorithms. In the study discussed by Barocas, Hardt, and Narayanan Barocas, Hardt, and Narayanan (2023), researchers used detailed statistical models to assess whether machine learning systems used in credit decisions perpetuated racial or gender disparities. By analyzing large datasets and employing fairness criteria, they were able to identify that certain features—even when not explicitly related to race or gender—acted as proxies for these protected attributes. The study not only highlighted the existence of bias but also suggested modifications to the algorithms that could lead to fairer outcomes.\nThe strength of this quantitative approach lies in its ability to pinpoint exactly which elements of the data contribute to discriminatory outcomes. For example, confusion matrices and false-positive rates provided clear metrics by which the performance of the credit-scoring model could be judged across different demographic groups. This analysis allowed for the identification of unintended consequences—such as systematically lower credit scores for minority applicants—that might have otherwise been overlooked. Thus, when used carefully and coupled with a deep understanding of the social context, quantitative methods can offer actionable insights that promote more equitable decision-making."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#the-drawbacks-and-limitations-of-quantitative-approaches",
    "href": "posts/limits of approach of quantitative bias/index.html#the-drawbacks-and-limitations-of-quantitative-approaches",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "The Drawbacks and Limitations of Quantitative Approaches",
    "text": "The Drawbacks and Limitations of Quantitative Approaches\nWhile the benefits are compelling, the drawbacks of relying solely on quantitative methods are equally significant, as Narayanan and other scholars have pointed out:\n\nReductionism and Oversimplification: Quantitative methods often require reducing complex human experiences and societal structures to numerical values. This abstraction can lead to a failure to account for the context and intersectionality inherent in discrimination. For instance, while statistical tests might show that women are underrepresented in leadership roles, they cannot capture the myriad ways in which gender intersects with race, socioeconomic status, and other factors to produce these outcomes D’Ignazio and Klein (2023).\nThe Null Hypothesis Problem: As Narayanan argues, the conventional use of the null hypothesis—assuming no discrimination until evidence suggests otherwise—can obscure genuine disparities. By demanding rigorous statistical proof before acknowledging discrimination, quantitative methods may inadvertently uphold the status quo. This methodological choice means that even when quantitative data reveal significant disparities, the interpretation of these results may lean towards rationalizing existing practices rather than questioning them.\nIgnoring Qualitative Nuance: Quantitative studies may fail to capture the lived experiences of those affected by discrimination. For example, while a machine learning model might quantify the disparity in false-positive rates, it cannot communicate the real-world impact of these errors on individuals’ lives. As critics like Buolamwini and Gebru Buolamwini and Gebru (2018) have argued, integrating qualitative insights is essential for understanding the human cost of algorithmic bias. Qualitative methods—through interviews, ethnographic research, and case studies—provide context that enriches the quantitative findings.\nReinforcing Existing Biases: When historical data, which often reflect long-standing societal prejudices, are used to train machine learning models, the models can perpetuate these biases. This phenomenon is sometimes referred to as “feedback loops,” where biased decisions lead to further bias in future data collection. Cathy O’Neil, in Weapons of Math Destruction, argues that such opaque, unaccountable algorithms can reinforce and even amplify societal inequalities O’Neil (2016). In such cases, quantitative methods may end up reinforcing the very inequalities they are meant to expose, unless corrective interventions are thoughtfully implemented."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#balancing-quantitative-and-qualitative-approaches",
    "href": "posts/limits of approach of quantitative bias/index.html#balancing-quantitative-and-qualitative-approaches",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Balancing Quantitative and Qualitative Approaches",
    "text": "Balancing Quantitative and Qualitative Approaches\nThe critique posed by Narayanan calls for a balanced approach that integrates both quantitative and qualitative methods. Quantitative analysis provides the rigorous statistical evidence needed to identify and measure disparities, while qualitative research adds depth and context to understand the root causes and real-world impacts of these disparities. For instance, while the quantitative study of credit-scoring algorithms identified proxy variables that contributed to discriminatory outcomes, qualitative interviews with affected applicants could reveal how these algorithmic decisions influence their access to financial services and overall economic mobility.\nSeveral scholars have advocated for mixed-methods research as a way to bridge the gap between numerical data and human experience. Selbst et al. Selbst et al. (2019) argue that incorporating qualitative insights into quantitative frameworks not only enriches the analysis but also helps to avoid the pitfalls of reductionism. Such approaches are particularly valuable in sensitive areas like discrimination, where numbers alone cannot capture the full spectrum of injustice."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#my-position-on-narayanans-claim",
    "href": "posts/limits of approach of quantitative bias/index.html#my-position-on-narayanans-claim",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "My Position on Narayanan’s Claim",
    "text": "My Position on Narayanan’s Claim\nAfter weighing the evidence and arguments, I find merit in Narayanan’s claim that quantitative methods, when used in isolation, risk justifying the status quo. It is clear that the methodological choices—such as adopting the null hypothesis of no discrimination—can skew interpretations and obscure the broader social dynamics at play. However, I would argue that quantitative methods are not inherently harmful; rather, their impact depends on how they are applied and interpreted.\nWhen used with qualitative research, quantitative methods become a powerful tool for highlighting inequities and informing policy. For example, the credit-scoring study discussed earlier shows that with careful design and a willingness to interrogate the data critically, quantitative analyses can lead to meaningful reforms. Therefore, I agree with Narayanan’s caution against a purely quantitative lens but also advocate for a balanced methodology that leverages the strengths of both approaches.\nIn practice, decision-makers and researchers should be transparent about the limitations of their quantitative analyses. They should complement statistical findings with qualitative research to ensure that the numbers do not become an end in themselves but serve as a means to a more comprehensive understanding of discrimination. This integrative approach can help shift the focus from merely justifying existing practices to actively challenging and transforming them."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#conclusion",
    "href": "posts/limits of approach of quantitative bias/index.html#conclusion",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Conclusion",
    "text": "Conclusion\nQuantitative methods have transformed the study of discrimination by offering precise, scalable, and policy-focused tools for analyzing disparities. However, as Narayanan (2022) points out, these methods have significant limitations, including the risk of reinforcing the status quo when used without proper context. For example, credit-scoring algorithms show that while quantitative techniques can detect and reduce bias, they must be paired with qualitative insights to fully address the complexities of discrimination.\nUltimately, the debate over fairness in quantitative methods is not about choosing between data and narratives. Instead, it requires integrating both—valuing statistical rigor while remaining mindful of the social, historical, and human factors that shape discrimination. By doing so, researchers and policymakers can use quantitative methods to promote meaningful change rather than unintentionally upholding existing inequities."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "File of Perceptron.py https://github.com/Pbabu-Github/csci-0451/blob/main/posts/perceptron/perceptron.py\n%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer"
  },
  {
    "objectID": "posts/perceptron/index.html#abstract",
    "href": "posts/perceptron/index.html#abstract",
    "title": "Implementing Perceptron",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I implement the perceptron algorithm using PyTorch. I experiment with linear boundaries that separate two classes in a dataset, as well as cases where a clear separation does not exist. My implementation includes a LinearModel class for basic operations (computing scores and predictions) and a Perceptron subclass that defines the loss (misclassification rate) and gradient update. The algorithm updates its weight vector whenever a data point is misclassified. Additionally, I use visualization functions to display the training data and, for 2D cases, both the decision boundary and the evolution of the loss over time."
  },
  {
    "objectID": "posts/perceptron/index.html#train-using-the-perceptron-optimizer-minimal-training-loop",
    "href": "posts/perceptron/index.html#train-using-the-perceptron-optimizer-minimal-training-loop",
    "title": "Implementing Perceptron",
    "section": "Train Using the Perceptron Optimizer (Minimal Training Loop)",
    "text": "Train Using the Perceptron Optimizer (Minimal Training Loop)"
  },
  {
    "objectID": "posts/perceptron/index.html#walk-through-of-grad-function",
    "href": "posts/perceptron/index.html#walk-through-of-grad-function",
    "title": "Implementing Perceptron",
    "section": "Walk through of Grad function",
    "text": "Walk through of Grad function\nThe grad() function in my implementation is responsible for computing the weight update for a single example. So what i did was:\n\nFirst, I made sure the data point X is in the right shape. If it came in as a row (1, p), I turned it into a 1D vector (p,).\nThen I checked if the label y is in {-1, 1}. If it wasnt, I converted it using y_mod = 2 * y - 1.\nI calculated the score by taking the dot product of the weights and the input (s = w • x). This score tells us how far the point is from the decision boundary.\nIf s * y_mod is less than 0, that means the point is misclassified, so I returned the gradient as -y_mod * X. This will move the weights in the right direction.\nIf the point was classified correctly, I just returned a zero vector (so there would be no update needed).\n\nThis matches the perceptron update rule taht if a point is wrong, we update the weights with (2y - 1) * x. Lets now perfom some experiments to check our implementation. Before that, teh code below does a simple check to see if our implementation is working by creating 2d data and perfroms the training and prints our loss after our update step.\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom perceptron import Perceptron, PerceptronOptimizer, perceptron_data, plot_perceptron_data\n\n# Generate the 2D data.\nX, y = perceptron_data(n_points=300, noise=0.2)\n\n# Plot the training data.\nfig, ax = plt.subplots(figsize=(6, 6))\nplot_perceptron_data(X, y, ax)\nax.set_title(\"Perceptron Training Data\")\nplt.show()\n\n# Create a perceptron and an optimizer.\np = Perceptron()\nopt = PerceptronOptimizer(p)\nloss = opt.step(X[0:1], y[0])  # update using a single data point.\nprint(\"Loss after one step:\", loss.item())\n\n\n\n\n\n\n\n\nLoss after one step: 1.0"
  },
  {
    "objectID": "posts/perceptron/index.html#runtime-complexity-of-a-single-iteration",
    "href": "posts/perceptron/index.html#runtime-complexity-of-a-single-iteration",
    "title": "Implementing Perceptron",
    "section": "Runtime Complexity of a Single Iteration",
    "text": "Runtime Complexity of a Single Iteration\nFor each update (i.e., for each single data point):\nDot Product: The calculation torch.dot(self.w, X) involves summing over the products of corresponding elements of two vectors. If there are p features, this operation is O(p).\nGradient Computation and Weight Update: Similarly, multiplying the feature vector by the scalar label and subtracting this from the weight vector both take O(p) time.\nThus, the runtime complexity for a single iteration (or update) of the perceptron algorithm is O(p), where p is the number of features in your dataset. In the context of the full training loop, if you compute the loss over n points, that operation is O(n×p), but each update step remains O(p).\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom perceptron import Perceptron, PerceptronOptimizer, perceptron_data, plot_perceptron_data\n\n# Generate the 2D data.\nX, y = perceptron_data(n_points=300, noise=0.2)\n\n# Plot the training data.\nfig, ax = plt.subplots(figsize=(6, 6))\nplot_perceptron_data(X, y, ax)\nax.set_title(\"Perceptron Training Data\")\nplt.show()\n\n# Optionally, test a training step:\np = Perceptron()\nopt = PerceptronOptimizer(p)\nloss = opt.step(X[0:1], y[0])  # update using a single data point.\nprint(\"Loss after one step:\", loss.item())\n\n\n\n\n\n\n\n\nLoss after one step: 1.0"
  },
  {
    "objectID": "posts/perceptron/index.html#experiment-1-perceptron-training-on-2d-data",
    "href": "posts/perceptron/index.html#experiment-1-perceptron-training-on-2d-data",
    "title": "Implementing Perceptron",
    "section": "Experiment 1: Perceptron Training on 2D data",
    "text": "Experiment 1: Perceptron Training on 2D data\nIn the experiment below, I use the perceptron algorithm to classify 2D data that is linearly separable.\n\nWe first generate 2D data using perceptron_data(), which creates two clearly separated classes and adds a bias column.\nWe next initialize a Perceptron model and an optimizer. For each iteration we calculate the current loss (misclassification rate), randomly select a single data point and if the model misclassifies it, we update the weights using the Perceptron rule.\nWe repeat this until the model achieves zero loss or we reach the maximum number of iterations.\n\n\n# Experiment 1: 2D Data (Linearly Separable\nX2, y2 = perceptron_data(n_points=300, noise=0.2)  #generate 2d data \np2 = Perceptron()\nopt2 = PerceptronOptimizer(p2)\nloss_vec = []\nn = X2.size(0)\nmax_iter = 1000\nfor _ in range(max_iter):\n    loss = p2.loss(X2, y2)\n    loss_vec.append(loss.item())\n    if loss.item() == 0: break\n    i = torch.randint(n, (1,))\n    opt2.step(X2[[i], :], y2[i])\n \n\n#Plot decision boundary on 2D data:\nw = p2.w  # final weight vector: [w1, w2, bias]\nx1_vals = torch.linspace(X2[:,0].min()-0.5, X2[:,0].max()+0.5, 100)\nx2_vals = -(w[0]*x1_vals + w[2]) / w[1]  # boundary: w1*x1 + w2*x2 + bias = 0\nplt.figure(figsize=(6,6))\nplt.scatter(X2[:,0], X2[:,1], c=y2)\nplt.plot(x1_vals, x2_vals,  linewidth=2)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Decision Boundary on 2D Data\")\nplt.show()\n\n\n\n\n\n\n\n\nVisualizing the loss evolution\n\n#plot loss evolution\nplt.figure(figsize=(6,4))\nplt.plot(loss_vec, marker='o', color='slategrey')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Evolution (2D Data)\")\nplt.show()\n\n\n\n\n\n\n\n\nAfter training we plot the loss vs. iteration to show how the loss decreases as the model learns adn we also plot the final decision boundary over the 2D data. The red and blue dots show the two classes and the straight line separates them. This shows that the perceptron can successfully find a separating line when the data is linearly separable."
  },
  {
    "objectID": "posts/perceptron/index.html#runtime-complexity-of-a-single-perceptron-iteration",
    "href": "posts/perceptron/index.html#runtime-complexity-of-a-single-perceptron-iteration",
    "title": "Implementing Perceptron",
    "section": "Runtime Complexity of a Single Perceptron Iteration",
    "text": "Runtime Complexity of a Single Perceptron Iteration\nIn each iteration of perceptron training, we:\n\nCompute a dot product between the weight vector w and a single input example x. If x has p features, this takes O(p) time.\nIf the example is misclassified, we compute the gradient and update the weights — this also takes O(p) time.\n\nSo overall, a single iteration takes O(p) time. It doesn’t depend on the total number of data points n, because we only update using one randomly selected point per iteration.\nLooking at the Loss Evolution graph above, we can also see that the model usually improves quickly (loss drops) and then flattens out. Even if a spike happens, it corrects quickly with just a couple more iterations. This shows that each iteration is efficient and contributes quickly to learning — especially when the data is linearly separable like in our 2D example."
  },
  {
    "objectID": "posts/perceptron/index.html#experiment-2-perceptron-training-on-high-dimensional-data-3-features",
    "href": "posts/perceptron/index.html#experiment-2-perceptron-training-on-high-dimensional-data-3-features",
    "title": "Implementing Perceptron",
    "section": "Experiment 2: Perceptron Training on High-Dimensional Data (3 Features)",
    "text": "Experiment 2: Perceptron Training on High-Dimensional Data (3 Features)\nIn this experiment, I train the perceptron algorithm on a dataset with 5 features instead of just 2.\n\nWe generate high-dimensional data using high_dim_data(), which creates 5-dimensional data points (4 random features plus a constant column for bias). The two classes are linearly separable, but harder to visualize.\nWe initialize a Perceptron model and an optimizer. As before, in each iteration we:\n\ncalculate the current loss,\nrandomly select a data point, and\napply the perceptron update rule if that point is misclassified.\n\nWe keep updating until the loss reaches zero or we hit the maximum number of iterations.\n\n\n\n#  Experiment 2: High-Dimensional Data (5 Features) \ndef high_dim_data(n_points=300, noise=0.2, p_dims=5):\n    # Generate data with p_dims-1 features and append constant column.\n    y = torch.arange(n_points) &gt;= n_points//2\n    X = y[:, None].float() + torch.normal(0.0, noise, size=(n_points, p_dims-1))\n    X = torch.cat((X, torch.ones((n_points, 1))), 1)\n    y = 2 * y.float() - 1  # convert targets to {-1, 1}\n    return X, y\n\nX5, y5 = high_dim_data(n_points=300, noise=0.2, p_dims=5)\np5 = Perceptron()\nopt5 = PerceptronOptimizer(p5)\nloss_vec_hd = []\nn_hd = X5.size(0)\nfor _ in range(max_iter):\n    loss = p5.loss(X5, y5)\n    loss_vec_hd.append(loss.item())\n    if loss.item() == 0: break\n    i = torch.randint(n_hd, (1,))\n    opt5.step(X5[[i], :], y5[i])\n    \n# Plot loss evolution for high-dimensional data:\nplt.figure(figsize=(6,4))\nplt.plot(loss_vec_hd, marker='o', color='teal')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Evolution (High-Dimensional Data)\")\nplt.show()\n\n\n\n\n\n\n\n\nSince the data lives in more than 2 dimensions, we cannot directly plot the decision boundary. But we still visualize the loss over time to see how the model is learning and the graph above shows that the perceptron is still able to learn from high-dimensional data. The loss decreases quickly, though it fluctuates a bit more than in the 2D case — this is makes sense, because higher dimensions can make it harder to find a good separating boundary."
  },
  {
    "objectID": "posts/perceptron/index.html#runtime-complexity-of-a-single-perceptron-iteration-high-dimensional-case",
    "href": "posts/perceptron/index.html#runtime-complexity-of-a-single-perceptron-iteration-high-dimensional-case",
    "title": "Implementing Perceptron",
    "section": "Runtime Complexity of a Single Perceptron Iteration (High-Dimensional Case)",
    "text": "Runtime Complexity of a Single Perceptron Iteration (High-Dimensional Case)\nEven though the data has more features now, the complexity of each iteration stays the same:\n\nWe compute the dot product w · x where x has p features → this takes O(p) time.\nIf the point is misclassified, the gradient computation and weight update also take O(p) time.\n\nSo a single update step still runs in O(p) time. The number of data points n does not affect the time for each step because we’re only using one data point per iteration.\nFrom the Loss Evolution graph above, we can see that perceptron learns effectively. The drops in loss happen fast, confirming that even in high dimensions, each iteration helps the model improve quickly — as long as the data is linearly separable.\nExperiment 3: Perceptron on 2D Data that is not linearly Separable\n\n# Generate nonseparable 2D data by increasing noise.\nX_ns, y_ns = perceptron_data(n_points=300, noise=0.6)\np_ns = Perceptron()\nopt_ns = PerceptronOptimizer(p_ns)\nloss_vec = []\nn = X_ns.size(0)\nmax_iter = 1000\n\n# Training loop: update on a random point each iteration.\nfor _ in range(max_iter):\n    loss = p_ns.loss(X_ns, y_ns).item()\n    loss_vec.append(loss)\n    # If by chance loss becomes 0, we could break—but for nonseparable data, it won't.\n    i = torch.randint(n, (1,))\n    opt_ns.step(X_ns[[i], :], y_ns[i])\n\n\n# Plot the data and the final decision boundary.\nw = p_ns.w  # final weight vector [w1, w2, bias]\nx_vals = torch.linspace(X_ns[:,0].min()-0.5, X_ns[:,0].max()+0.5, 100)\ny_vals = -(w[0]*x_vals + w[2]) / w[1]  # decision boundary: w1*x + w2*y + bias = 0\nplt.figure(figsize=(6,6))\nax = plt.gca()\nplot_perceptron_data(X_ns, y_ns, ax)\nax.plot(x_vals, y_vals, linewidth=2)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_title(\"Final Decision Boundary (Nonseparable Data)\")\nplt.show()\n\n# Plot evolution of loss.\nplt.figure(figsize=(6,4))\nplt.plot(loss_vec, marker='o', markersize=3, color='slategrey')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Evolution (Nonseparable Data)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn our experiment above , I tested the perceptron algorithm on 2D data that is not linearly separable by increasing the noise when generating data with perceptron_data().\n\nWe use a noise level of 0.6 which causes the two classes to overlap significantly. This makes it impossible to draw a perfect straight line that separates all the points.\nWe then train the perceptron for up to 1000 iterations. Like before, we calculate the loss at each step and perform updates on a randomly chosen data point.\nAfter training, we plot the loss over time and the final decision boundary. Since the data is not linearly separable, the loss never reaches zero. The model keeps fluctuating as it tries to fit the noisy data.\n\nThe plot of the final decision boundary shows that the perceptron still finds a line that roughly splits the two classes, but some red and blue points are still on the wrong side. The loss curve also shows this as it goes around up and down and never stays flat like it did in the previous experiments. This shows a key limitation of the basic perceptron that it only works well when the data is linearly separable"
  },
  {
    "objectID": "posts/perceptron/index.html#conclusion",
    "href": "posts/perceptron/index.html#conclusion",
    "title": "Implementing Perceptron",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I learned to build and implement the Perceptron algorithm using PyTorch. I started with linearly separable data and showed that the perceptron can successfully learn a straight-line boundary that separates the two classes. Then, I tried the algorithm on higher-dimensional data and saw that it still learns well. Finally, I tested it on noisy, nonseparable data and noticed that the perceptron could not find a perfect boundary, and the loss never reached zero.\nThese experiments show that the perceptron works best when the data is linearly separable. It’s fast and simple, but has limitations when data is noisy or can’t be split with a straight line.\nI also learnt how the perceptron behaves under different minibatch sizes. With small batches (like 1 or 10), the model updates quickly but sometimes with more noise. Using the entire dataset as a batch (size 300) made the updates smoother and more stable, though slightly slower. Overall, these experiments helped me understand how minibatch size affects convergence and learning behavior in perceptron training."
  },
  {
    "objectID": "posts/perceptron/index.html#implementing-minibatch-perceptron",
    "href": "posts/perceptron/index.html#implementing-minibatch-perceptron",
    "title": "Implementing Perceptron",
    "section": "Implementing Minibatch Perceptron",
    "text": "Implementing Minibatch Perceptron"
  },
  {
    "objectID": "posts/perceptron/index.html#walk-through-of-grad-function-in-minibatch-implementation",
    "href": "posts/perceptron/index.html#walk-through-of-grad-function-in-minibatch-implementation",
    "title": "Implementing Perceptron",
    "section": "Walk through of grad function in Minibatch implementation",
    "text": "Walk through of grad function in Minibatch implementation\nThe grad() function in my minibatch implementation is responsible for computing the weight updates for multiple examples at once (a minibatch). I did this by\n\nFirst, I made sure the labels (y) were in the set {-1, 1}. If they weren’t, I converted them using:\ny_mod = torch.where((y == 1) | (y == -1), y, 2 * y - 1)\nThen, I calculated the scores (s) for the entire minibatch simultaneously by multiplying the feature matrix (X) by the weight vector (w):\ns = X @ self.w\nAfter that, I checked which data points were misclassified by seeing where s * y_mod was less than 0.\nIf there were any misclassified points, I computed the gradient by averaging the updates from all misclassified points:\ngrad = -(y_mod[misclassified, None] * X[misclassified]).mean(dim=0)\nIf all points were classified correctly, I simply returned a zero vector (since no update would be needed).\n\nThis matches the minibatch perceptron update rule: if multiple points are wrong, we average their updates together to adjust our weights efficiently. Let’s now perform some experiments with different batch sizes to check how this implementation performs.\nIn the code below, I implemented the Minibatch Perceptron algorithm to examine how varying the batch size (k) impacts the training process and convergence behavior.\nThe function minibatch(batch_size) performs training on randomly selected subsets (minibatches) of data points. Each minibatch is used to update the perceptron weights according to the perceptron learning rule.\nWithin the training loop, the loss is calculated after every update and we store it in the loss_history. If the loss reaches zero (which would indicate perfect classification), the training stops early.\nWe then by visualize the evolution of the loss, showing how quickly and effectively the perceptron learns for each specified minibatch size.\nI will be calling this function with various batch sizes (k=1, k=10, k=n) to show how the perceptron’s learning rate differ across these scenarios:\n\nimport torch\nfrom perceptron import Perceptron, PerceptronOptimizer, perceptron_data\nimport matplotlib.pyplot as plt\n\n# Generate data\nX, y = perceptron_data(n_points=300, noise=0.2)\n# Create a perceptron and an optimizer\nmodel = Perceptron()\noptimizer = PerceptronOptimizer(model)\nmax_iter = 1000\nalpha = 0.1  # Learning rate\n\ndef minibatch(batch_size):\n    # Loss tracking\n    loss_history = []\n\n    for _ in range(max_iter):\n        indices = torch.randperm(X.size(0))[:batch_size]  # random minibatch selection\n        X_batch, y_batch = X[indices], y[indices]\n\n        loss = optimizer.step(X_batch, y_batch, alpha)\n        loss_history.append(loss.item()) # adding loss to history\n\n        # we stop if loss reaches zero (perfect separation)\n        if loss.item() == 0:\n            break\n\n    # Plot loss evolution\n    plt.figure(figsize=(6, 4))\n    plt.plot(loss_history, marker='o')\n    plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Loss\")\n    plt.title(f\"Loss Evolution (Minibatch size: {batch_size})\")\n    plt.show()\n\n\nExperiment: Minibatch Size = 1\nIn this experiment, I set the minibatch size to 1, which is the same as the standard perceptron update rule (one point at a time). As shown in the plot, the model quickly converges — it only takes two iterations for the loss to drop from 1.0 to 0.0. This is because when the data is linearly separable, since each misclassified point directly contributes to updating the weights. It confirms that this minibatch implementation works just like the regular perceptron when k = 1.\n\nminibatch(1)#when batch size is 1\n\n\n\n\n\n\n\n\n\n\nExperiment: Minibatch Size = 10\nIn this experiment, I set the minibatch size to 10. The perceptron model updates its weights using 10 data points at a time instead of just one. As shown in the graph, the loss starts high and fluctuates for the first few steps, but overall it keeps going down. Around the 10th iteration, the model reaches zero loss, which means it has learned to classify the data correctly. This shows that using a small batch size still works well and the model is able to find a good separating line.\n\nminibatch(10)# batch size 10\n\n\n\n\n\n\n\n\n\n\nExperiment: Minibatch Size = n\nIn this experiment, I used a minibatch size of 300 because we have 300 points, which means the perceptron updates its weights using the entire dataset at each step. As shown in the plot, the loss starts off high and stays flat for a few steps, but then it begins to decrease smoothly. By the 12th iteration, the loss reaches almost zero. This shows that even when using the whole dataset at once, the perceptron is still able to learn and improve — just a bit more gradually. The updates are more stable and less noisy compared to smaller batch sizes.\n\nminibatch(300)# batch size 300 which is the number of points"
  },
  {
    "objectID": "posts/perceptron/index.html#runtime-complexity-of-minibatch-implementation",
    "href": "posts/perceptron/index.html#runtime-complexity-of-minibatch-implementation",
    "title": "Implementing Perceptron",
    "section": "Runtime Complexity of Minibatch implementation",
    "text": "Runtime Complexity of Minibatch implementation\nIn my minibatch perceptron implementation, each iteration takes time based on how many points are in the batch and how many features each point has. If the batch has k points and each point has p features, then the runtime is O(kp). This means the more features or bigger the batch, the it will take longer for each update to happen."
  },
  {
    "objectID": "posts/implementing logistic regression/index.html",
    "href": "posts/implementing logistic regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\nFile of logistic.py"
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#abstract",
    "href": "posts/implementing logistic regression/index.html#abstract",
    "title": "Implementing Logistic Regression",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I implemented logistic regression from scratch using PyTorch. I started by creating data to test gradient descent with and without momentum. Then, I created experiments to see how the model behaves under overfitting conditions. Finally, I trained the model on a real-world dataset (Campus Placement Prediction from Kaggle) and evaluated its performance on training, validation, and test splits. This helped me understand how logistic regression works in practice, both on simple and real datasets."
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#experiments",
    "href": "posts/implementing logistic regression/index.html#experiments",
    "title": "Implementing Logistic Regression",
    "section": "Experiments",
    "text": "Experiments\nFirst I am going to setup a function classification_data to ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍generate ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍for a classification problem in our experiments.\n\nimport matplotlib.pyplot as plt\nimport torch\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.5)"
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#vanilla-gradient-descent-when-β-0",
    "href": "posts/implementing logistic regression/index.html#vanilla-gradient-descent-when-β-0",
    "title": "Implementing Logistic Regression",
    "section": "Vanilla Gradient Descent (when β = 0)",
    "text": "Vanilla Gradient Descent (when β = 0)\nIn this experiment, I used logistic regression with vanilla gradient descent, which means I didn’t use any momentum (β = 0). I set the number of features to 2 (p_dims = 2) so that I could plot and visualize the loss later.\nThe goal here was to check if my implementation works correctly. I trained the model for 500 steps and recorded the loss at each step. The graph shows that the loss decreases over time, which means the model is learning to separate the two classes better.\nThis experiment helps make sure the basic gradient descent is working as expected.\n\n# Generate data\nX, y = classification_data(n_points=300, noise=0.3, p_dims=2)\n\n# Train with vanilla gradient descent\nLR_vanilla = LogisticRegression()\nopt_vanilla = GradientDescentOptimizer(LR_vanilla)\n\nloss_history_vanilla = []\n\nfor _ in range(500): #training model for 500 steps\n    loss = LR_vanilla.loss(X, y)\n    loss_history_vanilla.append(loss.item())\n    opt_vanilla.step(X, y, alpha=0.1, beta=0.0)  # No momentum"
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#loss-curve-vanilla-gradient-descent-β-0",
    "href": "posts/implementing logistic regression/index.html#loss-curve-vanilla-gradient-descent-β-0",
    "title": "Implementing Logistic Regression",
    "section": "Loss Curve – Vanilla Gradient Descent (β = 0)",
    "text": "Loss Curve – Vanilla Gradient Descent (β = 0)\nThis graph shows how the loss changes during training when using vanilla gradient descent (with no momentum). We see that the loss starts around 0.69 and goes down steadily as the number of iterations increases.\nThis smooth and steady decrease means the model is learning properly, and the gradient descent algorithm is working as expected. There’s no sudden jump or spike, which is what we want in a good stable learning process.\n\n\nplt.figure(figsize=(6, 4))\nplt.plot(loss_history_vanilla, label=\"Vanilla Gradient Descent\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Over Time (β = 0)\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#gradient-descent-with-momentum-β-0.9",
    "href": "posts/implementing logistic regression/index.html#gradient-descent-with-momentum-β-0.9",
    "title": "Implementing Logistic Regression",
    "section": "Gradient Descent with Momentum (β = 0.9)",
    "text": "Gradient Descent with Momentum (β = 0.9)\nIn this experiment, I trained my logistic regression model on the same data, but this time I used momentum with β = 0.9. We use momentum to help the model learn a lot faster. This should make the loss go down a lot more quickly when compaerd to vanilla gradient descent. I kept the learning rate the same and trained the model for 500 steps to see the full convergence.\n\n#Training it with momentum now\nLR_momentum = LogisticRegression()\nopt_momentum = GradientDescentOptimizer(LR_momentum)\n\nloss_history_momentum = []\n\nfor _ in range(500): #training model for 500 steps\n    loss = LR_momentum.loss(X, y)\n    loss_history_momentum.append(loss.item())\n    opt_momentum.step(X, y, alpha=0.1, beta=0.9)"
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#loss-comparison-for-with-and-without-momentum",
    "href": "posts/implementing logistic regression/index.html#loss-comparison-for-with-and-without-momentum",
    "title": "Implementing Logistic Regression",
    "section": "Loss comparison for with and without momentum",
    "text": "Loss comparison for with and without momentum\nThe graph below compares how the loss changes over time for the two models:\n\nOne trained with no momentum (β = 0, blue line)\nOne trained with momentum (β = 0.9, orange line)\n\nWe can see that the model with momentum learns much faster. The orange curve drops more quickly and reaches a lower loss in fewer steps. This shows that momentum helps the model converge faster by speeding up learning in the right direction. Whereas the model without moemntum still learns but it is a more slowl. This experiment clearly shows the benefit of using momentum in gradient descent.\n\nplt.figure(figsize=(6, 4))\nplt.plot(loss_history_vanilla, label=\"No Momentum (β=0)\")\nplt.plot(loss_history_momentum, label=\"With Momentum (β=0.9)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Comparison: Vanilla vs Momentum\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#overfitting-high-p_dim-low-n_points",
    "href": "posts/implementing logistic regression/index.html#overfitting-high-p_dim-low-n_points",
    "title": "Implementing Logistic Regression",
    "section": "Overfitting: High p_dim, Low n_points",
    "text": "Overfitting: High p_dim, Low n_points\nIn this experiment, I wanted to create a situation where my model would overfit. I set up the data with only 50 examples, but gave each one 100 features (p_dim = 100). This makes it easy for the model to memorize the training data.\nTo make the overfitting effect more noticeable, I added:\n\nLow noise to the training data, so the model could learn it perfectly\nHigh noise to the test data, so it would be much harder and look different\n\nI trained the model using gradient descent with momentum (β = 0.9) for 100 iterations.\nThe model achieved very high accuracy on the training data, but much lower accuracy on the test data. This showed classic overfitting where my model learned the training examples really well and was not able to generalize to new, (noisier) examples.\n\n# Create overfitting data\nX_train, y_train = classification_data(n_points=50, p_dims=100, noise=0.1)\nX_test, y_test = classification_data(n_points=50, p_dims=100, noise=0.8)\n\n# Train the model\nLR_overfit = LogisticRegression()\nopt_overfit = GradientDescentOptimizer(LR_overfit)\n\nfor _ in range(500):\n    opt_overfit.step(X_train, y_train, alpha=0.1, beta=0.9)\n\n# Evaluate on training and test sets\ntrain_preds = (torch.sigmoid(LR_overfit.score(X_train)) &gt;= 0.5).float()\ntest_preds = (torch.sigmoid(LR_overfit.score(X_test)) &gt;= 0.5).float()\n\ntrain_acc = (train_preds == y_train).float().mean().item()\ntest_acc = (test_preds == y_test).float().mean().item()\n\nprint(f\"Training Accuracy: {train_acc:.2f}\")\nprint(f\"Test Accuracy: {test_acc:.2f}\")\n\nTraining Accuracy: 1.00\nTest Accuracy: 0.92"
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#performance-on-empirical-data",
    "href": "posts/implementing logistic regression/index.html#performance-on-empirical-data",
    "title": "Implementing Logistic Regression",
    "section": "Performance on Empirical Data",
    "text": "Performance on Empirical Data\nDataset: Campus Placement Prediction\nI used a dataset from Kaggle called Campus Placement PredictionROSHAN (2024) . This dataset includes information about students such as:\n\nEducation percentages (high school, college, MBA)\n\nDegree types and specializations\n\nWork experience and test scores\n\nWhether they got placed — this is our target variable\n\nOur goal is to predict the outcome of candidate selection during campus placement processes using logistic regression\nFor this empirical experiment, I worked with student profiles and their final placement status (placed or not placed). The features include academic scores, education streams, specialization, work experience, and more.\nOur target variable is status — whether the student was placed (1) or not placed (0). The task is to predict campus placement outcomes based on these features.\nROSHAN (2024)\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Loading our dataset\ndf = pd.read_csv(\"Campus_Selection.csv\") \n\n# Dropping rows with missing values\ndf.dropna(inplace=True)\n\n# one hot encoding the target column\ndf['status'] = df['status'].map({'Placed': 1, 'Not Placed': 0})\n\n# Features and target\nX_raw = df.drop('status', axis=1)\ny = df['status'].values"
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#preprocessing-the-data",
    "href": "posts/implementing logistic regression/index.html#preprocessing-the-data",
    "title": "Implementing Logistic Regression",
    "section": "Preprocessing the Data",
    "text": "Preprocessing the Data\nFor the preprocessing, I scaled the numeric features (like percentages) so they are all on a similar scale. Then, I one-hot encoded the categorical features (like gender or degree type) to turn them into numbers the model can understand. Finally, I converted everything to PyTorch tensors so I could use them in my logistic regression model.\n\nimport numpy as np\nimport torch\n\n#One hot encoding the categoricals and standarising the numerical colums\ncat_cols = X_raw.select_dtypes(include='object').columns.tolist()\nnum_cols = X_raw.select_dtypes(include='number').columns.tolist()\n\npreprocessor = ColumnTransformer([\n    ('num', StandardScaler(), num_cols),\n    ('cat', OneHotEncoder(drop='first'), cat_cols)\n])\n\n# Fit transform\nX_processed = preprocessor.fit_transform(X_raw)\nX_processed = X_processed.toarray() if hasattr(X_processed, \"toarray\") else X_processed \n\n# Add bias column of 1s\nX_processed = np.concatenate([X_processed, np.ones((X_processed.shape[0], 1))], axis=1)\n\n# Convert to PyTorch\nX_tensor = torch.tensor(X_processed, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32)\n\nIn my code below I created a function to train logistic regression with a certain beta which will be the momentum for the the training. And, I will plot my loss after.\n\n\ndef train_model(X, y, alpha=0.1, beta=0.0, steps=500):\n    model = LogisticRegression()\n    optimizer = GradientDescentOptimizer(model)\n    losses = []\n\n    for _ in range(steps):\n        loss = model.loss(X, y)\n        losses.append(loss.item())\n        optimizer.step(X, y, alpha=alpha, beta=beta)\n\n    return model, losses\n\nAfter training the model with and without momentum, I plotted the training loss over time. As expected, the model with momentum (orange) learns faster and reaches a lower loss more quickly than the model without momentum (blue). This shows that momentum helps speed up convergence.\n\nmodel_nomomentum, loss_no_mom = train_model(X_train, y_train, alpha=0.1, beta=0.0)\nmodel_momentum, loss_with_mom = train_model(X_train, y_train, alpha=0.1, beta=0.9)\n\n# Plot\nplt.figure(figsize=(6, 4))\nplt.plot(loss_no_mom, label=\"No Momentum (β = 0)\")\nplt.plot(loss_with_mom, label=\"With Momentum (β = 0.9)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training Loss Over Time\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#evaluating-final-accuracy-for-our-model-on-the-datset",
    "href": "posts/implementing logistic regression/index.html#evaluating-final-accuracy-for-our-model-on-the-datset",
    "title": "Implementing Logistic Regression",
    "section": "Evaluating final accuracy for our model on the datset",
    "text": "Evaluating final accuracy for our model on the datset\nAfter training the model on the empirical dataset, I tested it on different splits of the data and got my reults as:\n\nTraining Accuracy: 95%\nValidation Accuracy: 81%\nTest Accuracy: 74%\n\nThis shows that the model learned the training data well and also performed reasonably on unseen data, although there is some drop in test accuracy. This is expected and shows that while the model generalizes fairly well, there’s still some fine tuning I could do.\n\n# Split 60% train, 20% validation, 20% test\nX_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42) # splitting the 40% of data to validation adn test split\n\nmodel_momentum, loss_with_mom = train_model(X_train, y_train, alpha=0.1, beta=0.9)\n\ndef evaluate(model, X, y):\n    preds = (torch.sigmoid(model.score(X)) &gt;= 0.5).float()\n    accuracy = (preds == y).float().mean().item()\n    return accuracy\n\ntrain_acc = evaluate(model_momentum, X_train, y_train)\nval_acc = evaluate(model_momentum, X_val, y_val)\ntest_acc = evaluate(model_momentum, X_test, y_test)\n\nprint(f\"Train Accuracy: {train_acc:.2f}\")\nprint(f\"Validation Accuracy: {val_acc:.2f}\")\nprint(f\"Test Accuracy: {test_acc:.2f}\")\n\nTrain Accuracy: 0.95\nValidation Accuracy: 0.84\nTest Accuracy: 0.74\n\n\nIn our code below, we plot both the training and validation loss for our data and see how training with and without moemntum affects it.\n\n# using our Preprocessed full data first\nX_processed = preprocessor.fit_transform(X_raw)\nX_processed = np.concatenate([X_processed, np.ones((X_processed.shape[0], 1))], axis=1)\n\n# Then convering it to a tensor\nX_tensor = torch.tensor(X_processed, dtype=torch.float32)\ny_tensor = torch.tensor(y, dtype=torch.float32)\n\n#splitting it\nX_train, X_temp, y_train, y_temp = train_test_split(X_tensor, y_tensor, test_size=0.4, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n\ndef train_with_val(X_train, y_train, X_val, y_val, alpha=0.1, beta=0.0, steps=200):\n    model = LogisticRegression()\n    optimizer = GradientDescentOptimizer(model)\n    train_losses = []\n    val_losses = []\n\n    for _ in range(steps):\n        train_loss = model.loss(X_train, y_train)\n        val_loss = model.loss(X_val, y_val)\n        train_losses.append(train_loss.item())\n        val_losses.append(val_loss.item())\n        optimizer.step(X_train, y_train, alpha=alpha, beta=beta)\n\n    return model, train_losses, val_losses\n\n# training models\nmodel_van, train_loss_van, val_loss_van = train_with_val(X_train, y_train, X_val, y_val, beta=0.0)\nmodel_mom, train_loss_mom, val_loss_mom = train_with_val(X_train, y_train, X_val, y_val, beta=0.9)\n# Plotting the training and validation loss\nplt.figure(figsize=(6, 4))\nplt.plot(train_loss_van, label=\"Train (β=0)\")\nplt.plot(val_loss_van, label=\"Val (β=0)\")\nplt.plot(train_loss_mom, label=\"Train (β=0.9)\")\nplt.plot(val_loss_mom, label=\"Val (β=0.9)\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Train vs Validation Loss (With/Without Momentum)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nIn my plot above, I trained two logistic regression models: one with momentum (β = 0.9) and one without momentum (β = 0). For both models, I stored the training loss and validation loss over 200 iterations.\nThe green line (Train β = 0.9) shows that the model with momentum quickly reduces training loss and reaches a low value faster than the one without momentum (blue line).\nThe red line (Val β = 0.9) also shows that validation loss drops quickly, but after a point, it flattens, which suggests the model might start overfitting if it was going to train longer.\nThe orange line (Val β = 0) decreases more slowly, meaning the model without momentum learns more gradually and generalizes more slowly too."
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#conclusion",
    "href": "posts/implementing logistic regression/index.html#conclusion",
    "title": "Implementing Logistic Regression",
    "section": "Conclusion",
    "text": "Conclusion\nWhat I learned from this project is how to build a working logistic regression model and train it using gradient descent. I tested the effect of momentum and saw that it helps the model learn faster. I also created an overfitting scenario and saw that a model can do really well on the training data but not on unseen data if it’s overfitted.\nFinally, I applied the model to a real-world dataset and saw that it achieved high training accuracy and reasonable generalization. This whole process helped me understand how preprocessing, training, and different values of beta (momentum) affect the model’s learning and how they impact the loss over time."
  },
  {
    "objectID": "posts/Overfitting, Overparameterization, and Double Descent/index.html",
    "href": "posts/Overfitting, Overparameterization, and Double Descent/index.html",
    "title": "Overfitting, Overparameterization, and Double Descent",
    "section": "",
    "text": "In this post I will be looking into the concepts of overfitting, overparameterization, and double descent through some linear regression experiments using random features. I began by implementing a custom linear model and optimizer that supports overparameterized settings using the Moore-Penrose pseudoinverse. I thentested the model on both synthetic nonlinear data and a real-world image corruption task. By changinge the number of random features, I was able to observe the double descent pattern in test error, showing how the model performance can improve even beyond the interpolation threshold.\n\n\nIn Equation 1, the equation \\((X^T X)^{-1}\\) requires for \\(X^T X\\) to be invertible. And this can only happen when the number of data points \\(n\\) is greater than or equal to the number of features \\(p\\). So if we have \\(p &gt; n\\), then \\(X^T X\\) becomes a singular matrix and it doesn’t have full rank( meaning it cant be inverted then)so our formula wont work.\n\n\n\nIn my LY.py file I created a regression model called MyLinearRegression that predicts outputs and calculates the mean-squared error loss. I also implemented an optimizer OverParameterizedLinearRegressionOptimizer that fits the model using the pseudoinverse. This allows the model to work even in the overparameterized setting, when our traditional matrix inversion is not possible.\n\n# Import our libraries\n%load_ext autoreload\n%autoreload 2\nfrom LR import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nIn the code below, I generated a 1D dataset. I also created a randmon features class to apply a random feature transformation to make the data linearly separable in the new feature space, and then I will fit my linear regression model to this transfomed data.\n\n\n# creating 1d data\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype=torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\n#Define square activation\ndef square(x):\n    return x**2\n\n# Creating a RandomFeatures class to generate random features\nclass RandomFeatures:\n    def __init__(self, n_features, activation=square):\n        self.n_features = n_features\n        self.activation = activation\n        self.u = None\n        self.b = None\n\n    def fit(self, X):\n        self.u = torch.randn((X.size(1), self.n_features), dtype=torch.float64)\n        self.b = torch.rand((self.n_features), dtype=torch.float64)\n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)\n\nI will now transorm our features and fit our model based of teh model we created. I then did teh predictions and plotted a graph to view it.\n\n\n# trnasform our features and fit model\nphi = RandomFeatures(n_features=300, activation=square)\nphi.fit(X)\n\nX_phi = phi.transform(X)\n\nmodel = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(model)\nopt.fit(X_phi, y)\n\n# making our prediction and plotting them\ny_pred = model.predict(X_phi).detach()\n\nplt.figure(figsize=(8, 5))\nplt.scatter(X.numpy(), y.numpy(), color='darkgrey', label='Data')\nplt.plot(X.numpy(), y_pred.numpy(), color='steelblue', label='Predictions')\nplt.title(\"Overparameterized Linear Regression\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn our graph about our plot shows that the model gets the general trend of the data very well. Despite the noise in the data, the predictions (which is the blue line) follow the overall U-shaped curve, which matches the true underlying function. This tell us that the random feature mapping allowed the linear model to learn a nonlinear relationship. This showse that our overparameterized linear regression setup is working well.\n\n\n\nSo we are trying to predict how many corruptions there are in an image. I will do this by first loading our image, then corrupting it multiple times to create a dataset. We will the fit our model using RandomFeatures and the vary n_features to see the double descent in test and training error.\n\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\nLets add our function that function that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍adds ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍random ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number of randomly sized greay patches to the image and also returns teh number of patches added ‍‍‍‍‍‍‍‍‍‍‍‍‍. Lets then create a corrupted image using that function.\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n# Create a corrupted image\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nIn our code below we create a dataset of corrupted imagesand also ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍reshape ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍image. ‍‍‍‍‍We then split ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍into ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍training ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍test ‍sets.\n\nn_samples = 200\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype=torch.float64)\ny = torch.zeros(n_samples, dtype=torch.float64)\n\nfor i in range(n_samples):\n    X[i], y[i] = corrupted_image(flower, mean_patches=100)\n\nX = X.reshape(n_samples, -1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n\n\n\nTo see how changing the number of features affects the performance of my model, I used the same RandomFeatures transformation from before, but each time with a different number of features—from 10 to 200.\nFor each value of n_features:\n\nI Created random features for the training and test data,\nTrained my linear regression model using those features,\nMeasured how well the model did on both the training and test data.\n\nThen I stored the training and test errors so I can plot how they change as the model becomes more complex whic I will use later to see the double descent happening.\n\n## we will use the same random features function as before\n\ntrain_errors = []\ntest_errors = []\nn_feature_list = list(range(10, 201))\n\nfor n_features in n_feature_list:\n    phi = RandomFeatures(n_features=n_features, activation=square) # creating random features\n    phi.fit(X_train) # fitting the random features\n    X_train_phi = phi.transform(X_train) # transforming the training data\n    X_test_phi = phi.transform(X_test) # transforming the test data\n\n    model = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(model)\n    opt.fit(X_train_phi, y_train)\n\n    train_loss = model.loss(X_train_phi, y_train).item() # calculating the loss on our training data\n    test_loss = model.loss(X_test_phi, y_test).item() # calculating the loss on our test data\n    \n    train_errors.append(train_loss)# adding the training loss to the list\n    test_errors.append(test_loss) # adding the test loss to the list\n\nPlotting our results\n\ninterp_threshold = X_train.shape[0]\n\nplt.figure(figsize=(12, 5))\n\n# Training Error\nplt.subplot(1, 2, 1)\nplt.plot(n_feature_list, train_errors, 'o', color='gray')\nplt.axvline(interp_threshold, color='black', linestyle='--')\nplt.yscale('log')\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Mean squared error (training)\")\nplt.title(\"Training Error\")\n\n# Test Error\nplt.subplot(1, 2, 2)\nplt.plot(n_feature_list, test_errors, 'o', color='brown')\nplt.axvline(interp_threshold, color='black', linestyle='--')\nplt.yscale('log')\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Mean squared error (testing)\")\nplt.title(\"Testing Error\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFrom the plots, we can see the double descent happening. As we increase the number of random features:\n\nThe training error decreases and becomes almost zero after the interpolation threshold (where the number of features equals the number of training samples).\nThe test error first decreases and then it goes up a little near the threshold, and then decreases again as we go into the overparameterized area.\n\nThe best testing error occurred at around 180 features, which is above the interpolation threshold of 100. This supports the idea that interpolation is not always bad, and going beyond the threshold can still sometmies lead to a better generalization.\n\n\n\nThrough these experiments, I learned how increasing model complexity and overfitting does not always lead to worse generalization;even when a model can perfectly fit the training data. The double descent shows us that test error can decrease again after the interpolation threshold, contradicting the classical view of overfitting. This taught me that, with the right feature transformations, highly overparameterized models can still be effective. It was very interesting to learn that overfittingis not always bad, if we can find the right balance between model size, data, and feature transformations."
  },
  {
    "objectID": "posts/Overfitting, Overparameterization, and Double Descent/index.html#abstract",
    "href": "posts/Overfitting, Overparameterization, and Double Descent/index.html#abstract",
    "title": "Overfitting, Overparameterization, and Double Descent",
    "section": "",
    "text": "In this post I will be looking into the concepts of overfitting, overparameterization, and double descent through some linear regression experiments using random features. I began by implementing a custom linear model and optimizer that supports overparameterized settings using the Moore-Penrose pseudoinverse. I thentested the model on both synthetic nonlinear data and a real-world image corruption task. By changinge the number of random features, I was able to observe the double descent pattern in test error, showing how the model performance can improve even beyond the interpolation threshold.\n\n\nIn Equation 1, the equation \\((X^T X)^{-1}\\) requires for \\(X^T X\\) to be invertible. And this can only happen when the number of data points \\(n\\) is greater than or equal to the number of features \\(p\\). So if we have \\(p &gt; n\\), then \\(X^T X\\) becomes a singular matrix and it doesn’t have full rank( meaning it cant be inverted then)so our formula wont work.\n\n\n\nIn my LY.py file I created a regression model called MyLinearRegression that predicts outputs and calculates the mean-squared error loss. I also implemented an optimizer OverParameterizedLinearRegressionOptimizer that fits the model using the pseudoinverse. This allows the model to work even in the overparameterized setting, when our traditional matrix inversion is not possible.\n\n# Import our libraries\n%load_ext autoreload\n%autoreload 2\nfrom LR import MyLinearRegression, OverParameterizedLinearRegressionOptimizer\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\n\n\nIn the code below, I generated a 1D dataset. I also created a randmon features class to apply a random feature transformation to make the data linearly separable in the new feature space, and then I will fit my linear regression model to this transfomed data.\n\n\n# creating 1d data\nX = torch.tensor(np.linspace(-3, 3, 100).reshape(-1, 1), dtype=torch.float64)\ny = X**4 - 4*X + torch.normal(0, 5, size=X.shape)\n\n#Define square activation\ndef square(x):\n    return x**2\n\n# Creating a RandomFeatures class to generate random features\nclass RandomFeatures:\n    def __init__(self, n_features, activation=square):\n        self.n_features = n_features\n        self.activation = activation\n        self.u = None\n        self.b = None\n\n    def fit(self, X):\n        self.u = torch.randn((X.size(1), self.n_features), dtype=torch.float64)\n        self.b = torch.rand((self.n_features), dtype=torch.float64)\n\n    def transform(self, X):\n        return self.activation(X @ self.u + self.b)\n\nI will now transorm our features and fit our model based of teh model we created. I then did teh predictions and plotted a graph to view it.\n\n\n# trnasform our features and fit model\nphi = RandomFeatures(n_features=300, activation=square)\nphi.fit(X)\n\nX_phi = phi.transform(X)\n\nmodel = MyLinearRegression()\nopt = OverParameterizedLinearRegressionOptimizer(model)\nopt.fit(X_phi, y)\n\n# making our prediction and plotting them\ny_pred = model.predict(X_phi).detach()\n\nplt.figure(figsize=(8, 5))\nplt.scatter(X.numpy(), y.numpy(), color='darkgrey', label='Data')\nplt.plot(X.numpy(), y_pred.numpy(), color='steelblue', label='Predictions')\nplt.title(\"Overparameterized Linear Regression\")\nplt.xlabel(\"X\")\nplt.ylabel(\"y\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nIn our graph about our plot shows that the model gets the general trend of the data very well. Despite the noise in the data, the predictions (which is the blue line) follow the overall U-shaped curve, which matches the true underlying function. This tell us that the random feature mapping allowed the linear model to learn a nonlinear relationship. This showse that our overparameterized linear regression setup is working well.\n\n\n\nSo we are trying to predict how many corruptions there are in an image. I will do this by first loading our image, then corrupting it multiple times to create a dataset. We will the fit our model using RandomFeatures and the vary n_features to see the double descent in test and training error.\n\nfrom sklearn.datasets import load_sample_images\nfrom scipy.ndimage import zoom\n\ndataset = load_sample_images()     \nX = dataset.images[1]\nX = zoom(X,.2) #decimate resolution\nX = X.sum(axis = 2)\nX = X.max() - X \nX = X / X.max()\nflower = torch.tensor(X, dtype = torch.float64)\n\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(flower)\noff = ax.axis(\"off\")\n\n\n\n\n\n\n\n\nLets add our function that function that ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍adds ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍a ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍random ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍number of randomly sized greay patches to the image and also returns teh number of patches added ‍‍‍‍‍‍‍‍‍‍‍‍‍. Lets then create a corrupted image using that function.\n\ndef corrupted_image(im, mean_patches = 5): \n    n_pixels = im.size()\n    num_pixels_to_corrupt = torch.round(mean_patches*torch.rand(1))\n    num_added = 0\n\n    X = im.clone()\n\n    for _ in torch.arange(num_pixels_to_corrupt.item()): \n        \n        try: \n            x = torch.randint(0, n_pixels[0], (2,))\n\n            x = torch.randint(0, n_pixels[0], (1,))\n            y = torch.randint(0, n_pixels[1], (1,))\n\n            s = torch.randint(5, 10, (1,))\n            \n            patch = torch.zeros((s.item(), s.item()), dtype = torch.float64) + 0.5\n\n            # place patch in base image X\n            X[x:x+s.item(), y:y+s.item()] = patch\n            num_added += 1\n\n            \n        except: \n            pass\n\n    return X, num_added\n\n# Create a corrupted image\n\nX, y = corrupted_image(flower, mean_patches = 50)\nfig, ax = plt.subplots(1, 1, figsize=(6, 6))\nax.imshow(X.numpy(), vmin = 0, vmax = 1)\nax.set(title = f\"Corrupted Image: {y} patches\")\noff = plt.gca().axis(\"off\")\n\n\n\n\n\n\n\n\nIn our code below we create a dataset of corrupted imagesand also ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍reshape ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍image. ‍‍‍‍‍We then split ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍into ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍training ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍test ‍sets.\n\nn_samples = 200\nX = torch.zeros((n_samples, flower.size()[0], flower.size()[1]), dtype=torch.float64)\ny = torch.zeros(n_samples, dtype=torch.float64)\n\nfor i in range(n_samples):\n    X[i], y[i] = corrupted_image(flower, mean_patches=100)\n\nX = X.reshape(n_samples, -1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n\n\n\n\nTo see how changing the number of features affects the performance of my model, I used the same RandomFeatures transformation from before, but each time with a different number of features—from 10 to 200.\nFor each value of n_features:\n\nI Created random features for the training and test data,\nTrained my linear regression model using those features,\nMeasured how well the model did on both the training and test data.\n\nThen I stored the training and test errors so I can plot how they change as the model becomes more complex whic I will use later to see the double descent happening.\n\n## we will use the same random features function as before\n\ntrain_errors = []\ntest_errors = []\nn_feature_list = list(range(10, 201))\n\nfor n_features in n_feature_list:\n    phi = RandomFeatures(n_features=n_features, activation=square) # creating random features\n    phi.fit(X_train) # fitting the random features\n    X_train_phi = phi.transform(X_train) # transforming the training data\n    X_test_phi = phi.transform(X_test) # transforming the test data\n\n    model = MyLinearRegression()\n    opt = OverParameterizedLinearRegressionOptimizer(model)\n    opt.fit(X_train_phi, y_train)\n\n    train_loss = model.loss(X_train_phi, y_train).item() # calculating the loss on our training data\n    test_loss = model.loss(X_test_phi, y_test).item() # calculating the loss on our test data\n    \n    train_errors.append(train_loss)# adding the training loss to the list\n    test_errors.append(test_loss) # adding the test loss to the list\n\nPlotting our results\n\ninterp_threshold = X_train.shape[0]\n\nplt.figure(figsize=(12, 5))\n\n# Training Error\nplt.subplot(1, 2, 1)\nplt.plot(n_feature_list, train_errors, 'o', color='gray')\nplt.axvline(interp_threshold, color='black', linestyle='--')\nplt.yscale('log')\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Mean squared error (training)\")\nplt.title(\"Training Error\")\n\n# Test Error\nplt.subplot(1, 2, 2)\nplt.plot(n_feature_list, test_errors, 'o', color='brown')\nplt.axvline(interp_threshold, color='black', linestyle='--')\nplt.yscale('log')\nplt.xlabel(\"Number of features\")\nplt.ylabel(\"Mean squared error (testing)\")\nplt.title(\"Testing Error\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFrom the plots, we can see the double descent happening. As we increase the number of random features:\n\nThe training error decreases and becomes almost zero after the interpolation threshold (where the number of features equals the number of training samples).\nThe test error first decreases and then it goes up a little near the threshold, and then decreases again as we go into the overparameterized area.\n\nThe best testing error occurred at around 180 features, which is above the interpolation threshold of 100. This supports the idea that interpolation is not always bad, and going beyond the threshold can still sometmies lead to a better generalization.\n\n\n\nThrough these experiments, I learned how increasing model complexity and overfitting does not always lead to worse generalization;even when a model can perfectly fit the training data. The double descent shows us that test error can decrease again after the interpolation threshold, contradicting the classical view of overfitting. This taught me that, with the right feature transformations, highly overparameterized models can still be effective. It was very interesting to learn that overfittingis not always bad, if we can find the right balance between model size, data, and feature transformations."
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#plotting-decision-boundary-for-vanilla-dradient-descent-wehn-paradim-2",
    "href": "posts/implementing logistic regression/index.html#plotting-decision-boundary-for-vanilla-dradient-descent-wehn-paradim-2",
    "title": "Implementing Logistic Regression",
    "section": "Plotting Decision Boundary for Vanilla dradient descent wehn Paradim = 2",
    "text": "Plotting Decision Boundary for Vanilla dradient descent wehn Paradim = 2\n\nimport numpy as np\n\ndef plot_decision_boundary(model, X, y):\n    x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5\n    x2_min, x2_max = X[:,1].min()-0.5, X[:,1].max()+0.5\n    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n                           np.linspace(x2_min, x2_max, 100))\n\n    # Add bias column to grid\n    grid = np.c_[xx1.ravel(), xx2.ravel()]\n    grid = np.concatenate([grid, np.ones((grid.shape[0], 1))], axis=1)\n    grid_tensor = torch.tensor(grid, dtype=torch.float32)\n\n    preds = (torch.sigmoid(model.score(grid_tensor)) &gt;= 0.5).float()\n    preds = preds.reshape(xx1.shape)\n\n    plt.contourf(xx1, xx2, preds, alpha=0.3, cmap='bwr')\n    plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr', edgecolor='k')\n    plt.xlabel(\"x1\")\n    plt.ylabel(\"x2\")\n    plt.title(\"Decision Boundary\")\n    plt.grid(True)\n    plt.show()\n\n# Plot decision boundary for vanilla gradient descent\nplot_decision_boundary(LR_vanilla, X.numpy(), y.numpy())"
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#plotting-decision-bounday-for-our-data-based-of-our-model-taht-we-trained-with-moemntum",
    "href": "posts/implementing logistic regression/index.html#plotting-decision-bounday-for-our-data-based-of-our-model-taht-we-trained-with-moemntum",
    "title": "Implementing Logistic Regression",
    "section": "Plotting Decision Bounday for our data based of our model taht we trained with moemntum",
    "text": "Plotting Decision Bounday for our data based of our model taht we trained with moemntum\nThis plot shows the decision boundary made by the logistic regression model trained with momentum. The red and blue areas show the prediction for each class. The model separated the two groups of data points (red and blue dots)really well and learned a good boundary between the classes. This shows that momentum helped the model find an accurate classifier. Infact, in comparison with our previous graph, this one is more accurate and has less misclassified points too.\n\nplot_decision_boundary(LR_momentum, X.numpy(), y.numpy())"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html",
    "href": "posts/Kernel_Machines/index.html",
    "title": "Sparse Kernel Machines",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nfrom sparse_kernel_logistic import KernelLogisticRegression"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#abstract",
    "href": "posts/Kernel_Machines/index.html#abstract",
    "title": "Sparse Kernel Machines",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I implemented a sparse kernel logistic regression model. This model uses a kernel function to allow logistic regression to learn nonlinear patterns, and it uses \\(\\ell_1\\) regularization to make the model sparse by setting many weights to zero.\nI did many experiments to show and learn how the model behaves: I show how a large regularization strength (λ) makes the model sparse, how changing the kernel parameter (γ) affects the decision boundary, and alsohow the model can fit nonlinear data like the make_moons dataset. Finally, I showed an example of overfitting by using a high γ and comparing training and testing performance with ROC curves."
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#goal-of-project",
    "href": "posts/Kernel_Machines/index.html#goal-of-project",
    "title": "Sparse Kernel Machines",
    "section": "Goal of Project",
    "text": "Goal of Project\nPredicting a penguin’s species based on its physical measurements is an interesting challenge in data analysis and machine learning. In this blog, we’ll explore the Palmer Penguins dataset, which was collected by Dr. Kristen Gorman and the Palmer Station, to analyze biological measurements of three penguin species in Antarctica. Through data visualization and feature selection, we’ll uncover key insights and build a classification model that achieves perfect accuracy using a carefully chosen set of features. Along the way, we’ll discuss our findings and ensure a reproducible and insightful approach to species classification."
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#loading-data",
    "href": "posts/Kernel_Machines/index.html#loading-data",
    "title": "Sparse Kernel Machines",
    "section": "Loading Data",
    "text": "Loading Data\nWe will now use the Pandas function to read the CSV and also take a look at the data and the different variables and columns we have in our dataset before we do and data explorations and analysis.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#data-cleaning",
    "href": "posts/Kernel_Machines/index.html#data-cleaning",
    "title": "Sparse Kernel Machines",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nOur dataset contains numerous variables, including comments and region information, that are not relevant for training our model. Since we will be focusing on variables such as island, culmen length, culmen depth, flipper length, and body mass, we will remove unnecessary columns before proceeding with data exploration, visualization, and model training. Let’s start by loading the necessary packages for data cleaning, visualization, and model training. We will also convert categorical feature columns like Sex and Island into one-hot encoded 0-1( or true or false) columns using the pd.get_dummies function. Additionally, we can see that Species is a categorical variable, but instead of one-hot encoding, we will use label encoding to convert it into numerical labels: 0 for Adelie, 1 for Chinstrap, and 2 for Gentoo. This transformation will make data visualization and model training much easier.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode categorical features and label\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Stage\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    # One-hot encode categorical variables\n    df =(pd.get_dummies(df)).astype(int)\n\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40\n16\n187\n3200\n9\n-24\n0\n1\n0\n0\n1\n1\n0\n\n\n1\n49\n19\n210\n3950\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n2\n50\n15\n218\n5700\n8\n-25\n1\n0\n0\n0\n1\n0\n1\n\n\n3\n45\n14\n210\n4200\n7\n-25\n1\n0\n0\n0\n1\n1\n0\n\n\n4\n51\n18\n203\n4100\n9\n-24\n0\n1\n0\n0\n1\n0\n1"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#visualizing-the-data",
    "href": "posts/Kernel_Machines/index.html#visualizing-the-data",
    "title": "Sparse Kernel Machines",
    "section": "Visualizing the data",
    "text": "Visualizing the data\nThe bar chart below shows the number of penguins of each species on different islands, helping us identify patterns in species distribution. Interestingly, we observe:\n\nTorgersen Island: Only Adelie penguins are present, with a population of about 30-40 individuals.\nDream Island: Has both Adelie and Chinstrap penguins, with Chinstrap being the dominant species.\nBiscoe Island: Has both Adelie and Gentoo penguins, with Gentoo having the largest population overall.\n\nChinstrap penguins are only found on Dream Island, while Gentoo penguins are exclusive to Biscoe Island. Adelie penguins, on the other hand, are the most widespread, appearing on all three islands. Based on this distribution, the island where a penguin is found could be a useful feature for predicting its species.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n# Map species labels back to names\nspecies_map = {0: 'Adelie', 1: 'Chinstrap', 2: 'Gentoo'}\nspecies_island_counts = X_train.copy()\nspecies_island_counts[\"Species\"] = [species_map[label] for label in y_train]\n\n# Convert one-hot encoded islands back to a single column\nspecies_island_counts[\"Island\"] = species_island_counts[[\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]].idxmax(axis=1)\nspecies_island_counts[\"Island\"] = species_island_counts[\"Island\"].str.replace(\"Island_\", \"\")\n\n# Count number of penguins per species per island\nplot_data = species_island_counts.groupby([\"Island\", \"Species\"]).size().reset_index(name=\"Count\")\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.barplot(data=plot_data, x=\"Island\", y=\"Count\", hue=\"Species\", palette=\"mako\")\nplt.xlabel(\"Island\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Penguin Species Distribution Across Islands\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, we can see that by simply looking at the islands, we can somewhat predict the species of the penguin. This is a good example of a feature that could help predict the species. However, the model would be much more efficient if each island only hosted one species of penguin. Since some islands have more than one species, we might want to explore additional features. For example, if a penguin is on Biscoe Island and has a certain Culmen length and Culmen depth, can we make more accurate predictions based on those measurements? To explore this, we will create a graph to assess whether including Culmen lenght and Culmen depth is a useful feature for predicting the species on each island.\n\n\n# Plotting the relationship between Culmen Length and Culmen Depth for each species on each island\ng = sns.relplot(data=train, hue=\"Species\", y=\"Culmen Depth (mm)\", x=\"Culmen Length (mm)\", col=\"Island\")\n\ng.set_titles(size=18)  # Increase subplot titles\ng.set_axis_labels(\"Culmen Length (mm)\", \"Culmen Depth (mm)\", fontsize=16)  # Increase axis labels font size\ng._legend.set_title(\"Species\", prop={'size': 16}) \n[g._legend.get_texts()[i].set_fontsize(14) for i in range(len(g._legend.get_texts()))]  # Increase legend text size\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe graph above looks at the relationship between Culmen Depth (mm) and Culmen Length (mm) for different penguin species, separated by island. Each species is color-coded to highlight patterns in their beak dimensions. From this graph, we can observe:\n\nGentoo penguins (orange) tend to have longer culmen lengths and shallower culmen depths.\nAdelie penguins (green) have moderate culmen lengths and a wide range of culmen depths.\nChinstrap penguins (blue) have longer culmen lenghts compared to Adelie penguins but exhibit similar culmen Depths."
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#can-culmen-features-help-in-classification",
    "href": "posts/Kernel_Machines/index.html#can-culmen-features-help-in-classification",
    "title": "Sparse Kernel Machines",
    "section": "Can Culmen Features Help in Classification?",
    "text": "Can Culmen Features Help in Classification?\nLooking at the separability of species by island:\n\nBiscoe Island: Gentoo and Adelie penguins are linearly separable, meaning a simple model could classify them effectively based on culmen features.\nDream Island: Chinstrap and Adelie penguins overlap in culmen depth, making them harder to separate linearly. However, their culmen lengths show some distinction, which could aid classification.\nTorgersen Island: Since only Adelie penguins are found here, culmen features are irrelevant for classification on this island.\n\nThis suggests that culmen depth and length can be strong features for classifying species, especially when combined with island location."
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#exploring-culmen-features-in-relation-to-flipper-lenght",
    "href": "posts/Kernel_Machines/index.html#exploring-culmen-features-in-relation-to-flipper-lenght",
    "title": "Sparse Kernel Machines",
    "section": "Exploring Culmen Features in relation to Flipper Lenght",
    "text": "Exploring Culmen Features in relation to Flipper Lenght\nThe graph below explores the relationship between flipper length and culmen length across penguin species. Gentoo penguins have both longer flipper lengths and culmen lengths compared to the other species, making them easily distinguishable. Adelie penguins, in contrast, have shorter values for both features, clustering in the lower-left section of the graph. Chinstrap penguins overlap with Adelie penguins in flipper length but tend to have slightly longer culmen lengths, creating some classification challenges.\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Length')\n\n\n\n\n\n\n\n\n\n\nsns.relplot(data = train, y = 'Culmen Depth (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Depth')\n\n\n\n\n\n\n\n\nThe graph above examines the relationship between flipper length and culmen depth. Here, Gentoo penguins are clearly separable from the other two species due to their significantly shallower culmen depths, forming a distinct cluster in the lower section. Howeber the Adelie and Chinstrap penguins overlap a lot more, particularly in flipper length and culmen depth. While Gentoo penguins can be classified easily, distinguishing between Adelie and Chinstrap penguins would be really difficult because of this clustering."
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#feature-analysis-flipper-and-culmen-measurements",
    "href": "posts/Kernel_Machines/index.html#feature-analysis-flipper-and-culmen-measurements",
    "title": "Sparse Kernel Machines",
    "section": "Feature Analysis: Flipper and Culmen Measurements",
    "text": "Feature Analysis: Flipper and Culmen Measurements\nThe table below shows the mean values of flipper length, culmen length, and culmen depth for each penguin species using the .groupby() function. The table summarizes the average flipper length, culmen length, and culmen depth for each penguin species. Gentoo penguins have the longest flipper and culmen lengths but the shallowest culmen depth. Chinstrap penguins have longer culmen lengths than Adelie penguins but similar culmen depths. Adelie penguins have the shortest flipper and culmen lengths but slightly deeper culmens than Chinstrap. These differences highlight key features that can help classify penguin species.\n\ntrain.groupby('Species').agg({\n    'Flipper Length (mm)': 'mean',\n    'Culmen Length (mm)': 'mean',\n    'Culmen Depth (mm)': 'mean'\n}).reset_index()\n\n\n\n\n\n\n\n\nSpecies\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n\n\n2\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#now-we-are-going-to-chose-features-to-train-our-model",
    "href": "posts/Kernel_Machines/index.html#now-we-are-going-to-chose-features-to-train-our-model",
    "title": "Sparse Kernel Machines",
    "section": "Now we are going to chose features to train our model",
    "text": "Now we are going to chose features to train our model\nBy using the combinations function from the itertools package, we generate different combinations of categorical and continuous features. So we will pair categorical variables like Sex and Clutch Completion with continuous variables such as Culmen Length, Culmen Depth, and Flipper Length. This allows us to explore various feature combinations and evaluate which ones might be most useful for accurately classifying the penguin species.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = None\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]  # Ensure categorical features are one-hot encoded\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        \n        # Train a logistic regression model to evaluate feature combinations\n        model = LogisticRegression(max_iter=1000)  # Increased iterations for better convergence\n        scores = cross_val_score(model, X_train[cols], y_train, cv=5, scoring='accuracy')\n        \n        avg_score = np.mean(scores)\n        if avg_score &gt; best_score:\n            best_score = avg_score\n            best_features = cols\n\nprint(\"Best features:\", best_features)\n\nBest features: ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nAccording to itertools, the best features to use are ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’. However, we wanted to explore how our model would perform based on our exploration of the island features along with the Culmen features. Thus, we will now reassign these as the best features and use them to train our model.\n\n#Rearranging Best featrues so that the first two columns are the quantitative columns and the rest are the qualitative columns\nbest_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nbest_features\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#lets-now-prepare-our-test-dataset",
    "href": "posts/Kernel_Machines/index.html#lets-now-prepare-our-test-dataset",
    "title": "Sparse Kernel Machines",
    "section": "Let’s now prepare our Test Dataset",
    "text": "Let’s now prepare our Test Dataset\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Prepare test data\nX_test, y_test = prepare_data(test)"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#training-and-evaluating-our-model",
    "href": "posts/Kernel_Machines/index.html#training-and-evaluating-our-model",
    "title": "Sparse Kernel Machines",
    "section": "Training and evaluating our Model",
    "text": "Training and evaluating our Model\nIn the code below, we use a Logistic Regression model and train it on the selected features: [‘Culmen Length (mm)’, ‘Culmen Depth (mm)’, ‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’]. We then evaluate the accuracy using the LR.score function. Our training accuracy was 99%, and our testing accuracy—measuring how well the model performed on unseen data—was 100%. The confusion matrix confirms that we correctly classified all penguins with no misclassifications. Specifically, our model correctly identified 31 Adelie, 11 Chinstrap, and 26 Gentoo penguins, achieving perfect classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(X_train[best_features], y_train)\n\n\n\n# Predict on test set\ny_pred = LR.predict(X_test[best_features])\n\n# Evaluate the model\nscore_train = LR.score(X_train[best_features], y_train)\nscore_test = LR.score(X_test[best_features], y_test)\n\nprint(\"Training Accuracy:\", score_train)\nprint(\"Test Accuracy:\", score_test)\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\nTraining Accuracy: 0.9921875\nTest Accuracy: 1.0\n\nConfusion Matrix:\n [[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#decision-region-plots",
    "href": "posts/Kernel_Machines/index.html#decision-region-plots",
    "title": "Sparse Kernel Machines",
    "section": "Decision Region Plots",
    "text": "Decision Region Plots\nThis code below helps us visualize the decision regions of our classification model by plotting how different penguin species are separated based on culmen features and island locations. It creates a grid of values for the culmen length and depth, predicts species across the grid. Each subplot represents a different island, showing how the model’s decision boundaries change depending on the island feature. This helps us understand how the model differentiates between Adelie, Chinstrap, and Gentoo penguins on each island.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    # Define a color mapping where Gentoo is green and Chinstrap is blue\n    species_colors = {0: 'red', 1: 'blue', 2: 'green'}\n    \n    # Adjust figure size for better visibility\n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(12 * len(qual_features), 10))  # Increase size\n    fig.suptitle(title, fontsize=40, y=1.05)  # Adjust title position and size\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    if len(qual_features) == 1:\n        axarr = [axarr] \n\n    for i, ax in enumerate(axarr):\n        XY = pd.DataFrame({\n            X.columns[0]: XX,\n            X.columns[1]: YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # Use contour plot to visualize the predictions\n        ax.contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n        # Convert y[ix] to a pandas Series and apply map to get colors\n        y_series = pd.Series(y[ix])  # Convert y[ix] to a Series\n        ax.scatter(x0[ix], x1[ix], c=y_series.map(species_colors), s=80)  # Larger scatter points\n\n        ax.set_xlabel(X.columns[0], fontsize=30)\n        ax.set_ylabel(X.columns[1], fontsize=30)\n        ax.set_title(qual_features[i], fontsize=35)\n\n        ax.tick_params(axis='both', labelsize=25)  # Larger tick labels\n\n    # Increase spacing between subplots and adjust title positioning\n    plt.subplots_adjust(wspace=0.4, hspace=0.3, top=0.85)  \n\n    # Create legend with specific colors for species\n    patches = [Patch(color=color, label=spec) for spec, color in zip([\"Adelie\", \"Chinstrap\", \"Gentoo\"], [\"red\", \"blue\", \"green\"])]\n    fig.legend(handles=patches, title=\"Species\", loc=\"upper right\", fontsize=30, title_fontsize=35, bbox_to_anchor=(1.03, 1)) \n\n    plt.show()\n\n# Run function for training and test sets\nplot_regions(LR, X_train[best_features], y_train, title=\"Training Set\")\nplot_regions(LR, X_test[best_features], y_pred, title=\"Test Set\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe decision region plots clearly show how our model classifies penguins based on culmen length and depth, with island location influencing the boundaries.\n\nIsland Biscoe: The model effectively separates Gentoo (green) from Adelie (red) penguins, as they occupy distinct regions.\nIsland Dream: Chinstrap (blue) and Adelie (red) penguins have overlapping regions, making classification slightly more challenging. However, longer culmen lengths help differentiate Chinstrap penguins.\nIsland Torgersen: Since only Adlie penguins are found here, the entire region is classified as red, confirming that island location alone can sometimes be a strong predictor.\n\nThe clean decision boundaries in most plots indicate that the selected features work really well for our classification."
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#discussion-and-takeaways",
    "href": "posts/Kernel_Machines/index.html#discussion-and-takeaways",
    "title": "Sparse Kernel Machines",
    "section": "Discussion And Takeaways",
    "text": "Discussion And Takeaways\nThrough this analysis, I gained several important insights about both the dataset and machine learning model performance. One of the key takeaways was the importance of data preprocessing, including encoding categorical variables and handling missing data, to ensure the model could learn effectively. By visualizing relationships between features, I noticed that certain physical traits, like culmen length and culmen depth, played a crucial role in distinguishing penguin species. Additionally, island location proved to be a strong predictor, as some species were exclusive to certain islands.\nTraining a logistic regression model with carefully selected features led to a 99% accuracy on the training set and 100% accuracy on the test set, showing that the model generalized well to unseen data. The decision region plots clearly illustrated how the model separated species based on their culmen features, with some overlap between Adelie and Chinstrap penguins making classification slightly more challenging. Overall, this project demonstrated how feature selection, visualization, and a well-trained model can achieve highly accurate classifications, reinforcing the power of machine learning in biological research."
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#sparse-kernel-logistic-regression-implementation",
    "href": "posts/Kernel_Machines/index.html#sparse-kernel-logistic-regression-implementation",
    "title": "Sparse Kernel Machines",
    "section": "Sparse Kernel Logistic Regression Implementation",
    "text": "Sparse Kernel Logistic Regression Implementation\nFor the implementation of my sparse_kernel_logistic.py where I implemented a custom logistic regression model that works in kernel space instead of feature space. This is different from regular logistic regression because instead of learning a weight vector \\(w\\), we learn a vector of dual weights \\(a\\) and use a kernel function to compute scores.\nIn my KernelLogisticRegression class I had the following function:\nScore function: * To predict outputs for new data, the model computes the score [ s = K(X, X_{})^a ] where ( K ) is the kernel matrix. This is for the model to make nonlinear decisions too.\nLoss function: * The loss has two parts: - The logistic loss. - An (_1) regularization on the dual weights (a) taht helps sparsity by making many entries of (a) equal to zero.\nGradient function: * I calculated the gradient of the total loss, including the gradient of the (_1) * This helps us to calculate the gradient descent.\nFit function: * Function that trained the model by calculating the kernel matrix, initializing ( a = 0 ), and updating ( a ) with gradient descent for many epochs.\nNow lets set up our kernel for our experiments.\n\ndef rbf_kernel(X1, X2, gamma):\n    return torch.exp(-gamma * torch.cdist(X1, X2)**2)"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#testing-our-implementaion-of-our-kernellized-logistic-regression",
    "href": "posts/Kernel_Machines/index.html#testing-our-implementaion-of-our-kernellized-logistic-regression",
    "title": "Sparse Kernel Machines",
    "section": "Testing our implementaion of our kernellized logistic regression",
    "text": "Testing our implementaion of our kernellized logistic regression\nSo in our code below we will generate a simple dataset that is linearly separable adn will be testing our implementation.\n\nimport torch \nfrom matplotlib import pyplot as plt\nplt.style.use('seaborn-v0_8-whitegrid')\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    # X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    X = X - X.mean(dim = 0, keepdim = True)\n    return X, y\n\n\ndef plot_classification_data(X, y, ax):\n    assert X.shape[1] == 2, \"This function only works for data created with p_dims == 2\"\n    targets = [0, 1]\n    markers = [\"o\" , \",\"]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], facecolors = \"none\", edgecolors = \"darkgrey\", cmap = \"BrBG\", vmin = -1, vmax = 2, alpha = 0.8, marker = markers[i])\n    ax.set(xlabel = r\"$x_1$\", ylabel = r\"$x_2$\")\n\nfig, ax = plt.subplots(1, 1)\nX, y = classification_data(n_points = 100, noise = 0.4)\nplot_classification_data(X, y, ax)\n\n\n\n\n\n\n\n\nNow that we have set up our dataset and also set up our Kernalized logistic regression, lets finally test it.\n\nfrom sparse_kernel_logistic import KernelLogisticRegression\nKR = KernelLogisticRegression(rbf_kernel, lam = 0.1, gamma = 1)\nKR.fit(X, y, m_epochs = 500000, lr = 0.0001)\n\nLets now look at our vector Kr.a\n\n(1.0*(KR.a &gt; 0.001)).mean()\n\ntensor(0.1000)\n\n\nNow in our code below we will plot ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍scores ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍along ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍with the training data, ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍and ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍we will also ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍highlight ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍pieces ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍of ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍the ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍training ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍data ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍which ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍have ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍weights that are distinguishable ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍from ‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍‍0\n\nix = torch.abs(KR.a) &gt; 0.001\n\nx1 = torch.linspace(X[:,0].min() - 0.2, X[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X[:,1].min() - 0.2, X[:,1].max() + 0.2, 101)\n\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nx1 = X1.ravel()\nx2 = X2.ravel()\n\nX_ = torch.stack((x1, x2), dim = 1)\n\npreds = KR.score(X_, recompute_kernel = True)\npreds = 1.0*torch.reshape(preds, X1.size())\n\nfig, ax = plt.subplots(1, 1)\nax.contourf(X1, X2, preds, origin = \"lower\", cmap = \"BrBG\", \nvmin = 2*preds.min() - preds.max(), vmax = 2*preds.max() - preds.min()\n)\nplot_classification_data(X, y, ax)\nplt.scatter(X[ix, 0],X[ix, 1], facecolors = \"none\", edgecolors = \"black\")\n\n\n\n\n\n\n\n\nWe can see in the plot above that our model separates the two classes with a smooth, curved boundary. Only a some of the training points(the ones with black outline) have large influence on the decision and most other points have little or no effect. The RBF kernel helps create a nonlinear boundary that fits pir data well."
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#part-a",
    "href": "posts/Kernel_Machines/index.html#part-a",
    "title": "Sparse Kernel Machines",
    "section": "Part A",
    "text": "Part A\n\nExperiment 1: Effect of Large λ (Sparsity)\nIn this experiment, I will show how increasing the regularization strength λ causes many entries of the weight vector \\(a\\) to become exactly zero. This is a feature of sparse models that they only rely on a few “important” training points.\nI trained my model with a very large value of λ (i.e. λ = 10). After training, I counted how many entries of \\(a\\) are distinguishable from 0 by checking if they are greater than 0.001 in absolute value.\nThis expeiremnt showed that the model became extremely sparse. None of the training points had weights greater than 0.001. This means that with such a strong regularization (λ = 10), the model ignored all the training points and didn’t form a usable boundary. This shows what happens when regularization is very strong and the model becomes so sparse that it underfits the data which means it is not a good model.\n\nKR = KernelLogisticRegression(rbf_kernel, lam=10.0, gamma=1)\nKR.fit(X, y, m_epochs=500000, lr=0.0001)\n(1.0 * (KR.a.abs() &gt; 0.001)).mean()\n\ntensor(0.)\n\n\n\n\nExperiment 2: Effect of γ on the Decision Boundary\nFor the second experiment I tried experimented with different values of the kernel parameter γ. This parameter controls the “wiggliness” of the decision boundary. A small γ leads to smoother decision boundaries, while large γ results in very wiggly boundaries.\nI trained two models with the same λ, but different γ values (γ = 0.1 vs. γ = 10), and plotted their decision boundaries and saw that the model with higher γ produced a much more complex decision boundary. In our plots below we can see both models and how changing γ leads to different boundary behaviors.\n\n# using a Low gamma = smooth\nKR_smooth = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=0.1)\nKR_smooth.fit(X, y, m_epochs=500000, lr=0.0001)\n\n# experimenting witha High gamma for wiggly\nKR_wiggly = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=10)\nKR_wiggly.fit(X, y, m_epochs=500000, lr=0.0001)\n\n\n# grid for teh plot for plotting\nx1 = torch.linspace(X[:, 0].min() - 0.2, X[:, 0].max() + 0.2, 101)\nx2 = torch.linspace(X[:, 1].min() - 0.2, X[:, 1].max() + 0.2, 101)\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\ngrid = torch.stack([X1.ravel(), X2.ravel()], dim=1)\n\n# Plot for KR_smooth\npreds_smooth = KR_smooth.score(grid, recompute_kernel=True)\npreds_smooth = preds_smooth.reshape(X1.shape)\n\nfig, ax = plt.subplots()\nax.contourf(X1, X2, preds_smooth, cmap=\"BrBG\", levels=20)\nplot_classification_data(X, y, ax)\nax.set_title(\"Decision Boundary with gamma = 0.1\")\nplt.show()\n\n# plot for KR_wiggly\npreds_wiggly = KR_wiggly.score(grid, recompute_kernel=True)\npreds_wiggly = preds_wiggly.reshape(X1.shape)\n\nfig, ax = plt.subplots()\nax.contourf(X1, X2, preds_wiggly, cmap=\"BrBG\", levels=20)\nplot_classification_data(X, y, ax)\nax.set_title(\"Decision Boundary with gamma = 10\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment 3: Learning Nonlinear Patterns with make_moons\nIn this experiment, I tested how well the sparse kernel logistic regression model can handle nonlinear data. I used the make_moons dataset, which contains two interlocking half circles which is something taht a linear model cant separate.\nI trained my kernelized model with an RBF kernel (λ = 0.1, γ = 5). The plot of our decision boundary was smooth and curved, perfectly separating the two classes.\nThis shows that by applying a kernel function, we can make the data in a higher-dimensional space to be linearly separable and the logistic regression model can still be applied there.\n\nfrom sklearn.datasets import make_moons\n\n# Make training data\nX_moons_np, y_moons_np = make_moons(n_samples=300, noise=0.2)\nX_moons = torch.tensor(X_moons_np, dtype=torch.float32)\ny_moons = torch.tensor(y_moons_np, dtype=torch.float32)\n\n# Training our model\nKR_moons = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=5)\nKR_moons.fit(X_moons, y_moons, m_epochs=500000, lr=0.0001)\n\nBy Plotting the decision boundary for our experiemnt we can see taht our model was able to separate the two half-moon shapes using a smooth, curved decision boundary.\nThis shows the that RBF kernel can help the model to create flexible, nonlinear boundaries that fit the true shape of the data very well helpong teh model have a good performance.\n\nx1 = torch.linspace(X_moons[:,0].min() - 0.2, X_moons[:,0].max() + 0.2, 101)\nx2 = torch.linspace(X_moons[:,1].min() - 0.2, X_moons[:,1].max() + 0.2, 101)\nX1, X2 = torch.meshgrid(x1, x2, indexing='ij')\n\nX_grid = torch.stack((X1.ravel(), X2.ravel()), dim=1)\npreds = KR_moons.score(X_grid, recompute_kernel=True)\npreds = torch.sigmoid(preds).reshape(X1.shape)\n\nfig, ax = plt.subplots()\nax.contourf(X1, X2, preds, cmap=\"BrBG\", alpha=0.7)\nplot_classification_data(X_moons, y_moons, ax)\nplt.title(\"Decision Boundary on make_moons Dataset\")\n\nText(0.5, 1.0, 'Decision Boundary on make_moons Dataset')"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#part-b",
    "href": "posts/Kernel_Machines/index.html#part-b",
    "title": "Sparse Kernel Machines",
    "section": "Part B",
    "text": "Part B\n\nDemonstrating Overfitting\nIn our code below we generated two datasets one for training and the other for testing. We then trained our model with a bad Gamma so that we can cause overfitting. By setting our gamma high to somewhere around 5-10 we force the model to overfit.\n\n# Training data\nX_train_np, y_train_np = make_moons(n_samples=300, noise=0.4)\nX_train = torch.tensor(X_train_np, dtype=torch.float32)\ny_train = torch.tensor(y_train_np, dtype=torch.float32)\n\n# Testing data\nX_test_np, y_test_np = make_moons(n_samples=300, noise=0.4)\nX_test = torch.tensor(X_test_np, dtype=torch.float32)\ny_test = torch.tensor(y_test_np, dtype=torch.float32)\n\n#Training our model\nKR_overfit = KernelLogisticRegression(rbf_kernel, lam=0.1, gamma=5)\nKR_overfit.fit(X_train, y_train, m_epochs=500000, lr=0.0001)\n\n\nNow that we have trained our model, we will now evaluate our model and get predictions on teh train and test set.\n\n\nPlotting our ROC curve\nIn our plot The training ROC curve is very close to the top-left (meaning high performance), while the test ROC curve is much lower, indicating poor generalization.\nThis large gap between the training and testing performance means our model is overfitting. The model fits the noise in the training data instead of learning a decision boundary that generalizes the new examples/ test data.\n\nfrom sklearn.metrics import roc_curve, auc\n\n\n# predicting on the training data and the test data\ntrain_scores = torch.sigmoid(KR_overfit.score(X_train)).detach().numpy()\ntest_scores = torch.sigmoid(KR_overfit.score(X_test)).detach().numpy()\n\n\n# calculate the ROC curves\nfpr_train, tpr_train, _ = roc_curve(y_train, train_scores)\nfpr_test, tpr_test, _ = roc_curve(y_test, test_scores)\n\n# Plot ROC curves\nplt.figure()\nplt.plot(fpr_train, tpr_train, label=f\"Train ROC (AUC = {auc(fpr_train, tpr_train):.2f})\")\nplt.plot(fpr_test, tpr_test, label=f\"Test ROC (AUC = {auc(fpr_test, tpr_test):.2f})\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"ROC Curve: Overfitting Example (High Gamma)\")\nplt.legend()\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/Kernel_Machines/index.html#conclusion",
    "href": "posts/Kernel_Machines/index.html#conclusion",
    "title": "Sparse Kernel Machines",
    "section": "Conclusion",
    "text": "Conclusion\nIn this project, I built a Sparse Kernal Machine which is version of logistic regression that encourages sparsity. I tested how different settings of λ and γ affect the model’s behavior.\nI learned that using a large λ makes the model sparse by reducing the number of important training points. I also learned that adjusting γ changes how flexible the decision boundary is: small γ values make smoother boundaries, while large γ values create more complex ones.\nLastly, I saw how using a high γ can cause the model to overfit, doing well on training data but poorly on new testing data. Overall, this project helped me better understand how regularization and kernel methods work together to affect model complexity and generalization."
  },
  {
    "objectID": "posts/implementing logistic regression/index.html#plotting-decision-boundary-for-vanilla-dradient-descent-when-paradim-2",
    "href": "posts/implementing logistic regression/index.html#plotting-decision-boundary-for-vanilla-dradient-descent-when-paradim-2",
    "title": "Implementing Logistic Regression",
    "section": "Plotting Decision Boundary for Vanilla dradient descent when Paradim = 2",
    "text": "Plotting Decision Boundary for Vanilla dradient descent when Paradim = 2\nThis plot shows the decision boundary learned by the model when using vanilla gradient descent with 2 input features. The red and blue regions represent the two predicted classes, and the boundary clearly separates most of the red and blue data points, which shows that the model has learned a good linear separator. However there are a few points that have been in the wrong class.\n\nimport numpy as np\n\ndef plot_decision_boundary(model, X, y):\n    x1_min, x1_max = X[:,0].min()-0.5, X[:,0].max()+0.5\n    x2_min, x2_max = X[:,1].min()-0.5, X[:,1].max()+0.5\n    xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max, 100),\n                           np.linspace(x2_min, x2_max, 100))\n\n    # Add bias column to grid\n    grid = np.c_[xx1.ravel(), xx2.ravel()]\n    grid = np.concatenate([grid, np.ones((grid.shape[0], 1))], axis=1)\n    grid_tensor = torch.tensor(grid, dtype=torch.float32)\n\n    preds = (torch.sigmoid(model.score(grid_tensor)) &gt;= 0.5).float()\n    preds = preds.reshape(xx1.shape)\n\n    plt.contourf(xx1, xx2, preds, alpha=0.3, cmap='bwr')\n    plt.scatter(X[:,0], X[:,1], c=y, cmap='bwr', edgecolor='k')\n    plt.xlabel(\"x1\")\n    plt.ylabel(\"x2\")\n    plt.title(\"Decision Boundary\")\n    plt.grid(True)\n    plt.show()\n\n# Plot decision boundary for vanilla gradient descent\nplot_decision_boundary(LR_vanilla, X.numpy(), y.numpy())"
  }
]
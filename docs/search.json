[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this bloghshjs"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Classifying Palmer Penguins",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Predicting a penguin’s species based on its physical measurements is an interesting challenge in data analysis and machine learning. In this blog, we’ll explore the Palmer Penguins dataset, which was collected by Dr. Kristen Gorman and the Palmer Station, to analyze biological measurements of three penguin species in Antarctica. Through data visualization and feature selection, we’ll uncover key insights and build a classification model that achieves perfect accuracy using a carefully chosen set of features. Along the way, we’ll discuss our findings and ensure a reproducible and insightful approach to species classification. ### Loading Data We will now use the Pandas function to read the CSV and also take a look at the data and the different variables and columns we have in our dataset before we do and data explorations and analysis.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Perceptron\n\n\n\n\n\nA blog post about Implementing Perceptron and some experiments with it.\n\n\n\n\n\nApr 2, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nDissecting racial bias in an algorithm used to manage the health of populations\n\n\n\n\n\nA replication study of Obermeyer et al. (2019), exploring racial bias in healthcare cost algorithms and its impact on resource allocation.\n\n\n\n\n\nMar 2, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nDesign and Impact of Automated Decision Systems\n\n\n\n\n\nA blog post about Design and Impact of Automated Decision Systems\n\n\n\n\n\nFeb 27, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nLimits of the Quantitative Approach to Bias and Fairness\n\n\n\n\n\nA blog post on Limits of the Quantitative Approach to Bias and Fairness.\n\n\n\n\n\nFeb 20, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nA blog post about classifying penguins with machine learning using logistic Regression.\n\n\n\n\n\nFeb 20, 2025\n\n\nPB\n\n\n\n\n\n\n\n\n\n\n\n\nAuditing Bias in Machine Learning Models\n\n\n\n\n\nA blog post about auditing bias in machine learning models.\n\n\n\n\n\nFeb 20, 2025\n\n\nPB\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html#data-cleaning",
    "href": "posts/example-blog-post/index.html#data-cleaning",
    "title": "Classifying Palmer Penguins",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nOur dataset contains numerous variables, including comments and region information, that are not relevant for training our model. Since we will be focusing on variables such as island, culmen length, culmen depth, flipper length, and body mass, we will remove unnecessary columns before proceeding with data exploration, visualization, and model training. Let’s start by loading the necessary packages for data cleaning, visualization, and model training. We will also convert categorical feature columns like Sex and Island into one-hot encoded 0-1( or true or false) columns using the pd.get_dummies function. Additionally, we can see that Species is a categorical variable, but instead of one-hot encoding, we will use label encoding to convert it into numerical labels: 0 for Adelie, 1 for Chinstrap, and 2 for Gentoo. This transformation will make data visualization and model training much easier.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode categorical features and label\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Stage\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    # One-hot encode categorical variables\n    df =(pd.get_dummies(df)).astype(int)\n\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40\n16\n187\n3200\n9\n-24\n0\n1\n0\n0\n1\n1\n0\n\n\n1\n49\n19\n210\n3950\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n2\n50\n15\n218\n5700\n8\n-25\n1\n0\n0\n0\n1\n0\n1\n\n\n3\n45\n14\n210\n4200\n7\n-25\n1\n0\n0\n0\n1\n1\n0\n\n\n4\n51\n18\n203\n4100\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n\n\n\n\n\n\nVisualizing the data\nThe bar chart below shows the number of penguins of each species on different islands, helping us identify patterns in species distribution. Interestingly, we observe: - Torgersen Island: Only Adelie penguins are present, with a population of about 30-40 individuals. - Dream Island: Has both Adelie and Chinstrap penguins, with Chinstrap being the dominant species. - Biscoe Island: Has both Adelie and Gentoo penguins, with Gentoo having the largest population overall.\nChinstrap penguins are only found on Dream Island, while Gentoo penguins are exclusive to Biscoe Island. Adelie penguins, on the other hand, are the most widespread, appearing on all three islands. Based on this distribution, the island where a penguin is found could be a useful feature for predicting its species.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n# Map species labels back to names\nspecies_map = {0: 'Adelie', 1: 'Chinstrap', 2: 'Gentoo'}\nspecies_island_counts = X_train.copy()\nspecies_island_counts[\"Species\"] = [species_map[label] for label in y_train]\n\n# Convert one-hot encoded islands back to a single column\nspecies_island_counts[\"Island\"] = species_island_counts[[\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]].idxmax(axis=1)\nspecies_island_counts[\"Island\"] = species_island_counts[\"Island\"].str.replace(\"Island_\", \"\")\n\n# Count number of penguins per species per island\nplot_data = species_island_counts.groupby([\"Island\", \"Species\"]).size().reset_index(name=\"Count\")\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.barplot(data=plot_data, x=\"Island\", y=\"Count\", hue=\"Species\", palette=\"mako\")\nplt.xlabel(\"Island\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Penguin Species Distribution Across Islands\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, we can see that by simply looking at the islands, we can somewhat predict the species of the penguin. This is a good example of a feature that could help predict the species. However, the model would be much more efficient if each island only hosted one species of penguin. Since some islands have more than one species, we might want to explore additional features. For example, if a penguin is on Biscoe Island and has a certain Culmen length and Culmen depth, can we make more accurate predictions based on those measurements? To explore this, we will create a graph to assess whether including Culmen lenght and Culmen depth is a useful feature for predicting the species on each island.\n\n\n# Plotting the relationship between Culmen Length and Culmen Depth for each species on each island\nsns.relplot(data =train, hue=\"Species\", y = 'Culmen Depth (mm)', x =  'Culmen Length (mm)',col = 'Island')\n\n&lt;seaborn.axisgrid.FacetGrid at 0x149d2fc10&gt;\n\n\n\n\n\n\n\n\n\nThe graph above looks at the relationship between Culmen Depth (mm) and Culmen Length (mm) for different penguin species, separated by island. Each species is color-coded to highlight patterns in their beak dimensions. From this graph, we can observe: - Gentoo penguins (orange) tend to have longer culmen lengths and shallower culmen depths. - Adelie penguins (green) have moderate culmen lengths and a wide range of culmen depths. - Chinstrap penguins (blue) have longer culmen lenghts compared to Adelie penguins but exhibit similar culmen Depths. ## Can Culmen Features Help in Classification? Looking at the separability of species by island: - Biscoe Island: Gentoo and Adelie penguins are linearly separable, meaning a simple model could classify them effectively based on culmen features. - Dream Island: Chinstrap and Adelie penguins overlap in culmen depth, making them harder to separate linearly. However, their culmen lengths show some distinction, which could aid classification. - Torgersen Island: Since only Adelie penguins are found here, culmen features are irrelevant for classification on this island.\nThis suggests that culmen depth and length can be strong features for classifying species, especially when combined with island location. ## Exploring Culmen Features in relation to Flipper Lenght\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Length')\nsns.relplot(data = train, y = 'Culmen Depth (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Depth')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first graph explores the relationship between flipper length and culmen length across penguin species. Gentoo penguins have both longer flipper lengths and culmen lengths compared to the other species, making them easily distinguishable. Adelie penguins, in contrast, have shorter values for both features, clustering in the lower-left section of the graph. Chinstrap penguins overlap with Adelie penguins in flipper length but tend to have slightly longer culmen lengths, creating some classification challenges.\nThe second graph examines the relationship between flipper length and culmen depth. Here, Gentoo penguins are clearly separable from the other two species due to their significantly shallower culmen depths, forming a distinct cluster in the lower section. Howeber the Adelie and Chinstrap penguins overlap a lot more, particularly in flipper length and culmen depth. While Gentoo penguins can be classified easily, distinguishing between Adelie and Chinstrap penguins would be really difficult because of this clustering.\n\ntrain.groupby('Species').agg({\n    'Flipper Length (mm)': 'mean',\n    'Culmen Length (mm)': 'mean',\n    'Culmen Depth (mm)': 'mean'\n}).reset_index()\n\n\n\n\n\n\n\n\nSpecies\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n\n\n2\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433\n\n\n\n\n\n\n\nThis code above calculates the mean values of flipper length, culmen length, and culmen depth for each penguin species using the .groupby() function. The table summarizes the average flipper length, culmen length, and culmen depth for each penguin species. Gentoo penguins have the longest flipper and culmen lengths but the shallowest culmen depth. Chinstrap penguins have longer culmen lengths than Adelie penguins but similar culmen depths. Adelie penguins have the shortest flipper and culmen lengths but slightly deeper culmens than Chinstrap. These differences highlight key features that can help classify penguin species."
  },
  {
    "objectID": "posts/example-blog-post/index.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "href": "posts/example-blog-post/index.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "title": "Classifying Palmer Penguins",
    "section": "Now we are going to chose feature to train our model",
    "text": "Now we are going to chose feature to train our model\nBy using the combinations function from the itertools package, we generate different combinations of categorical and continuous features. So we will pair categorical variables like Sex and Clutch Completion with continuous variables such as Culmen Length, Culmen Depth, and Flipper Length. This allows us to explore various feature combinations and evaluate which ones might be most useful for accurately classifying the penguin species.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = None\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]  # Ensure categorical features are one-hot encoded\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        \n        # Train a logistic regression model to evaluate feature combinations\n        model = LogisticRegression(max_iter=1000)  # Increased iterations for better convergence\n        scores = cross_val_score(model, X_train[cols], y_train, cv=5, scoring='accuracy')\n        \n        avg_score = np.mean(scores)\n        if avg_score &gt; best_score:\n            best_score = avg_score\n            best_features = cols\n\nprint(\"Best features:\", best_features)\n\nBest features: ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nAccording to itertools, the best features to use are ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’. However, we wanted to explore how our model would perform based on our exploration of the island features along with the Culmen features. Thus, we will now reassign these as the best features and use them to train our model.\n\n#Rearranging Best featrues so that the first two columns are the quantitative columns and the rest are the qualitative columns\nbest_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nbest_features\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']"
  },
  {
    "objectID": "posts/example-blog-post/index.html#let-us-now-prepare-our-test-dataset",
    "href": "posts/example-blog-post/index.html#let-us-now-prepare-our-test-dataset",
    "title": "Classifying Palmer Penguins",
    "section": "Let us now prepare our Test Dataset",
    "text": "Let us now prepare our Test Dataset\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Prepare test data\nX_test, y_test = prepare_data(test)"
  },
  {
    "objectID": "posts/example-blog-post/index.html#training-and-evaluating-our-model",
    "href": "posts/example-blog-post/index.html#training-and-evaluating-our-model",
    "title": "Classifying Palmer Penguins",
    "section": "Training and evaluating our Model",
    "text": "Training and evaluating our Model\nIn the code below, we use a Logistic Regression model and train it on the selected features: [‘Culmen Length (mm)’, ‘Culmen Depth (mm)’, ‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’]. We then evaluate the accuracy using the LR.score function. Our training accuracy was 99%, and our testing accuracy—measuring how well the model performed on unseen data—was 100%. The confusion matrix confirms that we correctly classified all penguins with no misclassifications. Specifically, our model correctly identified 31 Adelie, 11 Chinstrap, and 26 Gentoo penguins, achieving perfect classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(X_train[best_features], y_train)\n\n\n\n# Predict on test set\ny_pred = LR.predict(X_test[best_features])\n\n# Evaluate the model\nscore_train = LR.score(X_train[best_features], y_train)\nscore_test = LR.score(X_test[best_features], y_test)\n\nprint(\"Training Accuracy:\", score_train)\nprint(\"Test Accuracy:\", score_test)\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\nTraining Accuracy: 0.9921875\nTest Accuracy: 1.0\n\nConfusion Matrix:\n [[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]"
  },
  {
    "objectID": "posts/example-blog-post/index.html#plotting-our-results",
    "href": "posts/example-blog-post/index.html#plotting-our-results",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting our results",
    "text": "Plotting our results\nThis code below helps us visualize the decision regions of our classification model by plotting how different penguin species are separated based on culmen features and island locations. It creates a grid of values for the culmen length and depth, predicts species across the grid. Each subplot represents a different island, showing how the model’s decision boundaries change depending on the island feature. This helps us understand how the model differentiates between Adelie, Chinstrap, and Gentoo penguins on each island.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7 * len(qual_features), 3))  # doing this to fit three plots side by side\n    fig.suptitle(title, fontsize=14)\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    if len(qual_features) == 1:\n        axarr = [axarr] \n\n    for i, ax in enumerate(axarr):\n        XY = pd.DataFrame({\n            X.columns[0]: XX,\n            X.columns[1]: YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # Use contour plot to visualize the predictions\n        ax.contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n        ax.scatter(x0[ix], x1[ix], c=y[ix], cmap=\"jet\", vmin=0, vmax=2)\n        \n        ax.set(xlabel=X.columns[0], ylabel=X.columns[1], title=qual_features[i])\n\n    patches = [Patch(color=color, label=spec) for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"])]\n    fig.legend(handles=patches, title=\"Species\", loc=\"upper right\")\n\n    plt.tight_layout(rect=[0, 0, 0.9, 1])\n    plt.show()\n\n# Run function for training and test sets\nplot_regions(LR, X_train[best_features], y_train, title=\"Training Set\")\nplot_regions(LR, X_test[best_features], y_pred, title=\"Test Set\")"
  },
  {
    "objectID": "posts/example-blog-post/blog1.html",
    "href": "posts/example-blog-post/blog1.html",
    "title": "Goal of Project",
    "section": "",
    "text": "---\ntitle: Hello Blog\nauthor: PB\ndate: '2025-02-20'\nimage: \"image.jpg\"\ndescription: \"A Blog post about classifying penguins with machine learning.\"\nformat: html\n---\nPredicting a penguin’s species based on its physical measurements is an interesting challenge in data analysis and machine learning. In this blog, we’ll explore the Palmer Penguins dataset, which was collected by Dr. Kristen Gorman and the Palmer Station, to analyze biological measurements of three penguin species in Antarctica. Through data visualization and feature selection, we’ll uncover key insights and build a classification model that achieves perfect accuracy using a carefully chosen set of features. Along the way, we’ll discuss our findings and ensure a reproducible and insightful approach to species classification. ### Loading Data We will now use the Pandas function to read the CSV and also take a look at the data and the different variables and columns we have in our dataset before we do and data explorations and analysis.\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/example-blog-post/blog1.html#data-cleaning",
    "href": "posts/example-blog-post/blog1.html#data-cleaning",
    "title": "Goal of Project",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nOur dataset contains numerous variables, including comments and region information, that are not relevant for training our model. Since we will be focusing on variables such as island, culmen length, culmen depth, flipper length, and body mass, we will remove unnecessary columns before proceeding with data exploration, visualization, and model training. Let’s start by loading the necessary packages for data cleaning, visualization, and model training. We will also convert categorical feature columns like Sex and Island into one-hot encoded 0-1( or true or false) columns using the pd.get_dummies function. Additionally, we can see that Species is a categorical variable, but instead of one-hot encoding, we will use label encoding to convert it into numerical labels: 0 for Adelie, 1 for Chinstrap, and 2 for Gentoo. This transformation will make data visualization and model training much easier.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode categorical features and label\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Stage\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    # One-hot encode categorical variables\n    df =(pd.get_dummies(df)).astype(int)\n\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40\n16\n187\n3200\n9\n-24\n0\n1\n0\n0\n1\n1\n0\n\n\n1\n49\n19\n210\n3950\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n2\n50\n15\n218\n5700\n8\n-25\n1\n0\n0\n0\n1\n0\n1\n\n\n3\n45\n14\n210\n4200\n7\n-25\n1\n0\n0\n0\n1\n1\n0\n\n\n4\n51\n18\n203\n4100\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n\n\n\n\n\n\nVisualizing the data\nThe bar chart below shows the number of penguins of each species on different islands, helping us identify patterns in species distribution. Interestingly, we observe: - Torgersen Island: Only Adelie penguins are present, with a population of about 30-40 individuals. - Dream Island: Has both Adelie and Chinstrap penguins, with Chinstrap being the dominant species. - Biscoe Island: Has both Adelie and Gentoo penguins, with Gentoo having the largest population overall.\nChinstrap penguins are only found on Dream Island, while Gentoo penguins are exclusive to Biscoe Island. Adelie penguins, on the other hand, are the most widespread, appearing on all three islands. Based on this distribution, the island where a penguin is found could be a useful feature for predicting its species.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n# Map species labels back to names\nspecies_map = {0: 'Adelie', 1: 'Chinstrap', 2: 'Gentoo'}\nspecies_island_counts = X_train.copy()\nspecies_island_counts[\"Species\"] = [species_map[label] for label in y_train]\n\n# Convert one-hot encoded islands back to a single column\nspecies_island_counts[\"Island\"] = species_island_counts[[\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]].idxmax(axis=1)\nspecies_island_counts[\"Island\"] = species_island_counts[\"Island\"].str.replace(\"Island_\", \"\")\n\n# Count number of penguins per species per island\nplot_data = species_island_counts.groupby([\"Island\", \"Species\"]).size().reset_index(name=\"Count\")\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.barplot(data=plot_data, x=\"Island\", y=\"Count\", hue=\"Species\", palette=\"mako\")\nplt.xlabel(\"Island\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Penguin Species Distribution Across Islands\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, we can see that by simply looking at the islands, we can somewhat predict the species of the penguin. This is a good example of a feature that could help predict the species. However, the model would be much more efficient if each island only hosted one species of penguin. Since some islands have more than one species, we might want to explore additional features. For example, if a penguin is on Biscoe Island and has a certain Culmen length and Culmen depth, can we make more accurate predictions based on those measurements? To explore this, we will create a graph to assess whether including Culmen lenght and Culmen depth is a useful feature for predicting the species on each island.\n\n\n# Plotting the relationship between Culmen Length and Culmen Depth for each species on each island\nsns.relplot(data =train, hue=\"Species\", y = 'Culmen Depth (mm)', x =  'Culmen Length (mm)',col = 'Island')\n\n\n\n\n\n\n\n\nThe graph above looks at the relationship between Culmen Depth (mm) and Culmen Length (mm) for different penguin species, separated by island. Each species is color-coded to highlight patterns in their beak dimensions. From this graph, we can observe: - Gentoo penguins (orange) tend to have longer culmen lengths and shallower culmen depths. - Adelie penguins (green) have moderate culmen lengths and a wide range of culmen depths. - Chinstrap penguins (blue) have longer culmen lenghts compared to Adelie penguins but exhibit similar culmen Depths. ## Can Culmen Features Help in Classification? Looking at the separability of species by island: - Biscoe Island: Gentoo and Adelie penguins are linearly separable, meaning a simple model could classify them effectively based on culmen features. - Dream Island: Chinstrap and Adelie penguins overlap in culmen depth, making them harder to separate linearly. However, their culmen lengths show some distinction, which could aid classification. - Torgersen Island: Since only Adelie penguins are found here, culmen features are irrelevant for classification on this island.\nThis suggests that culmen depth and length can be strong features for classifying species, especially when combined with island location. ## Exploring Culmen Features in relation to Flipper Lenght\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Length')\nsns.relplot(data = train, y = 'Culmen Depth (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Depth')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first graph explores the relationship between flipper length and culmen length across penguin species. Gentoo penguins have both longer flipper lengths and culmen lengths compared to the other species, making them easily distinguishable. Adelie penguins, in contrast, have shorter values for both features, clustering in the lower-left section of the graph. Chinstrap penguins overlap with Adelie penguins in flipper length but tend to have slightly longer culmen lengths, creating some classification challenges.\nThe second graph examines the relationship between flipper length and culmen depth. Here, Gentoo penguins are clearly separable from the other two species due to their significantly shallower culmen depths, forming a distinct cluster in the lower section. Howeber the Adelie and Chinstrap penguins overlap a lot more, particularly in flipper length and culmen depth. While Gentoo penguins can be classified easily, distinguishing between Adelie and Chinstrap penguins would be really difficult because of this clustering.\n\ntrain.groupby('Species').agg({\n    'Flipper Length (mm)': 'mean',\n    'Culmen Length (mm)': 'mean',\n    'Culmen Depth (mm)': 'mean'\n}).reset_index()\n\n\n\n\n\n\n\n\nSpecies\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n\n\n2\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433\n\n\n\n\n\n\n\nThis code above calculates the mean values of flipper length, culmen length, and culmen depth for each penguin species using the .groupby() function. The table summarizes the average flipper length, culmen length, and culmen depth for each penguin species. Gentoo penguins have the longest flipper and culmen lengths but the shallowest culmen depth. Chinstrap penguins have longer culmen lengths than Adelie penguins but similar culmen depths. Adelie penguins have the shortest flipper and culmen lengths but slightly deeper culmens than Chinstrap. These differences highlight key features that can help classify penguin species."
  },
  {
    "objectID": "posts/example-blog-post/blog1.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "href": "posts/example-blog-post/blog1.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "title": "Goal of Project",
    "section": "Now we are going to chose feature to train our model",
    "text": "Now we are going to chose feature to train our model\nBy using the combinations function from the itertools package, we generate different combinations of categorical and continuous features. So we will pair categorical variables like Sex and Clutch Completion with continuous variables such as Culmen Length, Culmen Depth, and Flipper Length. This allows us to explore various feature combinations and evaluate which ones might be most useful for accurately classifying the penguin species.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = None\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]  # Ensure categorical features are one-hot encoded\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        \n        # Train a logistic regression model to evaluate feature combinations\n        model = LogisticRegression(max_iter=1000)  # Increased iterations for better convergence\n        scores = cross_val_score(model, X_train[cols], y_train, cv=5, scoring='accuracy')\n        \n        avg_score = np.mean(scores)\n        if avg_score &gt; best_score:\n            best_score = avg_score\n            best_features = cols\n\nprint(\"Best features:\", best_features)\n\nBest features: ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nAccording to itertools, the best features to use are ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’. However, we wanted to explore how our model would perform based on our exploration of the island features along with the Culmen features. Thus, we will now reassign these as the best features and use them to train our model.\n\n#Rearranging Best featrues so that the first two columns are the quantitative columns and the rest are the qualitative columns\nbest_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nbest_features\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']"
  },
  {
    "objectID": "posts/example-blog-post/blog1.html#let-us-now-prepare-our-test-dataset",
    "href": "posts/example-blog-post/blog1.html#let-us-now-prepare-our-test-dataset",
    "title": "Goal of Project",
    "section": "Let us now prepare our Test Dataset",
    "text": "Let us now prepare our Test Dataset\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Prepare test data\nX_test, y_test = prepare_data(test)"
  },
  {
    "objectID": "posts/example-blog-post/blog1.html#training-and-evaluating-our-model",
    "href": "posts/example-blog-post/blog1.html#training-and-evaluating-our-model",
    "title": "Goal of Project",
    "section": "Training and evaluating our Model",
    "text": "Training and evaluating our Model\nIn the code below, we use a Logistic Regression model and train it on the selected features: [‘Culmen Length (mm)’, ‘Culmen Depth (mm)’, ‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’]. We then evaluate the accuracy using the LR.score function. Our training accuracy was 99%, and our testing accuracy—measuring how well the model performed on unseen data—was 100%. The confusion matrix confirms that we correctly classified all penguins with no misclassifications. Specifically, our model correctly identified 31 Adelie, 11 Chinstrap, and 26 Gentoo penguins, achieving perfect classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(X_train[best_features], y_train)\n\n\n\n# Predict on test set\ny_pred = LR.predict(X_test[best_features])\n\n# Evaluate the model\nscore_train = LR.score(X_train[best_features], y_train)\nscore_test = LR.score(X_test[best_features], y_test)\n\nprint(\"Training Accuracy:\", score_train)\nprint(\"Test Accuracy:\", score_test)\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\nTraining Accuracy: 0.9921875\nTest Accuracy: 1.0\n\nConfusion Matrix:\n [[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]"
  },
  {
    "objectID": "posts/example-blog-post/blog1.html#plotting-our-results",
    "href": "posts/example-blog-post/blog1.html#plotting-our-results",
    "title": "Goal of Project",
    "section": "Plotting our results",
    "text": "Plotting our results\nThis code below helps us visualize the decision regions of our classification model by plotting how different penguin species are separated based on culmen features and island locations. It creates a grid of values for the culmen length and depth, predicts species across the grid. Each subplot represents a different island, showing how the model’s decision boundaries change depending on the island feature. This helps us understand how the model differentiates between Adelie, Chinstrap, and Gentoo penguins on each island.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7 * len(qual_features), 3))  # doing this to fit three plots side by side\n    fig.suptitle(title, fontsize=14)\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    if len(qual_features) == 1:\n        axarr = [axarr] \n\n    for i, ax in enumerate(axarr):\n        XY = pd.DataFrame({\n            X.columns[0]: XX,\n            X.columns[1]: YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # Use contour plot to visualize the predictions\n        ax.contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n        ax.scatter(x0[ix], x1[ix], c=y[ix], cmap=\"jet\", vmin=0, vmax=2)\n        \n        ax.set(xlabel=X.columns[0], ylabel=X.columns[1], title=qual_features[i])\n\n    patches = [Patch(color=color, label=spec) for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"])]\n    fig.legend(handles=patches, title=\"Species\", loc=\"upper right\")\n\n    plt.tight_layout(rect=[0, 0, 0.9, 1])\n    plt.show()\n\n# Run function for training and test sets\nplot_regions(LR, X_train[best_features], y_train, title=\"Training Set\")\nplot_regions(LR, X_test[best_features], y_pred, title=\"Test Set\")"
  },
  {
    "objectID": "posts/Penguins/index.html",
    "href": "posts/Penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset, collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, provides physiological measurements for three species of penguins—Adélie, Chinstrap, and Gentoo—inhabiting the Palmer Archipelago. First published in 2014 and later made widely accessible to the data science community, this dataset serves as a valuable resource for exploring classification techniques and species differentiation. In this blog post, I analyze the dataset using machine learning models to classify penguin species based on features such as flipper length, body mass, and culmen dimensions. By analyzing physical measurements we identify key features that contribute to accurate species prediction. Through data visualization and feature selection, we uncover patterns that distinguish species based on island location and physical traits. Using logistic regression, we evaluate different feature combinations and determine an optimal set for classification and acheive a 99% training accuracy and a 100% testing accuracy. Our findings highlight the power of data-driven approaches in biological classification while ensuring a reproducible and insightful methodology."
  },
  {
    "objectID": "posts/Penguins/index.html#data-cleaning",
    "href": "posts/Penguins/index.html#data-cleaning",
    "title": "Classifying Palmer Penguins",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nOur dataset contains numerous variables, including comments and region information, that are not relevant for training our model. Since we will be focusing on variables such as island, culmen length, culmen depth, flipper length, and body mass, we will remove unnecessary columns before proceeding with data exploration, visualization, and model training. Let’s start by loading the necessary packages for data cleaning, visualization, and model training. We will also convert categorical feature columns like Sex and Island into one-hot encoded 0-1( or true or false) columns using the pd.get_dummies function. Additionally, we can see that Species is a categorical variable, but instead of one-hot encoding, we will use label encoding to convert it into numerical labels: 0 for Adelie, 1 for Chinstrap, and 2 for Gentoo. This transformation will make data visualization and model training much easier.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC # support vector classifier\nfrom mlxtend.plotting import plot_decision_regions # for visualization later\nfrom sklearn.preprocessing import LabelEncoder\n\n# Encode categorical features and label\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\", \"Stage\"], axis=1)\n    df = df[df[\"Sex\"] != \".\"]\n    df = df.dropna()\n    y = le.transform(df[\"Species\"])\n    df = df.drop([\"Species\"], axis=1)\n    # One-hot encode categorical variables\n    df =(pd.get_dummies(df)).astype(int)\n\n    return df, y\n\n# Prepare training data\nX_train, y_train = prepare_data(train)\nX_train.head()\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40\n16\n187\n3200\n9\n-24\n0\n1\n0\n0\n1\n1\n0\n\n\n1\n49\n19\n210\n3950\n9\n-24\n0\n1\n0\n0\n1\n0\n1\n\n\n2\n50\n15\n218\n5700\n8\n-25\n1\n0\n0\n0\n1\n0\n1\n\n\n3\n45\n14\n210\n4200\n7\n-25\n1\n0\n0\n0\n1\n1\n0\n\n\n4\n51\n18\n203\n4100\n9\n-24\n0\n1\n0\n0\n1\n0\n1"
  },
  {
    "objectID": "posts/Penguins/index.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "href": "posts/Penguins/index.html#now-we-are-going-to-chose-feature-to-train-our-model",
    "title": "Classifying Palmer Penguins",
    "section": "Now we are going to chose feature to train our model",
    "text": "Now we are going to chose feature to train our model\nBy using the combinations function from the itertools package, we generate different combinations of categorical and continuous features. So we will pair categorical variables like Sex and Clutch Completion with continuous variables such as Culmen Length, Culmen Depth, and Flipper Length. This allows us to explore various feature combinations and evaluate which ones might be most useful for accurately classifying the penguin species.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = None\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]  # Ensure categorical features are one-hot encoded\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        \n        # Train a logistic regression model to evaluate feature combinations\n        model = LogisticRegression(max_iter=1000)  # Increased iterations for better convergence\n        scores = cross_val_score(model, X_train[cols], y_train, cv=5, scoring='accuracy')\n        \n        avg_score = np.mean(scores)\n        if avg_score &gt; best_score:\n            best_score = avg_score\n            best_features = cols\n\nprint(\"Best features:\", best_features)\n\nBest features: ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nAccording to itertools, the best features to use are ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’. However, we wanted to explore how our model would perform based on our exploration of the island features along with the Culmen features. Thus, we will now reassign these as the best features and use them to train our model.\n\n#Rearranging Best featrues so that the first two columns are the quantitative columns and the rest are the qualitative columns\nbest_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nbest_features\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']"
  },
  {
    "objectID": "posts/Penguins/index.html#let-us-now-prepare-our-test-dataset",
    "href": "posts/Penguins/index.html#let-us-now-prepare-our-test-dataset",
    "title": "Classifying Palmer Penguins",
    "section": "Let us now prepare our Test Dataset",
    "text": "Let us now prepare our Test Dataset\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Prepare test data\nX_test, y_test = prepare_data(test)"
  },
  {
    "objectID": "posts/Penguins/index.html#training-and-evaluating-our-model",
    "href": "posts/Penguins/index.html#training-and-evaluating-our-model",
    "title": "Classifying Palmer Penguins",
    "section": "Training and evaluating our Model",
    "text": "Training and evaluating our Model\nIn the code below, we use a Logistic Regression model and train it on the selected features: [‘Culmen Length (mm)’, ‘Culmen Depth (mm)’, ‘Island_Biscoe’, ‘Island_Dream’, ‘Island_Torgersen’]. We then evaluate the accuracy using the LR.score function. Our training accuracy was 99%, and our testing accuracy—measuring how well the model performed on unseen data—was 100%. The confusion matrix confirms that we correctly classified all penguins with no misclassifications. Specifically, our model correctly identified 31 Adelie, 11 Chinstrap, and 26 Gentoo penguins, achieving perfect classification.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nLR = LogisticRegression()\nLR.fit(X_train[best_features], y_train)\n\n\n\n# Predict on test set\ny_pred = LR.predict(X_test[best_features])\n\n# Evaluate the model\nscore_train = LR.score(X_train[best_features], y_train)\nscore_test = LR.score(X_test[best_features], y_test)\n\nprint(\"Training Accuracy:\", score_train)\nprint(\"Test Accuracy:\", score_test)\n\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\nTraining Accuracy: 0.9921875\nTest Accuracy: 1.0\n\nConfusion Matrix:\n [[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]"
  },
  {
    "objectID": "posts/Penguins/index.html#plotting-our-results",
    "href": "posts/Penguins/index.html#plotting-our-results",
    "title": "Classifying Palmer Penguins",
    "section": "Plotting our results",
    "text": "Plotting our results\nThis code below helps us visualize the decision regions of our classification model by plotting how different penguin species are separated based on culmen features and island locations. It creates a grid of values for the culmen length and depth, predicts species across the grid. Each subplot represents a different island, showing how the model’s decision boundaries change depending on the island feature. This helps us understand how the model differentiates between Adelie, Chinstrap, and Gentoo penguins on each island.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(7 * len(qual_features), 3))  # doing this to fit three plots side by side\n    fig.suptitle(title, fontsize=14)\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    if len(qual_features) == 1:\n        axarr = [axarr] \n\n    for i, ax in enumerate(axarr):\n        XY = pd.DataFrame({\n            X.columns[0]: XX,\n            X.columns[1]: YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # Use contour plot to visualize the predictions\n        ax.contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n        ax.scatter(x0[ix], x1[ix], c=y[ix], cmap=\"jet\", vmin=0, vmax=2)\n        \n        ax.set(xlabel=X.columns[0], ylabel=X.columns[1], title=qual_features[i])\n\n    patches = [Patch(color=color, label=spec) for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"])]\n    fig.legend(handles=patches, title=\"Species\", loc=\"upper right\")\n\n    plt.tight_layout(rect=[0, 0, 0.9, 1])\n    plt.show()\n\n# Run function for training and test sets\nplot_regions(LR, X_train[best_features], y_train, title=\"Training Set\")\nplot_regions(LR, X_test[best_features], y_pred, title=\"Test Set\")"
  },
  {
    "objectID": "posts/Penguins/index.html#can-culmen-features-help-in-classification",
    "href": "posts/Penguins/index.html#can-culmen-features-help-in-classification",
    "title": "Classifying Palmer Penguins",
    "section": "Can Culmen Features Help in Classification?",
    "text": "Can Culmen Features Help in Classification?\nLooking at the separability of species by island:\n\nBiscoe Island: Gentoo and Adelie penguins are linearly separable, meaning a simple model could classify them effectively based on culmen features.\nDream Island: Chinstrap and Adelie penguins overlap in culmen depth, making them harder to separate linearly. However, their culmen lengths show some distinction, which could aid classification.\nTorgersen Island: Since only Adelie penguins are found here, culmen features are irrelevant for classification on this island.\n\nThis suggests that culmen depth and length can be strong features for classifying species, especially when combined with island location."
  },
  {
    "objectID": "posts/Penguins/index.html#exploring-culmen-features-in-relation-to-flipper-lenght",
    "href": "posts/Penguins/index.html#exploring-culmen-features-in-relation-to-flipper-lenght",
    "title": "Classifying Palmer Penguins",
    "section": "Exploring Culmen Features in relation to Flipper Lenght",
    "text": "Exploring Culmen Features in relation to Flipper Lenght\nThe graph below explores the relationship between flipper length and culmen length across penguin species. Gentoo penguins have both longer flipper lengths and culmen lengths compared to the other species, making them easily distinguishable. Adelie penguins, in contrast, have shorter values for both features, clustering in the lower-left section of the graph. Chinstrap penguins overlap with Adelie penguins in flipper length but tend to have slightly longer culmen lengths, creating some classification challenges.\n\nsns.relplot(data = train, y = 'Culmen Length (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Length')\n\n\n\n\n\n\n\n\n\n\nsns.relplot(data = train, y = 'Culmen Depth (mm)', x =  'Flipper Length (mm)',  hue = 'Species').set(title='Relation between Flipper Length and Culmen Depth')\n\n\n\n\n\n\n\n\nThe graph above examines the relationship between flipper length and culmen depth. Here, Gentoo penguins are clearly separable from the other two species due to their significantly shallower culmen depths, forming a distinct cluster in the lower section. Howeber the Adelie and Chinstrap penguins overlap a lot more, particularly in flipper length and culmen depth. While Gentoo penguins can be classified easily, distinguishing between Adelie and Chinstrap penguins would be really difficult because of this clustering."
  },
  {
    "objectID": "posts/Penguins/index.html#decision-region-plots",
    "href": "posts/Penguins/index.html#decision-region-plots",
    "title": "Classifying Palmer Penguins",
    "section": "Decision Region Plots",
    "text": "Decision Region Plots\nThis code below helps us visualize the decision regions of our classification model by plotting how different penguin species are separated based on culmen features and island locations. It creates a grid of values for the culmen length and depth, predicts species across the grid. Each subplot represents a different island, showing how the model’s decision boundaries change depending on the island feature. This helps us understand how the model differentiates between Adelie, Chinstrap, and Gentoo penguins on each island.\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y, title):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    # Define a color mapping where Gentoo is green and Chinstrap is blue\n    species_colors = {0: 'red', 1: 'blue', 2: 'green'}\n    \n    # Adjust figure size for better visibility\n    fig, axarr = plt.subplots(1, len(qual_features), figsize=(12 * len(qual_features), 10))  # Increase size\n    fig.suptitle(title, fontsize=40, y=1.05)  # Adjust title position and size\n\n    # Create a grid\n    grid_x = np.linspace(x0.min(), x0.max(), 501)\n    grid_y = np.linspace(x1.min(), x1.max(), 501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    if len(qual_features) == 1:\n        axarr = [axarr] \n\n    for i, ax in enumerate(axarr):\n        XY = pd.DataFrame({\n            X.columns[0]: XX,\n            X.columns[1]: YY\n        })\n\n        for j in qual_features:\n            XY[j] = 0\n\n        XY[qual_features[i]] = 1\n\n        p = model.predict(XY)\n        p = p.reshape(xx.shape)\n        \n        # Use contour plot to visualize the predictions\n        ax.contourf(xx, yy, p, cmap=\"jet\", alpha=0.2, vmin=0, vmax=2)\n\n        ix = X[qual_features[i]] == 1\n        # Convert y[ix] to a pandas Series and apply map to get colors\n        y_series = pd.Series(y[ix])  # Convert y[ix] to a Series\n        ax.scatter(x0[ix], x1[ix], c=y_series.map(species_colors), s=80)  # Larger scatter points\n\n        ax.set_xlabel(X.columns[0], fontsize=30)\n        ax.set_ylabel(X.columns[1], fontsize=30)\n        ax.set_title(qual_features[i], fontsize=35)\n\n        ax.tick_params(axis='both', labelsize=25)  # Larger tick labels\n\n    # Increase spacing between subplots and adjust title positioning\n    plt.subplots_adjust(wspace=0.4, hspace=0.3, top=0.85)  \n\n    # Create legend with specific colors for species\n    patches = [Patch(color=color, label=spec) for spec, color in zip([\"Adelie\", \"Chinstrap\", \"Gentoo\"], [\"red\", \"blue\", \"green\"])]\n    fig.legend(handles=patches, title=\"Species\", loc=\"upper right\", fontsize=30, title_fontsize=35, bbox_to_anchor=(1.03, 1)) \n\n    plt.show()\n\n# Run function for training and test sets\nplot_regions(LR, X_train[best_features], y_train, title=\"Training Set\")\nplot_regions(LR, X_test[best_features], y_pred, title=\"Test Set\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe decision region plots clearly show how our model classifies penguins based on culmen length and depth, with island location influencing the boundaries.\n\nIsland Biscoe: The model effectively separates Gentoo (green) from Adelie (red) penguins, as they occupy distinct regions.\nIsland Dream: Chinstrap (blue) and Adelie (red) penguins have overlapping regions, making classification slightly more challenging. However, longer culmen lengths help differentiate Chinstrap penguins.\nIsland Torgersen: Since only Adlie penguins are found here, the entire region is classified as red, confirming that island location alone can sometimes be a strong predictor.\n\nThe clean decision boundaries in most plots indicate that the selected features work really well for our classification."
  },
  {
    "objectID": "posts/Penguins/index.html#discussion-and-takeaways",
    "href": "posts/Penguins/index.html#discussion-and-takeaways",
    "title": "Classifying Palmer Penguins",
    "section": "Discussion And Takeaways",
    "text": "Discussion And Takeaways\nThrough this analysis, I gained several important insights about both the dataset and machine learning model performance. One of the key takeaways was the importance of data preprocessing, including encoding categorical variables and handling missing data, to ensure the model could learn effectively. By visualizing relationships between features, I noticed that certain physical traits, like culmen length and culmen depth, played a crucial role in distinguishing penguin species. Additionally, island location proved to be a strong predictor, as some species were exclusive to certain islands.\nTraining a logistic regression model with carefully selected features led to a 99% accuracy on the training set and 100% accuracy on the test set, showing that the model generalized well to unseen data. The decision region plots clearly illustrated how the model separated species based on their culmen features, with some overlap between Adelie and Chinstrap penguins making classification slightly more challenging. Overall, this project demonstrated how feature selection, visualization, and a well-trained model can achieve highly accurate classifications, reinforcing the power of machine learning in biological research."
  },
  {
    "objectID": "posts/Penguins/index.html#goal-of-project",
    "href": "posts/Penguins/index.html#goal-of-project",
    "title": "Classifying Palmer Penguins",
    "section": "Goal of Project",
    "text": "Goal of Project\nPredicting a penguin’s species based on its physical measurements is an interesting challenge in data analysis and machine learning. In this blog, we’ll explore the Palmer Penguins dataset, which was collected by Dr. Kristen Gorman and the Palmer Station, to analyze biological measurements of three penguin species in Antarctica. Through data visualization and feature selection, we’ll uncover key insights and build a classification model that achieves perfect accuracy using a carefully chosen set of features. Along the way, we’ll discuss our findings and ensure a reproducible and insightful approach to species classification."
  },
  {
    "objectID": "posts/Penguins/index.html#loading-data",
    "href": "posts/Penguins/index.html#loading-data",
    "title": "Classifying Palmer Penguins",
    "section": "Loading Data",
    "text": "Loading Data\nWe will now use the Pandas function to read the CSV and also take a look at the data and the different variables and columns we have in our dataset before we do and data explorations and analysis.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\ntrain.head()\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN"
  },
  {
    "objectID": "posts/Penguins/index.html#abstract",
    "href": "posts/Penguins/index.html#abstract",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "The Palmer Penguins dataset, collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, provides physiological measurements for three species of penguins—Adélie, Chinstrap, and Gentoo—inhabiting the Palmer Archipelago. First published in 2014 and later made widely accessible to the data science community, this dataset serves as a valuable resource for exploring classification techniques and species differentiation. In this blog post, I analyze the dataset using machine learning models to classify penguin species based on features such as flipper length, body mass, and culmen dimensions. By analyzing physical measurements we identify key features that contribute to accurate species prediction. Through data visualization and feature selection, we uncover patterns that distinguish species based on island location and physical traits. Using logistic regression, we evaluate different feature combinations and determine an optimal set for classification and acheive a 99% training accuracy and a 100% testing accuracy. Our findings highlight the power of data-driven approaches in biological classification while ensuring a reproducible and insightful methodology."
  },
  {
    "objectID": "posts/Penguins/index.html#visualizing-the-data",
    "href": "posts/Penguins/index.html#visualizing-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Visualizing the data",
    "text": "Visualizing the data\nThe bar chart below shows the number of penguins of each species on different islands, helping us identify patterns in species distribution. Interestingly, we observe:\n\nTorgersen Island: Only Adelie penguins are present, with a population of about 30-40 individuals.\nDream Island: Has both Adelie and Chinstrap penguins, with Chinstrap being the dominant species.\nBiscoe Island: Has both Adelie and Gentoo penguins, with Gentoo having the largest population overall.\n\nChinstrap penguins are only found on Dream Island, while Gentoo penguins are exclusive to Biscoe Island. Adelie penguins, on the other hand, are the most widespread, appearing on all three islands. Based on this distribution, the island where a penguin is found could be a useful feature for predicting its species.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n# Map species labels back to names\nspecies_map = {0: 'Adelie', 1: 'Chinstrap', 2: 'Gentoo'}\nspecies_island_counts = X_train.copy()\nspecies_island_counts[\"Species\"] = [species_map[label] for label in y_train]\n\n# Convert one-hot encoded islands back to a single column\nspecies_island_counts[\"Island\"] = species_island_counts[[\"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]].idxmax(axis=1)\nspecies_island_counts[\"Island\"] = species_island_counts[\"Island\"].str.replace(\"Island_\", \"\")\n\n# Count number of penguins per species per island\nplot_data = species_island_counts.groupby([\"Island\", \"Species\"]).size().reset_index(name=\"Count\")\n\n# Plot\nplt.figure(figsize=(8, 5))\nsns.barplot(data=plot_data, x=\"Island\", y=\"Count\", hue=\"Species\", palette=\"mako\")\nplt.xlabel(\"Island\")\nplt.ylabel(\"Number of Penguins\")\nplt.title(\"Penguin Species Distribution Across Islands\")\nplt.legend(title=\"Species\")\nplt.show()\n\n\n\n\n\n\n\n\nBased on the graph above, we can see that by simply looking at the islands, we can somewhat predict the species of the penguin. This is a good example of a feature that could help predict the species. However, the model would be much more efficient if each island only hosted one species of penguin. Since some islands have more than one species, we might want to explore additional features. For example, if a penguin is on Biscoe Island and has a certain Culmen length and Culmen depth, can we make more accurate predictions based on those measurements? To explore this, we will create a graph to assess whether including Culmen lenght and Culmen depth is a useful feature for predicting the species on each island.\n\n\n# Plotting the relationship between Culmen Length and Culmen Depth for each species on each island\ng = sns.relplot(data=train, hue=\"Species\", y=\"Culmen Depth (mm)\", x=\"Culmen Length (mm)\", col=\"Island\")\n\ng.set_titles(size=18)  # Increase subplot titles\ng.set_axis_labels(\"Culmen Length (mm)\", \"Culmen Depth (mm)\", fontsize=16)  # Increase axis labels font size\ng._legend.set_title(\"Species\", prop={'size': 16}) \n[g._legend.get_texts()[i].set_fontsize(14) for i in range(len(g._legend.get_texts()))]  # Increase legend text size\n\nplt.show()\n\n\n\n\n\n\n\n\n\nThe graph above looks at the relationship between Culmen Depth (mm) and Culmen Length (mm) for different penguin species, separated by island. Each species is color-coded to highlight patterns in their beak dimensions. From this graph, we can observe:\n\nGentoo penguins (orange) tend to have longer culmen lengths and shallower culmen depths.\nAdelie penguins (green) have moderate culmen lengths and a wide range of culmen depths.\nChinstrap penguins (blue) have longer culmen lenghts compared to Adelie penguins but exhibit similar culmen Depths."
  },
  {
    "objectID": "posts/Penguins/index.html#feature-analysis-flipper-and-culmen-measurements",
    "href": "posts/Penguins/index.html#feature-analysis-flipper-and-culmen-measurements",
    "title": "Classifying Palmer Penguins",
    "section": "Feature Analysis: Flipper and Culmen Measurements",
    "text": "Feature Analysis: Flipper and Culmen Measurements\nThe table below shows the mean values of flipper length, culmen length, and culmen depth for each penguin species using the .groupby() function. The table summarizes the average flipper length, culmen length, and culmen depth for each penguin species. Gentoo penguins have the longest flipper and culmen lengths but the shallowest culmen depth. Chinstrap penguins have longer culmen lengths than Adelie penguins but similar culmen depths. Adelie penguins have the shortest flipper and culmen lengths but slightly deeper culmens than Chinstrap. These differences highlight key features that can help classify penguin species.\n\ntrain.groupby('Species').agg({\n    'Flipper Length (mm)': 'mean',\n    'Culmen Length (mm)': 'mean',\n    'Culmen Depth (mm)': 'mean'\n}).reset_index()\n\n\n\n\n\n\n\n\nSpecies\nFlipper Length (mm)\nCulmen Length (mm)\nCulmen Depth (mm)\n\n\n\n\n0\nAdelie Penguin (Pygoscelis adeliae)\n190.084034\n38.970588\n18.409244\n\n\n1\nChinstrap penguin (Pygoscelis antarctica)\n196.000000\n48.826316\n18.366667\n\n\n2\nGentoo penguin (Pygoscelis papua)\n216.752577\n47.073196\n14.914433"
  },
  {
    "objectID": "posts/Penguins/index.html#lets-now-prepare-our-test-dataset",
    "href": "posts/Penguins/index.html#lets-now-prepare-our-test-dataset",
    "title": "Classifying Palmer Penguins",
    "section": "Let’s now prepare our Test Dataset",
    "text": "Let’s now prepare our Test Dataset\n\n# Load test data\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n# Prepare test data\nX_test, y_test = prepare_data(test)"
  },
  {
    "objectID": "posts/Penguins/index.html#now-we-are-going-to-chose-features-to-train-our-model",
    "href": "posts/Penguins/index.html#now-we-are-going-to-chose-features-to-train-our-model",
    "title": "Classifying Palmer Penguins",
    "section": "Now we are going to chose features to train our model",
    "text": "Now we are going to chose features to train our model\nBy using the combinations function from the itertools package, we generate different combinations of categorical and continuous features. So we will pair categorical variables like Sex and Clutch Completion with continuous variables such as Culmen Length, Culmen Depth, and Flipper Length. This allows us to explore various feature combinations and evaluate which ones might be most useful for accurately classifying the penguin species.\n\nfrom itertools import combinations\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nimport numpy as np\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nall_qual_cols = [\"Clutch Completion\", \"Sex\", \"Island_Biscoe\", \"Island_Dream\", \"Island_Torgersen\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nbest_score = 0\nbest_features = None\n\nfor qual in all_qual_cols:\n    qual_cols = [col for col in X_train.columns if qual in col]  # Ensure categorical features are one-hot encoded\n    for pair in combinations(all_quant_cols, 2):\n        cols = qual_cols + list(pair)\n        \n        # Train a logistic regression model to evaluate feature combinations\n        model = LogisticRegression(max_iter=1000)  # Increased iterations for better convergence\n        scores = cross_val_score(model, X_train[cols], y_train, cv=5, scoring='accuracy')\n        \n        avg_score = np.mean(scores)\n        if avg_score &gt; best_score:\n            best_score = avg_score\n            best_features = cols\n\nprint(\"Best features:\", best_features)\n\nBest features: ['Sex_FEMALE', 'Sex_MALE', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nAccording to itertools, the best features to use are ‘Sex_FEMALE’, ‘Sex_MALE’, ‘Culmen Length (mm)’, and ‘Culmen Depth (mm)’. However, we wanted to explore how our model would perform based on our exploration of the island features along with the Culmen features. Thus, we will now reassign these as the best features and use them to train our model.\n\n#Rearranging Best featrues so that the first two columns are the quantitative columns and the rest are the qualitative columns\nbest_features = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\nbest_features\n\n['Culmen Length (mm)',\n 'Culmen Depth (mm)',\n 'Island_Biscoe',\n 'Island_Dream',\n 'Island_Torgersen']"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#loading-the-data",
    "href": "posts/new-new-test-post/index.html#loading-the-data",
    "title": "Classifying Palmer Penguins",
    "section": "Loading the data",
    "text": "Loading the data\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#abstract-and-methodologies",
    "href": "posts/new-new-test-post/index.html#abstract-and-methodologies",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#analysis-of-loan-intent-by-age-group",
    "href": "posts/new-new-test-post/index.html#analysis-of-loan-intent-by-age-group",
    "title": "Classifying Palmer Penguins",
    "section": "Analysis of Loan Intent by Age Group",
    "text": "Analysis of Loan Intent by Age Group\nThis bar chart below shows how different age groups use their loans for various purposes—such as venture, education, medical, home improvement, personal, and debt consolidation. We can see that borrowers aged 18–29 make up a large portion of total loans, often driven by education and personal loan needs. As age increases, the number of loans generally decreases, but certain categories—like debt consolidation—can become more common in older groups.\nOverall, this chart highlights that younger borrowers borrow a lot more money may be more focused on educational or personal financing, while older borrowers might shift their attention to consolidating debt or improving their homes. Understanding these patterns help us in understanding different patternns in who would default on a loan and not.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n# Create additional features for analysis:\n# 1. Age groups: we create bins to see how loan intent varies with age.\nage_bins = [18, 30, 40, 50, 60, 100]\nage_labels = ['18-29', '30-39', '40-49', '50-59', '60+']\ndf['age_group'] = pd.cut(df['person_age'], bins=age_bins, labels=age_labels)\n\n# ---------------------------\n# Visualization 1:\n# How does loan intent vary with age and home ownership status?\nplt.figure(figsize=(10, 6))\nsns.countplot(data=df, x='age_group', hue='loan_intent')\nplt.title('Loan Intent Distribution by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Count')\nplt.legend(title='Loan Intent')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "href": "posts/new-new-test-post/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "title": "Classifying Palmer Penguins",
    "section": "Analysis of Average Loan Amount by Credit History Length",
    "text": "Analysis of Average Loan Amount by Credit History Length\nThis bar chart shows how the average loan amount changes based on the number of years a borrower has had a credit history. In general, we see that some longer lengths of credit history are associated with higher average loan amounts than others, though the pattern isn’t strictly increasing or decreasing. This suggests that lenders may be willing to extend larger lines of credit to individuals with certain credit history profiles.\nFor our automated decision system, credit history length could be an important feature because it often reflects a borrower’s past experience with credit and repayment behavior. However, we must be mindful of fairness and potential biases: borrowers who are younger or newer to credit might be at a disadvantage if the model heavily weighs credit history length. Balancing profitability for the bank with equitable access to credit remains a key challenge in designing our scoring and thresholding methods.\n\n\n# 2. Employment length groups: useful for exploring patterns with job experience.\nemp_bins = [0, 2, 5, 10, df['person_emp_length'].max()]\nemp_labels = ['0-1 yrs', '2-4 yrs', '5-9 yrs', '10+ yrs']\ndf['emp_length_group'] = pd.cut(df['person_emp_length'], bins=emp_bins, labels=emp_labels)\n\n# ---------------------------\n# Visualization 2:\n# Which segments are offered different interest rates? Compare distributions by home ownership.\nplt.figure(figsize=(10, 6))\navg_loan = df.groupby('cb_person_cred_hist_length')['loan_amnt'].mean().reset_index()\nsns.barplot(data=avg_loan, x='cb_person_cred_hist_length', y='loan_amnt')\nplt.title('Average Loan Amount by Credit History Length')\nplt.xlabel('Credit History Length (Years)')\nplt.ylabel('Average Loan Amount')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "href": "posts/new-new-test-post/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "title": "Classifying Palmer Penguins",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership\nIn our scatter plot below, each dot represents a borrower, with the x-axis showing how large the loan is relative to their income (as a percentage) and the y-axis showing the absolute loan amount. The colors indicate different types of home ownership (RENT, MORTGAGE, OWN, OTHER).\nAs you can see, the data points overlap heavily, making the chart look cluttered. To get a clearer picture, I wo;; split these data into separate graphs for each home ownership category. This will help us see more nuanced patterns—like whether renters tend to have higher loan-to-income ratios compared to those who own or have a mortgage.\n\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=df, \n    x='loan_percent_income_pct', \n    y='loan_amnt', \n    hue='person_home_ownership',\n    alpha=0.7\n)\nplt.title('Loan Amount vs. Loan % of Income by Home Ownership')\nplt.xlabel('Loan as % of Person Income')\nplt.ylabel('Loan Amount')\nplt.xlim(0, 100)  # Focus on 0–100% if most loans fall in this range\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "href": "posts/new-new-test-post/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "title": "Classifying Palmer Penguins",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)\nBy splitting the data into four subplots (one for each home ownership category), we can see that:\n\nRenters typically have loan amounts averaging around $10,000–$12,000, with a wide spread of loan-to-income ratios (averaging around 15–20%). In addition to this we see the most clustering for this group which means, in our dataset most of the people taking our loans are from this group.\nMortgage holders often take out larger loans (averaging $16,000–$18,000) but may have lower loan-to-income ratios (closer to 10% on average).\nOwners (those who fully own their homes) tend to borrow moderate amounts ($12,000–$15,000) at ratios of around 12–15%.\nOthers (less common categories) show a broad mix but generally fall between these ranges.\n\nThese distinctions are important for our automated decision system, since each home ownership group presents a different risk and borrowing profile. When designing a score function and threshold to maximize the bank’s profit, it’s important for us to consider whether certain groups (like renters) might be unfairly penalized if they tend to have higher loan-to-income ratios. Ultimately, these separate plots help us fine-tune our model so that we balance profitability with equitable access to credit across different segments of borrowers.\n\ndf['loan_percent_income_pct'] = df['loan_percent_income'] * 100\n\n# Create a FacetGrid: one subplot per home ownership category\ng = sns.FacetGrid(df, col=\"person_home_ownership\", col_wrap=2, height=4)\ng.map(sns.scatterplot, \"loan_percent_income_pct\", \"loan_amnt\", alpha=0.7)\n\n# Set the x-axis limits and labels for clarity\ng.set(xlim=(0, 100))\ng.set_axis_labels(\"Loan as % of Income\", \"Loan Amount\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "href": "posts/new-new-test-post/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "title": "Classifying Palmer Penguins",
    "section": "Summary of Loan Intent and Home Ownership Segments",
    "text": "Summary of Loan Intent and Home Ownership Segments\nThis table shows how different combinations of loan intent (e.g., EDUCATION, MEDICAL, PERSONAL) and home ownership (MORTGAGE, OWN, RENT, OTHER) compare in terms of average interest rate, average loan amount, and count of borrowers. We’ve sorted the table by average loan amount in descending order to identify which segments receive the largest lines of credit.\n\nHighest Averages: Segments like PERSONAL–OTHER and MEDICAL–OTHER appear near the top, suggesting they receive higher loan amounts (over $12,000 on average), but also tend to have higher interest rates (11–12%).\nMortgage vs. Rent: Many MORTGAGE segments (e.g., DEBTCONSOLIDATION–MORTGAGE, EDUCATION–MORTGAGE) cluster in the middle, with average loan amounts around $10,000–$11,000 and interest rates near 10–10.6%. Renters often see slightly higher interest rates (11–12%) and somewhat lower loan amounts (around $8,000–$9,000).\nLow Counts: Some segments have very few borrowers (like DEBTCONSOLIDATION–OWN with a count of only 62), which may not be reliable for broad conclusions.\n\nFrom the perspective of building an automated decision system, these patterns hint at where the bank’s profit opportunities and risks might lie. For instance, segments with higher average loan amounts but also higher interest rates could be more profitable—but might also carry greater default risk. Tracking how many borrowers fall into each segment (the “count” column) helps ensure the model doesn’t overly focus on small, potentially unrepresentative groups.\n\nsummary_table = (\n    df\n    .groupby(['loan_intent', 'person_home_ownership'], as_index=False)\n    .agg(\n        avg_interest_rate=('loan_int_rate', 'mean'),\n        avg_loan_amount=('loan_amnt', 'mean'),\n        count=('loan_amnt', 'count')  # how many borrowers in each segment\n    )\n)\n\n# Sort by average loan amount (descending) to see which segments get the largest lines of credit\nsummary_table_sorted_by_amount = summary_table.sort_values('avg_loan_amount', ascending=False)\n\nsummary_table_sorted_by_amount\n\n\n\n\n\n\n\n\n\nloan_intent\nperson_home_ownership\navg_interest_rate\navg_loan_amount\ncount\n\n\n\n\n17\nPERSONAL\nOTHER\n11.675714\n12366.666667\n15\n\n\n13\nMEDICAL\nOTHER\n12.745000\n12200.000000\n13\n\n\n5\nEDUCATION\nOTHER\n12.400833\n12142.857143\n14\n\n\n9\nHOMEIMPROVEMENT\nOTHER\n11.683000\n10959.090909\n11\n\n\n8\nHOMEIMPROVEMENT\nMORTGAGE\n10.613916\n10764.017341\n1384\n\n\n20\nVENTURE\nMORTGAGE\n10.468000\n10606.281060\n1811\n\n\n0\nDEBTCONSOLIDATION\nMORTGAGE\n10.400489\n10588.756111\n1841\n\n\n4\nEDUCATION\nMORTGAGE\n10.554563\n10502.178076\n2089\n\n\n12\nMEDICAL\nMORTGAGE\n10.505553\n10485.867052\n1730\n\n\n16\nPERSONAL\nMORTGAGE\n10.426285\n10481.223233\n1868\n\n\n21\nVENTURE\nOTHER\n12.274211\n10367.500000\n20\n\n\n11\nHOMEIMPROVEMENT\nRENT\n11.812287\n10109.604633\n1252\n\n\n1\nDEBTCONSOLIDATION\nOTHER\n11.566667\n9783.333333\n15\n\n\n14\nMEDICAL\nOWN\n10.749097\n9367.755682\n352\n\n\n10\nHOMEIMPROVEMENT\nOWN\n10.922405\n9242.450980\n255\n\n\n6\nEDUCATION\nOWN\n10.797507\n8996.177184\n412\n\n\n18\nPERSONAL\nOWN\n10.867262\n8944.209040\n354\n\n\n3\nDEBTCONSOLIDATION\nRENT\n11.358223\n8882.754425\n2260\n\n\n19\nPERSONAL\nRENT\n11.535415\n8826.900046\n2171\n\n\n23\nVENTURE\nRENT\n11.438455\n8804.566745\n2135\n\n\n22\nVENTURE\nOWN\n10.560000\n8789.621914\n648\n\n\n7\nEDUCATION\nRENT\n11.315216\n8685.308193\n2612\n\n\n15\nMEDICAL\nRENT\n11.422580\n8426.925182\n2740\n\n\n2\nDEBTCONSOLIDATION\nOWN\n14.432909\n7749.193548\n62"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#training-and-evaluating-our-logistic-regression-model",
    "href": "posts/new-new-test-post/index.html#training-and-evaluating-our-logistic-regression-model",
    "title": "Classifying Palmer Penguins",
    "section": "Training and evaluating our Logistic Regression model",
    "text": "Training and evaluating our Logistic Regression model\nI used a logistic regression model to predict whether a prospective borrower will default on a loan. After preprocessing the data—by standardizing numerical features and one-hot encoding categorical variables. I removed rows with missing values, and split the dataset into training and test sets. The model achieved a test accuracy of about 84.2%.\nThe confusion matrix provides additional insight into the model’s performance:\n\nTrue Negatives (TN): 3,393 borrowers who did not default and were correctly predicted as non-default.\nFalse Positives (FP): 221 borrowers who did not default but were incorrectly flagged as defaults.\nFalse Negatives (FN): 501 borrowers who defaulted but were missed by the model.\nTrue Positives (TP): 467 borrowers who defaulted and were correctly identified.\n\nOur results suggest that while the model performs reasonably well overall, there is still a balance to be struck between avoiding false positives and false negatives. This is particularly important when designing an automated decision system for credit risk, because both profitability for the bank and equitable access to credit are critical. Further tuning of the threshold and exploration of additional features could help optimize the model even further for its intended purpose\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Drop rows with missing values\ndf_train = df_train.dropna(subset=numeric_features + categorical_features)\n\n\ntarget = 'loan_status'\nX = df_train.drop(columns=[target])\ny = df_train[target]\n\nnumeric_features = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', \n                      'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\ncategorical_features = ['person_home_ownership', 'loan_intent', 'cb_person_default_on_file']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n    ])\nX_transformed = preprocessor.fit_transform(X)\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, test_size=0.2, random_state=123\n)\n\n# Fit a logistic regression model using the preprocessed features\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(\"Test Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\nTest Accuracy: 0.8424268878219118\nConfusion Matrix:\n [[3393  221]\n [ 501  467]]"
  },
  {
    "objectID": "posts/new-new-test-post/index.html#finding-weight-and-threshold",
    "href": "posts/new-new-test-post/index.html#finding-weight-and-threshold",
    "title": "Classifying Palmer Penguins",
    "section": "Finding weight and threshold",
    "text": "Finding weight and threshold\n\n# ------------------------------------------------------\n# 1. Define the profit formulas (vectorized)\n# ------------------------------------------------------\ndef profit_if_repaid(loan_amnt, loan_int_rate):\n    \"\"\"\n    If the loan is repaid in full, the bank's profit is:\n      loan_amnt * (1 + 0.25*loan_int_rate)^10 - loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 10 - loan_amnt\n\ndef profit_if_default(loan_amnt, loan_int_rate):\n    \"\"\"\n    If the borrower defaults, we assume default happens 3 years into the loan, \n    and the bank loses 70% of the principal:\n      loan_amnt*(1 + 0.25*loan_int_rate)^3 - 1.7*loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 3 - 1.7 * loan_amnt\n\nWe will now compute predicted probabilities (probability of default) for the training data\n\ny_prob_train = model.predict_proba(X_train)[:, 1]  # column 1 = probability of default\n\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)\nloan_amnt_array = X_train['loan_amnt'].to_numpy()\nloan_int_rate_array = X_train['loan_int_rate'].to_numpy()\ny_train_array = y_train.to_numpy()\nprofit_repaid = profit_if_repaid(loan_amnt_array, loan_int_rate_array)\nprofit_default = profit_if_default(loan_amnt_array, loan_int_rate_array)\n\nNow we are going to find a threshold and identify teh best threshold\n\n# 5. Sweep over thresholds to find the one that maximizes average profit\n\nthresholds = np.linspace(0, 1, 101)\navg_profits = []\n\nfor t in thresholds:\n    # Predict default if probability &gt;= t\n    predicted_default = (y_prob_train &gt;= t).astype(int)\n      # If we predict default, we do NOT give the loan =&gt; profit = 0\n    # If we predict no default, we DO give the loan =&gt; actual profit depends on y_train_array\n    #    - If actual y=0 (no default), profit = profit_repaid\n    #    - If actual y=1 (default), profit = profit_default\n    give_loan = 1 - predicted_default  # 1 = give loan, 0 = no loan\n    # total_profit[i] = give_loan[i] * [ (1 - y[i])*profit_repaid[i] + y[i]*profit_default[i] ]\n    total_profit = give_loan * ((1 - y_train_array) * profit_repaid + y_train_array * profit_default)\n    \n    # Compute average profit per borrower\n    avg_profit = total_profit.mean()\n    avg_profits.append(avg_profit)\n\navg_profits = np.array(avg_profits)\n\n# 6. Identify the best threshold\n\nbest_idx = np.argmax(avg_profits) # index of the best threshold\nbest_threshold = thresholds[best_idx]\n\n\nprint(f\"Best Threshold: {best_threshold:.3f}\")\n\nBest Threshold: 1.000\n\n\n\n# 7. Plot profit vs. threshold\n\nplt.figure(figsize=(8, 5))\nplt.plot(thresholds, avg_profits, label='Profit per Borrower')\nplt.scatter(best_threshold, best_profit, color='red', zorder=10, label='Optimal Threshold')\nplt.title('Profit per Borrower (Training Set) vs. Threshold')\nplt.xlabel('Threshold (Probability of Default)')\nplt.ylabel('Average Profit per Borrower')\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html",
    "href": "posts/discecting_racial_bias_algo/index.html",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "In this project, I aim to replicate and extend the findings of Obermeyer et al. (2019) by exploring how healthcare cost predictions relate to patients’ chronic illness burden and race. Using a randomized dataset, we first visualize the relationship between risk score percentiles, chronic conditions, and medical expenditures, revealing that White patients tend to generate higher costs than Black patients despite similar illness burdens. We then built a Ridge regression model with polynomial features to quantify the disparity in costs between Black and White patients. Our final model estimates that, holding illness burden constant, Black patients incur roughly 81% of the costs incurred by White patients, suggesting that the cost-based risk score underestimates the true care needs of Black patients."
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#exporing-the-data",
    "href": "posts/discecting_racial_bias_algo/index.html#exporing-the-data",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "Below we will accesss teh data clean it and explore the different variables and features.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\n# df.dropna(inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\n\nrace_counts_bf_clean = df['race'].value_counts()\ndf.dropna(inplace=True)\nrace_counts = df['race'].value_counts()\nprint(\"Count of patients by race before cleaning data\",race_counts_bf_clean)\nprint(\"Count of patients by race after cleaning data\", race_counts)\n\nCount of patients by race before cleaning data race\nwhite    43202\nblack     5582\nName: count, dtype: int64\nCount of patients by race after cleaning data race\nwhite    5911\nblack    1000\nName: count, dtype: int64\n\n\n\nimport  matplotlib.pyplot as plt\n# Graph 1: Bar chart of race counts\nplt.figure(figsize=(6, 4))\nplt.bar(race_counts.index, race_counts.values, color=['blue', 'orange'])\nplt.xlabel('Race')\nplt.ylabel('Number of Patients')\nplt.title('Number of Black vs. White Patients')\nplt.show()\n\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x=\"risk_score_t\", y=\"cost_t\", hue=\"race\", alpha=0.7)\n\n# Labels and title\nplt.xlabel(\"Risk Score\")\nplt.ylabel(\"Cost\")\nplt.title(\"Risk Score vs Cost by Race\")\n\n# Show the plot\nplt.show()"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html",
    "href": "posts/linear_score_function-Banks/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#abstract-and-methodologies",
    "href": "posts/linear_score_function-Banks/index.html#abstract-and-methodologies",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#loading-the-data",
    "href": "posts/linear_score_function-Banks/index.html#loading-the-data",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Loading the data",
    "text": "Loading the data\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#analysis-of-loan-intent-by-age-group",
    "href": "posts/linear_score_function-Banks/index.html#analysis-of-loan-intent-by-age-group",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Intent by Age Group",
    "text": "Analysis of Loan Intent by Age Group\nThis bar chart below shows how different age groups use their loans for various purposes—such as venture, education, medical, home improvement, personal, and debt consolidation. We can see that borrowers aged 18–29 make up a large portion of total loans, often driven by education and personal loan needs. As age increases, the number of loans generally decreases, but certain categories—like debt consolidation—can become more common in older groups.\nOverall, this chart highlights that younger borrowers borrow a lot more money may be more focused on educational or personal financing, while older borrowers might shift their attention to consolidating debt or improving their homes. Understanding these patterns help us in understanding different patternns in who would default on a loan and not.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n# Create additional features for analysis:\n# 1. Age groups: we create bins to see how loan intent varies with age.\nage_bins = [18, 30, 40, 50, 60, 100]\nage_labels = ['18-29', '30-39', '40-49', '50-59', '60+']\ndf['age_group'] = pd.cut(df['person_age'], bins=age_bins, labels=age_labels)\n\n# ---------------------------\n# Visualization 1:\n# How does loan intent vary with age and home ownership status?\nplt.figure(figsize=(10, 6))\nsns.countplot(data=df, x='age_group', hue='loan_intent')\nplt.title('Loan Intent Distribution by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Count')\nplt.legend(title='Loan Intent')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "href": "posts/linear_score_function-Banks/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Average Loan Amount by Credit History Length",
    "text": "Analysis of Average Loan Amount by Credit History Length\nThis bar chart shows how the average loan amount changes based on the number of years a borrower has had a credit history. In general, we see that some longer lengths of credit history are associated with higher average loan amounts than others, though the pattern isn’t strictly increasing or decreasing. This suggests that lenders may be willing to extend larger lines of credit to individuals with certain credit history profiles.\nFor our automated decision system, credit history length could be an important feature because it often reflects a borrower’s past experience with credit and repayment behavior. However, we must be mindful of fairness and potential biases: borrowers who are younger or newer to credit might be at a disadvantage if the model heavily weighs credit history length. Balancing profitability for the bank with equitable access to credit remains a key challenge in designing our scoring and thresholding methods.\n\n\n# 2. Employment length groups: useful for exploring patterns with job experience.\nemp_bins = [0, 2, 5, 10, df['person_emp_length'].max()]\nemp_labels = ['0-1 yrs', '2-4 yrs', '5-9 yrs', '10+ yrs']\ndf['emp_length_group'] = pd.cut(df['person_emp_length'], bins=emp_bins, labels=emp_labels)\n\n# ---------------------------\n# Visualization 2:\n# Which segments are offered different interest rates? Compare distributions by home ownership.\nplt.figure(figsize=(10, 6))\navg_loan = df.groupby('cb_person_cred_hist_length')['loan_amnt'].mean().reset_index()\nsns.barplot(data=avg_loan, x='cb_person_cred_hist_length', y='loan_amnt')\nplt.title('Average Loan Amount by Credit History Length')\nplt.xlabel('Credit History Length (Years)')\nplt.ylabel('Average Loan Amount')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "href": "posts/linear_score_function-Banks/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership\nIn our scatter plot below, each dot represents a borrower, with the x-axis showing how large the loan is relative to their income (as a percentage) and the y-axis showing the absolute loan amount. The colors indicate different types of home ownership (RENT, MORTGAGE, OWN, OTHER).\nAs you can see, the data points overlap heavily, making the chart look cluttered. To get a clearer picture, I wo;; split these data into separate graphs for each home ownership category. This will help us see more nuanced patterns—like whether renters tend to have higher loan-to-income ratios compared to those who own or have a mortgage.\n\ndf['loan_percent_income_pct'] = df['loan_percent_income'] * 100\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=df, \n    x='loan_percent_income_pct', \n    y='loan_amnt', \n    hue='person_home_ownership',\n    alpha=0.7\n)\nplt.title('Loan Amount vs. Loan % of Income by Home Ownership')\nplt.xlabel('Loan as % of Person Income')\nplt.ylabel('Loan Amount')\nplt.xlim(0, 100)  # Focus on 0–100% if most loans fall in this range\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "href": "posts/linear_score_function-Banks/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)\nBy splitting the data into four subplots (one for each home ownership category), we can see that:\n\nRenters typically have loan amounts averaging around $10,000–$12,000, with a wide spread of loan-to-income ratios (averaging around 15–20%). In addition to this we see the most clustering for this group which means, in our dataset most of the people taking our loans are from this group.\nMortgage holders often take out larger loans (averaging $16,000–$18,000) but may have lower loan-to-income ratios (closer to 10% on average).\nOwners (those who fully own their homes) tend to borrow moderate amounts ($12,000–$15,000) at ratios of around 12–15%.\nOthers (less common categories) show a broad mix but generally fall between these ranges.\n\nThese distinctions are important for our automated decision system, since each home ownership group presents a different risk and borrowing profile. When designing a score function and threshold to maximize the bank’s profit, it’s important for us to consider whether certain groups (like renters) might be unfairly penalized if they tend to have higher loan-to-income ratios. Ultimately, these separate plots help us fine-tune our model so that we balance profitability with equitable access to credit across different segments of borrowers.\n\n\n\n# Create a FacetGrid: one subplot per home ownership category\ng = sns.FacetGrid(df, col=\"person_home_ownership\", col_wrap=2, height=4)\ng.map(sns.scatterplot, \"loan_percent_income_pct\", \"loan_amnt\", alpha=0.7)\n\n# Set the x-axis limits and labels for clarity\ng.set(xlim=(0, 100))\ng.set_axis_labels(\"Loan as % of Income\", \"Loan Amount\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "href": "posts/linear_score_function-Banks/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Summary of Loan Intent and Home Ownership Segments",
    "text": "Summary of Loan Intent and Home Ownership Segments\nThis table shows how different combinations of loan intent (e.g., EDUCATION, MEDICAL, PERSONAL) and home ownership (MORTGAGE, OWN, RENT, OTHER) compare in terms of average interest rate, average loan amount, and count of borrowers. We’ve sorted the table by average loan amount in descending order to identify which segments receive the largest lines of credit.\n\nHighest Averages: Segments like PERSONAL–OTHER and MEDICAL–OTHER appear near the top, suggesting they receive higher loan amounts (over $12,000 on average), but also tend to have higher interest rates (11–12%).\nMortgage vs. Rent: Many MORTGAGE segments (e.g., DEBTCONSOLIDATION–MORTGAGE, EDUCATION–MORTGAGE) cluster in the middle, with average loan amounts around $10,000–$11,000 and interest rates near 10–10.6%. Renters often see slightly higher interest rates (11–12%) and somewhat lower loan amounts (around $8,000–$9,000).\nLow Counts: Some segments have very few borrowers (like DEBTCONSOLIDATION–OWN with a count of only 62), which may not be reliable for broad conclusions.\n\nFrom the perspective of building an automated decision system, these patterns hint at where the bank’s profit opportunities and risks might lie. For instance, segments with higher average loan amounts but also higher interest rates could be more profitable—but might also carry greater default risk. Tracking how many borrowers fall into each segment (the “count” column) helps ensure the model doesn’t overly focus on small, potentially unrepresentative groups.\n\nsummary_table = (\n    df\n    .groupby(['loan_intent', 'person_home_ownership'], as_index=False)\n    .agg(\n        avg_interest_rate=('loan_int_rate', 'mean'),\n        avg_loan_amount=('loan_amnt', 'mean'),\n        count=('loan_amnt', 'count')  # how many borrowers in each segment\n    )\n)\n\n# Sort by average loan amount (descending) to see which segments get the largest lines of credit\nsummary_table_sorted_by_amount = summary_table.sort_values('avg_loan_amount', ascending=False)\n\nsummary_table_sorted_by_amount\n\n\n\n\n\n\n\n\n\nloan_intent\nperson_home_ownership\navg_interest_rate\navg_loan_amount\ncount\n\n\n\n\n17\nPERSONAL\nOTHER\n11.675714\n12366.666667\n15\n\n\n13\nMEDICAL\nOTHER\n12.745000\n12200.000000\n13\n\n\n5\nEDUCATION\nOTHER\n12.400833\n12142.857143\n14\n\n\n9\nHOMEIMPROVEMENT\nOTHER\n11.683000\n10959.090909\n11\n\n\n8\nHOMEIMPROVEMENT\nMORTGAGE\n10.613916\n10764.017341\n1384\n\n\n20\nVENTURE\nMORTGAGE\n10.468000\n10606.281060\n1811\n\n\n0\nDEBTCONSOLIDATION\nMORTGAGE\n10.400489\n10588.756111\n1841\n\n\n4\nEDUCATION\nMORTGAGE\n10.554563\n10502.178076\n2089\n\n\n12\nMEDICAL\nMORTGAGE\n10.505553\n10485.867052\n1730\n\n\n16\nPERSONAL\nMORTGAGE\n10.426285\n10481.223233\n1868\n\n\n21\nVENTURE\nOTHER\n12.274211\n10367.500000\n20\n\n\n11\nHOMEIMPROVEMENT\nRENT\n11.812287\n10109.604633\n1252\n\n\n1\nDEBTCONSOLIDATION\nOTHER\n11.566667\n9783.333333\n15\n\n\n14\nMEDICAL\nOWN\n10.749097\n9367.755682\n352\n\n\n10\nHOMEIMPROVEMENT\nOWN\n10.922405\n9242.450980\n255\n\n\n6\nEDUCATION\nOWN\n10.797507\n8996.177184\n412\n\n\n18\nPERSONAL\nOWN\n10.867262\n8944.209040\n354\n\n\n3\nDEBTCONSOLIDATION\nRENT\n11.358223\n8882.754425\n2260\n\n\n19\nPERSONAL\nRENT\n11.535415\n8826.900046\n2171\n\n\n23\nVENTURE\nRENT\n11.438455\n8804.566745\n2135\n\n\n22\nVENTURE\nOWN\n10.560000\n8789.621914\n648\n\n\n7\nEDUCATION\nRENT\n11.315216\n8685.308193\n2612\n\n\n15\nMEDICAL\nRENT\n11.422580\n8426.925182\n2740\n\n\n2\nDEBTCONSOLIDATION\nOWN\n14.432909\n7749.193548\n62"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#training-and-evaluating-our-logistic-regression-model",
    "href": "posts/linear_score_function-Banks/index.html#training-and-evaluating-our-logistic-regression-model",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Training and evaluating our Logistic Regression model",
    "text": "Training and evaluating our Logistic Regression model\nI used a logistic regression model to predict whether a prospective borrower will default on a loan. After preprocessing the data—by standardizing numerical features and one-hot encoding categorical variables. I removed rows with missing values, and split the dataset into training and test sets. The model achieved a test accuracy of about 84.2%.\nThe confusion matrix provides additional insight into the model’s performance:\n\nTrue Negatives (TN): 3,393 borrowers who did not default and were correctly predicted as non-default.\nFalse Positives (FP): 221 borrowers who did not default but were incorrectly flagged as defaults.\nFalse Negatives (FN): 501 borrowers who defaulted but were missed by the model.\nTrue Positives (TP): 467 borrowers who defaulted and were correctly identified.\n\nOur results suggest that while the model performs reasonably well overall, there is still a balance to be struck between avoiding false positives and false negatives. This is particularly important when designing an automated decision system for credit risk, because both profitability for the bank and equitable access to credit are critical. Further tuning of the threshold and exploration of additional features could help optimize the model even further for its intended purpose\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Drop rows with missing values\ndf_train = df_train.dropna(subset=numeric_features + categorical_features)\n\n\n\n\ntarget = 'loan_status'\nX = df_train.drop(columns=[target])\ny = df_train[target]\n\nnumeric_features = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', \n                      'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\ncategorical_features = ['person_home_ownership', 'loan_intent', 'cb_person_default_on_file']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n    ])\nX_transformed = preprocessor.fit_transform(X)\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, test_size=0.2, random_state=123\n)\n\n# Fit a logistic regression model using the preprocessed features\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(\"Test Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\nTest Accuracy: 0.8424268878219118\nConfusion Matrix:\n [[3393  221]\n [ 501  467]]"
  },
  {
    "objectID": "posts/linear_score_function-Banks/index.html#finding-weight-and-threshold",
    "href": "posts/linear_score_function-Banks/index.html#finding-weight-and-threshold",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Finding weight and threshold",
    "text": "Finding weight and threshold\n\ndf['loan_int_rate_decimal'] = df['loan_int_rate'] / 100.0\n\ndef profit_if_repaid(loan_amnt, loan_int_rate_decimal):\n    \"\"\"\n    If the loan is repaid in full, the bank's profit is:\n      loan_amnt * (1 + 0.25*loan_int_rate)^10 - loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 10 - loan_amnt\n\ndef profit_if_default(loan_amnt, loan_int_rate_decimal):\n    \"\"\"\n    If the borrower defaults, we assume default happens 3 years into the loan, \n    and the bank loses 70% of the principal:\n      loan_amnt*(1 + 0.25*loan_int_rate)^3 - 1.7*loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 3 - 1.7 * loan_amnt\n\nWe will now compute predicted probabilities (probability of default) for the training data\n\ny_prob_train = model.predict_proba(X_train)[:, 1]  # column 1 = probability of default\n\n\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)\nloan_amnt_array = X_train['loan_amnt'].to_numpy()\nloan_int_rate_array = X_train['loan_int_rate_decimal'].to_numpy()\ny_train_array = y_train.to_numpy()\nprofit_repaid = profit_if_repaid(loan_amnt_array, loan_int_rate_array)\nprofit_default = profit_if_default(loan_amnt_array, loan_int_rate_array)\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'loan_int_rate_decimal'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\n/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb Cell 22 line 6\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; X_train, X_test, y_train, y_test = train_test_split(\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt;     X, y, test_size=0.2, random_state=123\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt; )\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt; loan_amnt_array = X_train['loan_amnt'].to_numpy()\n----&gt; &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt; loan_int_rate_array = X_train['loan_int_rate_decimal'].to_numpy()\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; y_train_array = y_train.to_numpy()\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt; profit_repaid = profit_if_repaid(loan_amnt_array, loan_int_rate_array)\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/frame.py:4102, in DataFrame.__getitem__(self, key)\n   4100 if self.columns.nlevels &gt; 1:\n   4101     return self._getitem_multilevel(key)\n-&gt; 4102 indexer = self.columns.get_loc(key)\n   4103 if is_integer(indexer):\n   4104     indexer = [indexer]\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-&gt; 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 'loan_int_rate_decimal'\n\n\n\nNow we are going to find a threshold and identify teh best threshold\n\n# 5. Sweep over thresholds to find the one that maximizes average profit\n\nthresholds = np.linspace(0, 1, 101)\navg_profits = []\n\nfor t in thresholds:\n    # Predict default if probability &gt;= t\n    predicted_default = (y_prob_train &gt;= t).astype(int)\n      # If we predict default, we do NOT give the loan =&gt; profit = 0\n    # If we predict no default, we DO give the loan =&gt; actual profit depends on y_train_array\n    #    - If actual y=0 (no default), profit = profit_repaid\n    #    - If actual y=1 (default), profit = profit_default\n    give_loan = 1 - predicted_default  # 1 = give loan, 0 = no loan\n    # total_profit[i] = give_loan[i] * [ (1 - y[i])*profit_repaid[i] + y[i]*profit_default[i] ]\n    total_profit = give_loan * ((1 - y_train_array) * profit_repaid + y_train_array * profit_default)\n    \n    # Compute average profit per borrower\n    avg_profit = total_profit.mean()\n    avg_profits.append(avg_profit)\n\navg_profits = np.array(avg_profits)\n\n# 6. Identify the best threshold\n\nbest_idx = np.argmax(avg_profits) # index of the best threshold\nbest_threshold = thresholds[best_idx]\n\n\nprint(f\"Best Threshold: {best_threshold:.3f}\")\n\nBest Threshold: 1.000\n\n\n\n# 7. Plot profit vs. threshold\n\nplt.figure(figsize=(8, 5))\nplt.plot(thresholds, avg_profits, label='Profit per Borrower')\nplt.scatter(best_threshold, best_profit, color='red', zorder=10, label='Optimal Threshold')\nplt.title('Profit per Borrower (Training Set) vs. Threshold')\nplt.xlabel('Threshold (Probability of Default)')\nplt.ylabel('Average Profit per Borrower')\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#plotting-risk-score-percentiles-against-mean-number-of-active-chronic-conditions-within-that-percentile",
    "href": "posts/discecting_racial_bias_algo/index.html#plotting-risk-score-percentiles-against-mean-number-of-active-chronic-conditions-within-that-percentile",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Plotting risk score percentiles against mean number of active chronic conditions within that percentile",
    "text": "Plotting risk score percentiles against mean number of active chronic conditions within that percentile\nOur code below shows us a plot explaining how the algorithm’s risk score percentile (y-axis) increases as the average number of chronic illnesses (x-axis) goes up, with points colored by race. Generally, patients with more chronic illnesses receive a higher risk score.\nHowever, if a Black patient and a White patient both have the same chronic illnesses, the Black patient often ends up with a lower risk score than the White patient. Because the care management program looks for patients with high risk scores, the Black patient with the same health conditions is less likely to be flagged for extra care—this is the key concern the Obermeyer et al. (2019) highlights.\n\nimport numpy as np\nimport seaborn as sns\n\ndf['risk_percentile'] = df['risk_score_t'].rank(pct=True) * 100\ndf['risk_p_bin'] = df['risk_percentile'].round() # Rounding it to the nearest integer\n\n# Group by race & risk percentile; compute mean chronic illnesses\ngrouped = (\n    df.groupby(['race', 'risk_p_bin'], as_index=False)\n      .agg(mean_chronic=('gagne_sum_t', 'mean'))\n)\n\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(10, 7))\n\nsns.scatterplot(\n    data=grouped,\n    x='mean_chronic',     \n    y='risk_p_bin',       \n    hue='race',\n    alpha=0.8\n)\n\n\nplt.xlabel(\"Mean Number of Chronic Illnesses\")\nplt.ylabel(\"Risk Score Percentile (binned)\")\nplt.title(\"Risk Score Percentile vs. Mean Chronic Illnesses by Race\")\nplt.legend(title=\"Race\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#visualizing-the-relationship-between-risk-score-chronic-illness-and-healthcare-costs",
    "href": "posts/discecting_racial_bias_algo/index.html#visualizing-the-relationship-between-risk-score-chronic-illness-and-healthcare-costs",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Visualizing the Relationship Between Risk Score, Chronic Illness, and Healthcare Costs",
    "text": "Visualizing the Relationship Between Risk Score, Chronic Illness, and Healthcare Costs\nSomething we notice in both our graphs is that White patients (displayed in orange) tend to generate higher medical costs than Black patients (displayed as blue dots) at similar risk score percentiles or similar numbers of chronic illnesses. This finding aligns with the paper’s discussion of a “wedge” between needing care and receiving (or using) care: even when Black and White patients have the same burden of chronic illnesses, White patients often end up with higher total expenditures.\nIt’s also notable that the vast majority of patients have relatively few chronic conditions . Only a small subset of patients have 10 or more illnesses, yet these high-illness groups appear to have an influence on total costs. This illustrates how a minority of patients with many chronic conditions can account for a disproportionately large share of healthcare spending.\n\n# Group for percentile risk score\ngrouped_risk = (\n    df.groupby(['race', 'risk_p_bin'], as_index=False)\n      .agg(mean_cost=('cost_t', 'mean'))\n)\n# Group for numbe rof chronic illness\ngrouped_chronic = (\n    df.groupby(['race', 'gagne_sum_t'], as_index=False)\n      .agg(mean_cost=('cost_t', 'mean'))\n)\n\n# fitting the  subplots, side by side\nfig, axes = plt.subplots(ncols=2, figsize=(12, 5))\n\n# plotting the percentile risk score\nsns.scatterplot(\n    data=grouped_risk,\n    x='risk_p_bin',\n    y='mean_cost',\n    hue='race',\n    alpha=0.8,\n    ax=axes[0]\n)\naxes[0].set_title(\"Mean Expenditure vs. Risk Score Percentile\")\naxes[0].set_xlabel(\"Percentile Risk Score\")\naxes[0].set_ylabel(\"Total Medical Expenditure\")\naxes[0].set_xticks([0, 20, 40, 60, 80, 100])  # Custom x-ticks\n\n# using a log scale for the y-axis\naxes[0].set_yscale('log')\n\n# plotting the number of chronic illnesses\nsns.scatterplot(\n    data=grouped_chronic,\n    x='gagne_sum_t',\n    y='mean_cost',\n    hue='race',\n    alpha=0.8,\n    ax=axes[1]\n)\naxes[1].set_title(\"Mean Expenditure vs. Number of Chronic Illnesses\")\naxes[1].set_xlabel(\"Number of Chronic Illnesses\")\naxes[1].set_ylabel(\"Total Medical Expenditure\")\n\n\n# Custom x-ticks\naxes[1].set_xticks([0, 5, 10, 15])\n\n# using a log scale for the y-axis\naxes[1].set_yscale('log')\n\naxes[1].legend(title=\"Race\", loc=\"upper left\", bbox_to_anchor=(1, 1))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIn our code below we calculate percentage of patients with 5 or fewer chronic conditions and see that about 90% of patients have five or fewer chronic conditions, so by focusing on this group, you’re still covering the vast majority of the dataset. That makes it a reasonable choice: it simplifies the analysis while still capturing most patients’ experiences.\n\n# 2. Remove patients with $0 medical costs because log(0) is undefined\ndf = df[df['cost_t'] &gt; 0]\n\n# Determine the percentage of patients with 5 or fewer chronic conditions\ntotal_patients = df.shape[0]\npatients_5_or_fewer = df[df['gagne_sum_t'] &lt;= 5].shape[0]\npercentage_5_or_fewer = (patients_5_or_fewer / total_patients) * 100\nprint(\"Percentage of patients with 5 or fewer chronic conditions: \", (percentage_5_or_fewer))\n\nPercentage of patients with 5 or fewer chronic conditions:  89.79413053000438"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#data-preparation-before-modeling",
    "href": "posts/discecting_racial_bias_algo/index.html#data-preparation-before-modeling",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Data preparation before modeling",
    "text": "Data preparation before modeling\n\n# 2. Remove patients with $0 medical costs because log(0) is undefined\ndf = df[df['cost_t'] &gt; 0]\n\n# Determine the percentage of patients with 5 or fewer chronic conditions\ntotal_patients = df.shape[0]\npatients_5_or_fewer = df[df['gagne_sum_t'] &lt;= 5].shape[0]\npercentage_5_or_fewer = (patients_5_or_fewer / total_patients) * 100\nprint(\"Percentage of patients with 5 or fewer chronic conditions: \", (percentage_5_or_fewer))\n\nPercentage of patients with 5 or fewer chronic conditions:  89.79413053000438\n\n\nWe see that about 90% of patients have five or fewer chronic conditions, so by focusing on this group, you’re still covering the vast majority of the dataset. That makes it a reasonable choice: it simplifies the analysis while still capturing most patients’ experiences."
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#modeling-our-data",
    "href": "posts/discecting_racial_bias_algo/index.html#modeling-our-data",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Modeling our Data",
    "text": "Modeling our Data\nIn our code belwo we perform three main steps to prepare our data for modeling:\n\nLog-Transforming Costs: We create a new column log_cost by taking the natural log of cost_t. This transformation helps manage the large range of healthcare expenses, making it easier for a regression model to handle.\nOne-Hot Encoding Race: We introduce a race_dummy variable where Black patients are assigned a value of 1 and White patients a value of 0. Turning the categorical race variable into a numeric format allows the model to incorporate race as a predictor.\nDefining Predictors and Target: Our predictor variables (X) are the dummy-coded race variable (race_dummy) and the total number of chronic conditions (gagne_sum_t). Our target variable (y) is the log-transformed cost (log_cost).\n\n\n#log-transform of the cost\ndf['log_cost'] = np.log(df['cost_t'])\n\n# one-hot encoding race\ndf['race_dummy'] = (df['race'] == 'black').astype(int)\n\n#Separate the data into predictor variables (X) and the target variable (y)\n#   For predictors, we use the onehot encoded race variavle and the number of chronic conditions.\nX = df[['race_dummy', 'gagne_sum_t']]\ny = df['log_cost']\n\nprint(X['race_dummy'].value_counts())\n\nprint(\"Predictor variables x\")\n\nprint(X.head()) \n\nprint(\"Target variable y\")\n\nprint(y.head())\n\nrace_dummy\n0    5851\n1     998\nName: count, dtype: int64\nPredictor variables x\n    race_dummy  gagne_sum_t\n1            0            3\n8            1            1\n15           0            1\n19           0            0\n21           0            2\nTarget variable y\n1     7.863267\n8     6.907755\n15    7.313220\n19    8.594154\n21    8.318742\nName: log_cost, dtype: float64"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#modeling-our-cost-disparity",
    "href": "posts/discecting_racial_bias_algo/index.html#modeling-our-cost-disparity",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Modeling our Cost Disparity",
    "text": "Modeling our Cost Disparity\nIn our code below we built our model to assess the cost disparity. So we do this by building a polynomial regression model to explore the relationship between the number of chronic illnesses, race, and healthcare costs. In order to this we had to do the following steps\n\nTransforming the Data: We applied a log transformation to the cost variable to handle its wide range of values. In addition to that We one-hot encoded race, where Black patients were assigned 1 and White patients were assigned 0.\nGenerating Polynomial Features: Since the relationship between chronic conditions and cost was not linear, we created polynomial features up to degree 11 to capture nonlinearity.\nHyperparameter Tuning: We tested different polynomial degrees (1 to 11) and regularization strengths (Ridge regression with \\(\\alpha = 10^k\\) for \\(k = -4, -3, \\dots, 3, 4\\)). Using cross-validation, we identified the combination that minimized the mean squared error (MSE).\nFitting the Final Model: Once we found the best degree (9) and regularization strength (\\(\\alpha = 10\\)), we trained our final Ridge regression model. We extracted the coefficient for race (\\(w_b\\)), which indicates the impact of being Black on predicted log-cost. We computed \\(e^{w_b}\\), which tells us the relative cost of Black patients compared to White patients with similar illness burdens.\n\n\nimport warnings\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.linear_model import Ridge\n\ndef add_polynomial_features(X, degree):\n    \"\"\"\n    Adds polynomial terms for 'gagne_sum_t' up to the specified degree.\n    \"\"\"\n    X_ = X.copy()\n\n    for j in range(1, degree + 1):\n        X_[f\"poly_{j}\"] = X_[\"gagne_sum_t\"] ** j\n    return X_\n\n# create a hyperparameter grid for the polynomial degree\ndegrees = range(1, 12)               # 1 through 11\nalphas = [10**k for k in range(-4, 5)]  # 10^-4 through 10^4\n\n# Initialize the best score to positive infinity and the best parameters to None\n# cross-validation will update these variables\nbest_score = np.inf\nbest_params = (None, None)  # (degree, alpha)\n\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n\n    for deg in degrees:\n        # Create polynomial features up to 'deg'\n        X_poly = add_polynomial_features(X[['gagne_sum_t']], deg)\n        # we include the race dummy column as well\n        X_poly['race_dummy'] = X['race_dummy']\n        \n        for alpha in alphas:\n            # creating a Ridge model with the given alpha\n            model = Ridge(alpha=alpha)\n            \n            # 5-fold cross-validation using negative MSE (sklearn uses negative MSE by default)\n            scores = cross_val_score(model, X_poly, y, cv=5, scoring='neg_mean_squared_error')\n            mean_mse = -np.mean(scores)  # convert negative MSE to MSE\n\n            # Update if we find a better (lower) MSE\n            if mean_mse &lt; best_score:\n                best_score = mean_mse\n                best_params = (deg, alpha)\n\nprint(\"Best degree and alpha:\", best_params)\nprint(\"Cross Validated MSE:\", best_score)\n\n# final model\nbest_degree = best_params[0]\nbest_alpha = best_params[1]\n\n# Build the final model using the polynomial degree and alpha that gave the best MSE\nX_final = add_polynomial_features(X[['gagne_sum_t']], best_degree)\nX_final['race_dummy'] = X['race_dummy']\n\n# Fit the final model\nfinal_model = Ridge(alpha=best_alpha)\nfinal_model.fit(X_final, y)\n\ncoef_names = list(X_final.columns)\ncoefs = final_model.coef_\n\nrace_index = coef_names.index(\"race_dummy\")\nrace_coef = coefs[race_index]\n\n# Compute e^(wb)\nrace_factor = np.exp(race_coef)\n\nprint(\"Race factor (e^(wb)):\", race_factor)\n\nBest degree and alpha: (9, 10)\nCross Validated MSE: 1.206333568523416\nRace factor (e^(wb)): 0.8095004382057049\n\n\n/Users/prashanthbabu/anaconda3/envs/ml-0451/lib/python3.11/site-packages/sklearn/linear_model/_ridge.py:215: LinAlgWarning: Ill-conditioned matrix (rcond=2.53171e-22): result may not be accurate.\n  return linalg.solve(A, Xy, assume_a=\"pos\", overwrite_a=True).T"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#interpretting-our-results",
    "href": "posts/discecting_racial_bias_algo/index.html#interpretting-our-results",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Interpretting our results:",
    "text": "Interpretting our results:\nThe best polynomial degree was 9, suggesting a complex, nonlinear relationship between chronic conditions and cost. The regularization strength \\(\\alpha = 10\\) helped control overfitting while preserving useful patterns.\nThe race coefficient (\\(w_b\\)) resulted in a computed value of \\(e^{w_b} \\approx 0.81\\), meaning Black patients incur only \\(\\sim 81\\%\\) of the healthcare costs of equally sick White patients. This result aligns with the argument in Obermeyer et al. (2019)—that healthcare cost data underestimates Black patients’ need for care. Since cost data is used to determine who gets extra healthcare resources, this disparity can lead to biased decision-making, where Black patients may be less likely to be enrolled in high-risk management programs despite having the same number of chronic conditions as White patients.\nIn summary, our model supports the claim that cost-based risk scores do not fully capture healthcare need, and the bias disproportionately affects Black patients."
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#abstract",
    "href": "posts/discecting_racial_bias_algo/index.html#abstract",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "",
    "text": "In this project, I aim to replicate and extend the findings of Obermeyer et al. (2019) by exploring how healthcare cost predictions relate to patients’ chronic illness burden and race. Using a randomized dataset, we first visualize the relationship between risk score percentiles, chronic conditions, and medical expenditures, revealing that White patients tend to generate higher costs than Black patients despite similar illness burdens. We then built a Ridge regression model with polynomial features to quantify the disparity in costs between Black and White patients. Our final model estimates that, holding illness burden constant, Black patients incur roughly 81% of the costs incurred by White patients, suggesting that the cost-based risk score underestimates the true care needs of Black patients."
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#exploring-the-data",
    "href": "posts/discecting_racial_bias_algo/index.html#exploring-the-data",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Exploring the Data",
    "text": "Exploring the Data\nBelow we will accesss the data clean it and explore the different variables and features.\n\nimport pandas as pd\nurl = \"https://gitlab.com/labsysmed/dissecting-bias/-/raw/master/data/data_new.csv?inline=false\"\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n\n\n\n\n\nrisk_score_t\nprogram_enrolled_t\ncost_t\ncost_avoidable_t\nbps_mean_t\nghba1c_mean_t\nhct_mean_t\ncre_mean_t\nldl_mean_t\nrace\n...\ntrig_min-high_tm1\ntrig_min-normal_tm1\ntrig_mean-low_tm1\ntrig_mean-high_tm1\ntrig_mean-normal_tm1\ntrig_max-low_tm1\ntrig_max-high_tm1\ntrig_max-normal_tm1\ngagne_sum_tm1\ngagne_sum_t\n\n\n\n\n0\n1.987430\n0\n1200.0\n0.0\nNaN\n5.4\nNaN\n1.110000\n194.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n7.677934\n0\n2600.0\n0.0\n119.0\n5.5\n40.4\n0.860000\n93.0\nwhite\n...\n0\n1\n0\n0\n1\n0\n0\n1\n4\n3\n\n\n2\n0.407678\n0\n500.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n0.798369\n0\n1300.0\n0.0\n117.0\nNaN\nNaN\nNaN\nNaN\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n17.513165\n0\n1100.0\n0.0\n116.0\nNaN\n34.1\n1.303333\n53.0\nwhite\n...\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n\n\n\n\n5 rows × 160 columns\n\n\n\nOur code below checks the ratio of white patients to black patiens before dropping NA values and after dorpping them. There is clearly a big difference in the ratio of black to white people in our dataset, which is something to keep in mind.\n\nrace_counts_bf_clean = df['race'].value_counts()\ndf.dropna(inplace=True)\nrace_counts = df['race'].value_counts()\nprint(\"Count of patients by race before cleaning data\",race_counts_bf_clean)\nprint(\"Count of patients by race after cleaning data\", race_counts)\n\nCount of patients by race before cleaning data race\nwhite    43202\nblack     5582\nName: count, dtype: int64\nCount of patients by race after cleaning data race\nwhite    5911\nblack    1000\nName: count, dtype: int64\n\n\n\nimport  matplotlib.pyplot as plt\nimport seaborn as sns\n# Graph 1: Bar chart of race counts\nplt.figure(figsize=(6, 4))\nplt.bar(race_counts.index, race_counts.values, color=['blue', 'orange'])\nplt.xlabel('Race')\nplt.ylabel('Number of Patients')\nplt.title('Number of Black vs. White Patients')\nplt.show()\n\n\n\n\n\n\n\n\nBelow is a scatter plot to visualize the Risk Score vs the Cost by race and see a lot of clustering. So we will need to further clean our data to get better visualizations.\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x=\"risk_score_t\", y=\"cost_t\", hue=\"race\", alpha=0.7)\n\n# Labels and title\nplt.xlabel(\"Risk Score\")\nplt.ylabel(\"Cost\")\nplt.title(\"Risk Score vs Cost by Race\")\nplt.show()"
  },
  {
    "objectID": "posts/discecting_racial_bias_algo/index.html#discussion",
    "href": "posts/discecting_racial_bias_algo/index.html#discussion",
    "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
    "section": "Discussion",
    "text": "Discussion\nOur modeling process used polynomial feature expansion and regularized linear regression which allowed us to capture the nonlinear relationship between chronic conditions and cost. The final model’s race coefficient, suggests that Black patients’ predicted expenditures are about 81% of those of White patients with the same number of chronic conditions. This finding supports the “wedge” hypothesis discussed by Obermeyer et al. (2019): even when Black and White patients are equally sick, Black patients tend to have lower healthcare costs, likely due to systemic inequities in access and utilization of care. In relation to the formal statistical discrimination criteria from Barocas, Hardt, and Narayanan (2023), the bias in this algorithm is best described by a failure of the separation criterion. In an ideal scenario, the classifier (or risk score) should yield equal error rates—that is, for any given risk score, the probability of high health need should be equal across racial groups. Our analysis shows that this is not the case: Black patients, at a given risk score, exhibit a higher burden of chronic illness, indicating that the algorithm’s misclassification rates differ by race. This violation of separation shows how using cost as a proxy for health can introduce systematic bias, ultimately disadvantaging Black patients. Overall, through this project I learned how to build a Ridge regression model using polynomial features, as well as how to tune hyperparameters effectively. This process revealed how these modeling techniques can uncover hidden biases in decision-making algorithms, underscoring the need for more equitable measures when predicting healthcare needs."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#abstract-and-methodologies",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#abstract-and-methodologies",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "",
    "text": "Automated decision systems are increasingly used in financial institutions to assess credit risk and determine loan eligibility. In this blog post, we build upon the theoretical framework of binary decision-making with a linear score function, applying it to a more realistic credit-risk prediction scenario. Our goal is twofold: first, to develop a score function and threshold that optimize a bank’s total expected profit while considering various borrower features; and second, to assess how this decision system impacts different demographic segments. By leveraging data-driven modeling, visualization, and profit-based optimization, we aim to create a more informed and equitable approach to automated lending decisions."
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#loading-the-data",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#loading-the-data",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Loading the data",
    "text": "Loading the data\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf_train = pd.read_csv(url)\n\n\ndf_train.head()\n\n\n\n\n\n\n\n\nperson_age\nperson_income\nperson_home_ownership\nperson_emp_length\nloan_intent\nloan_grade\nloan_amnt\nloan_int_rate\nloan_status\nloan_percent_income\ncb_person_default_on_file\ncb_person_cred_hist_length\n\n\n\n\n0\n25\n43200\nRENT\nNaN\nVENTURE\nB\n1200\n9.91\n0\n0.03\nN\n4\n\n\n1\n27\n98000\nRENT\n3.0\nEDUCATION\nC\n11750\n13.47\n0\n0.12\nY\n6\n\n\n2\n22\n36996\nRENT\n5.0\nEDUCATION\nA\n10000\n7.51\n0\n0.27\nN\n4\n\n\n3\n24\n26000\nRENT\n2.0\nMEDICAL\nC\n1325\n12.87\n1\n0.05\nN\n4\n\n\n4\n29\n53004\nMORTGAGE\n2.0\nHOMEIMPROVEMENT\nA\n15000\n9.63\n0\n0.28\nN\n10"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-intent-by-age-group",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-intent-by-age-group",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Intent by Age Group",
    "text": "Analysis of Loan Intent by Age Group\nThis bar chart below shows how different age groups use their loans for various purposes—such as venture, education, medical, home improvement, personal, and debt consolidation. We can see that borrowers aged 18–29 make up a large portion of total loans, often driven by education and personal loan needs. As age increases, the number of loans generally decreases, but certain categories—like debt consolidation—can become more common in older groups.\nOverall, this chart highlights that younger borrowers borrow a lot more money may be more focused on educational or personal financing, while older borrowers might shift their attention to consolidating debt or improving their homes. Understanding these patterns help us in understanding different patternns in who would default on a loan and not.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\nurl = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/credit-risk/train.csv\"\ndf = pd.read_csv(url)\n\n# Create additional features for analysis:\n# 1. Age groups: we create bins to see how loan intent varies with age.\nage_bins = [18, 30, 40, 50, 60, 100]\nage_labels = ['18-29', '30-39', '40-49', '50-59', '60+']\ndf['age_group'] = pd.cut(df['person_age'], bins=age_bins, labels=age_labels)\n\n# ---------------------------\n# Visualization 1:\n# How does loan intent vary with age and home ownership status?\nplt.figure(figsize=(10, 6))\nsns.countplot(data=df, x='age_group', hue='loan_intent')\nplt.title('Loan Intent Distribution by Age Group')\nplt.xlabel('Age Group')\nplt.ylabel('Count')\nplt.legend(title='Loan Intent')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-average-loan-amount-by-credit-history-length",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Average Loan Amount by Credit History Length",
    "text": "Analysis of Average Loan Amount by Credit History Length\nThis bar chart shows how the average loan amount changes based on the number of years a borrower has had a credit history. In general, we see that some longer lengths of credit history are associated with higher average loan amounts than others, though the pattern isn’t strictly increasing or decreasing. This suggests that lenders may be willing to extend larger lines of credit to individuals with certain credit history profiles.\nFor our automated decision system, credit history length could be an important feature because it often reflects a borrower’s past experience with credit and repayment behavior. However, we must be mindful of fairness and potential biases: borrowers who are younger or newer to credit might be at a disadvantage if the model heavily weighs credit history length. Balancing profitability for the bank with equitable access to credit remains a key challenge in designing our scoring and thresholding methods.\n\n\n# 2. Employment length groups: useful for exploring patterns with job experience.\nemp_bins = [0, 2, 5, 10, df['person_emp_length'].max()]\nemp_labels = ['0-1 yrs', '2-4 yrs', '5-9 yrs', '10+ yrs']\ndf['emp_length_group'] = pd.cut(df['person_emp_length'], bins=emp_bins, labels=emp_labels)\n\n# ---------------------------\n# Visualization 2:\n# Which segments are offered different interest rates? Compare distributions by home ownership.\nplt.figure(figsize=(10, 6))\navg_loan = df.groupby('cb_person_cred_hist_length')['loan_amnt'].mean().reset_index()\nsns.barplot(data=avg_loan, x='cb_person_cred_hist_length', y='loan_amnt')\nplt.title('Average Loan Amount by Credit History Length')\nplt.xlabel('Credit History Length (Years)')\nplt.ylabel('Average Loan Amount')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership\nIn our scatter plot below, each dot represents a borrower, with the x-axis showing how large the loan is relative to their income (as a percentage) and the y-axis showing the absolute loan amount. The colors indicate different types of home ownership (RENT, MORTGAGE, OWN, OTHER).\nAs you can see, the data points overlap heavily, making the chart look cluttered. To get a clearer picture, I wo;; split these data into separate graphs for each home ownership category. This will help us see more nuanced patterns—like whether renters tend to have higher loan-to-income ratios compared to those who own or have a mortgage.\n\ndf['loan_percent_income_pct'] = df['loan_percent_income'] * 100\nplt.figure(figsize=(10, 6))\nsns.scatterplot(\n    data=df, \n    x='loan_percent_income_pct', \n    y='loan_amnt', \n    hue='person_home_ownership',\n    alpha=0.7\n)\nplt.title('Loan Amount vs. Loan % of Income by Home Ownership')\nplt.xlabel('Loan as % of Person Income')\nplt.ylabel('Loan Amount')\nplt.xlim(0, 100)  # Focus on 0–100% if most loans fall in this range\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#analysis-of-loan-amount-vs.-loan-of-income-by-home-ownership-separated-plots",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)",
    "text": "Analysis of Loan Amount vs. Loan % of Income by Home Ownership (Separated Plots)\nBy splitting the data into four subplots (one for each home ownership category), we can see that:\n\nRenters typically have loan amounts averaging around $10,000–$12,000, with a wide spread of loan-to-income ratios (averaging around 15–20%). In addition to this we see the most clustering for this group which means, in our dataset most of the people taking our loans are from this group.\nMortgage holders often take out larger loans (averaging $16,000–$18,000) but may have lower loan-to-income ratios (closer to 10% on average).\nOwners (those who fully own their homes) tend to borrow moderate amounts ($12,000–$15,000) at ratios of around 12–15%.\nOthers (less common categories) show a broad mix but generally fall between these ranges.\n\nThese distinctions are important for our automated decision system, since each home ownership group presents a different risk and borrowing profile. When designing a score function and threshold to maximize the bank’s profit, it’s important for us to consider whether certain groups (like renters) might be unfairly penalized if they tend to have higher loan-to-income ratios. Ultimately, these separate plots help us fine-tune our model so that we balance profitability with equitable access to credit across different segments of borrowers.\n\n\n\n# Create a FacetGrid: one subplot per home ownership category\ng = sns.FacetGrid(df, col=\"person_home_ownership\", col_wrap=2, height=4)\ng.map(sns.scatterplot, \"loan_percent_income_pct\", \"loan_amnt\", alpha=0.7)\n\n# Set the x-axis limits and labels for clarity\ng.set(xlim=(0, 100))\ng.set_axis_labels(\"Loan as % of Income\", \"Loan Amount\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#summary-of-loan-intent-and-home-ownership-segments",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Summary of Loan Intent and Home Ownership Segments",
    "text": "Summary of Loan Intent and Home Ownership Segments\nThis table shows how different combinations of loan intent (e.g., EDUCATION, MEDICAL, PERSONAL) and home ownership (MORTGAGE, OWN, RENT, OTHER) compare in terms of average interest rate, average loan amount, and count of borrowers. We’ve sorted the table by average loan amount in descending order to identify which segments receive the largest lines of credit.\n\nHighest Averages: Segments like PERSONAL–OTHER and MEDICAL–OTHER appear near the top, suggesting they receive higher loan amounts (over $12,000 on average), but also tend to have higher interest rates (11–12%).\nMortgage vs. Rent: Many MORTGAGE segments (e.g., DEBTCONSOLIDATION–MORTGAGE, EDUCATION–MORTGAGE) cluster in the middle, with average loan amounts around $10,000–$11,000 and interest rates near 10–10.6%. Renters often see slightly higher interest rates (11–12%) and somewhat lower loan amounts (around $8,000–$9,000).\nLow Counts: Some segments have very few borrowers (like DEBTCONSOLIDATION–OWN with a count of only 62), which may not be reliable for broad conclusions.\n\nFrom the perspective of building an automated decision system, these patterns hint at where the bank’s profit opportunities and risks might lie. For instance, segments with higher average loan amounts but also higher interest rates could be more profitable—but might also carry greater default risk. Tracking how many borrowers fall into each segment (the “count” column) helps ensure the model doesn’t overly focus on small, potentially unrepresentative groups.\n\nsummary_table = (\n    df\n    .groupby(['loan_intent', 'person_home_ownership'], as_index=False)\n    .agg(\n        avg_interest_rate=('loan_int_rate', 'mean'),\n        avg_loan_amount=('loan_amnt', 'mean'),\n        count=('loan_amnt', 'count')  # how many borrowers in each segment\n    )\n)\n\n# Sort by average loan amount (descending) to see which segments get the largest lines of credit\nsummary_table_sorted_by_amount = summary_table.sort_values('avg_loan_amount', ascending=False)\n\nsummary_table_sorted_by_amount\n\n\n\n\n\n\n\n\n\nloan_intent\nperson_home_ownership\navg_interest_rate\navg_loan_amount\ncount\n\n\n\n\n17\nPERSONAL\nOTHER\n11.675714\n12366.666667\n15\n\n\n13\nMEDICAL\nOTHER\n12.745000\n12200.000000\n13\n\n\n5\nEDUCATION\nOTHER\n12.400833\n12142.857143\n14\n\n\n9\nHOMEIMPROVEMENT\nOTHER\n11.683000\n10959.090909\n11\n\n\n8\nHOMEIMPROVEMENT\nMORTGAGE\n10.613916\n10764.017341\n1384\n\n\n20\nVENTURE\nMORTGAGE\n10.468000\n10606.281060\n1811\n\n\n0\nDEBTCONSOLIDATION\nMORTGAGE\n10.400489\n10588.756111\n1841\n\n\n4\nEDUCATION\nMORTGAGE\n10.554563\n10502.178076\n2089\n\n\n12\nMEDICAL\nMORTGAGE\n10.505553\n10485.867052\n1730\n\n\n16\nPERSONAL\nMORTGAGE\n10.426285\n10481.223233\n1868\n\n\n21\nVENTURE\nOTHER\n12.274211\n10367.500000\n20\n\n\n11\nHOMEIMPROVEMENT\nRENT\n11.812287\n10109.604633\n1252\n\n\n1\nDEBTCONSOLIDATION\nOTHER\n11.566667\n9783.333333\n15\n\n\n14\nMEDICAL\nOWN\n10.749097\n9367.755682\n352\n\n\n10\nHOMEIMPROVEMENT\nOWN\n10.922405\n9242.450980\n255\n\n\n6\nEDUCATION\nOWN\n10.797507\n8996.177184\n412\n\n\n18\nPERSONAL\nOWN\n10.867262\n8944.209040\n354\n\n\n3\nDEBTCONSOLIDATION\nRENT\n11.358223\n8882.754425\n2260\n\n\n19\nPERSONAL\nRENT\n11.535415\n8826.900046\n2171\n\n\n23\nVENTURE\nRENT\n11.438455\n8804.566745\n2135\n\n\n22\nVENTURE\nOWN\n10.560000\n8789.621914\n648\n\n\n7\nEDUCATION\nRENT\n11.315216\n8685.308193\n2612\n\n\n15\nMEDICAL\nRENT\n11.422580\n8426.925182\n2740\n\n\n2\nDEBTCONSOLIDATION\nOWN\n14.432909\n7749.193548\n62"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#training-and-evaluating-our-logistic-regression-model",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#training-and-evaluating-our-logistic-regression-model",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Training and evaluating our Logistic Regression model",
    "text": "Training and evaluating our Logistic Regression model\nI used a logistic regression model to predict whether a prospective borrower will default on a loan. After preprocessing the data—by standardizing numerical features and one-hot encoding categorical variables. I removed rows with missing values, and split the dataset into training and test sets. The model achieved a test accuracy of about 84.2%.\nThe confusion matrix provides additional insight into the model’s performance:\n\nTrue Negatives (TN): 3,393 borrowers who did not default and were correctly predicted as non-default.\nFalse Positives (FP): 221 borrowers who did not default but were incorrectly flagged as defaults.\nFalse Negatives (FN): 501 borrowers who defaulted but were missed by the model.\nTrue Positives (TP): 467 borrowers who defaulted and were correctly identified.\n\nOur results suggest that while the model performs reasonably well overall, there is still a balance to be struck between avoiding false positives and false negatives. This is particularly important when designing an automated decision system for credit risk, because both profitability for the bank and equitable access to credit are critical. Further tuning of the threshold and exploration of additional features could help optimize the model even further for its intended purpose\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Drop rows with missing values\ndf_train = df_train.dropna(subset=numeric_features + categorical_features)\n\n\n\n\ntarget = 'loan_status'\nX = df_train.drop(columns=[target])\ny = df_train[target]\n\nnumeric_features = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', \n                      'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']\ncategorical_features = ['person_home_ownership', 'loan_intent', 'cb_person_default_on_file']\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numeric_features),\n        ('cat', OneHotEncoder(drop='first'), categorical_features)\n    ])\nX_transformed = preprocessor.fit_transform(X)\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_transformed, y, test_size=0.2, random_state=123\n)\n\n# Fit a logistic regression model using the preprocessed features\nmodel = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\nprint(\"Test Accuracy:\", accuracy)\nprint(\"Confusion Matrix:\\n\", conf_matrix)\n\nTest Accuracy: 0.8424268878219118\nConfusion Matrix:\n [[3393  221]\n [ 501  467]]"
  },
  {
    "objectID": "posts/Design and Impact of Automated Decision Systems/index.html#finding-weight-and-threshold",
    "href": "posts/Design and Impact of Automated Decision Systems/index.html#finding-weight-and-threshold",
    "title": "Design and Impact of Automated Decision Systems",
    "section": "Finding weight and threshold",
    "text": "Finding weight and threshold\n\ndf['loan_int_rate_decimal'] = df['loan_int_rate'] / 100.0\n\ndef profit_if_repaid(loan_amnt, loan_int_rate_decimal):\n    \"\"\"\n    If the loan is repaid in full, the bank's profit is:\n      loan_amnt * (1 + 0.25*loan_int_rate)^10 - loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 10 - loan_amnt\n\ndef profit_if_default(loan_amnt, loan_int_rate_decimal):\n    \"\"\"\n    If the borrower defaults, we assume default happens 3 years into the loan, \n    and the bank loses 70% of the principal:\n      loan_amnt*(1 + 0.25*loan_int_rate)^3 - 1.7*loan_amnt\n    \"\"\"\n    return loan_amnt * (1 + 0.25 * loan_int_rate) ** 3 - 1.7 * loan_amnt\n\nWe will now compute predicted probabilities (probability of default) for the training data\n\ny_prob_train = model.predict_proba(X_train)[:, 1]  # column 1 = probability of default\n\n\n\n# Split the transformed data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=123\n)\nloan_amnt_array = X_train['loan_amnt'].to_numpy()\nloan_int_rate_array = X_train['loan_int_rate_decimal'].to_numpy()\ny_train_array = y_train.to_numpy()\nprofit_repaid = profit_if_repaid(loan_amnt_array, loan_int_rate_array)\nprofit_default = profit_if_default(loan_amnt_array, loan_int_rate_array)\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)\n   3804 try:\n-&gt; 3805     return self._engine.get_loc(casted_key)\n   3806 except KeyError as err:\n\nFile index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n\nFile index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nFile pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n\nKeyError: 'loan_int_rate_decimal'\n\nThe above exception was the direct cause of the following exception:\n\nKeyError                                  Traceback (most recent call last)\n/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb Cell 22 line 6\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=1'&gt;2&lt;/a&gt; X_train, X_test, y_train, y_test = train_test_split(\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=2'&gt;3&lt;/a&gt;     X, y, test_size=0.2, random_state=123\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=3'&gt;4&lt;/a&gt; )\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=4'&gt;5&lt;/a&gt; loan_amnt_array = X_train['loan_amnt'].to_numpy()\n----&gt; &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=5'&gt;6&lt;/a&gt; loan_int_rate_array = X_train['loan_int_rate_decimal'].to_numpy()\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=6'&gt;7&lt;/a&gt; y_train_array = y_train.to_numpy()\n      &lt;a href='vscode-notebook-cell:/Users/prashanthbabu/Desktop/machine_learning/csci-0451/posts/new-new-test-post/index.ipynb#X42sZmlsZQ%3D%3D?line=7'&gt;8&lt;/a&gt; profit_repaid = profit_if_repaid(loan_amnt_array, loan_int_rate_array)\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/frame.py:4102, in DataFrame.__getitem__(self, key)\n   4100 if self.columns.nlevels &gt; 1:\n   4101     return self._getitem_multilevel(key)\n-&gt; 4102 indexer = self.columns.get_loc(key)\n   4103 if is_integer(indexer):\n   4104     indexer = [indexer]\n\nFile ~/anaconda3/envs/ml-0451/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(self, key)\n   3807     if isinstance(casted_key, slice) or (\n   3808         isinstance(casted_key, abc.Iterable)\n   3809         and any(isinstance(x, slice) for x in casted_key)\n   3810     ):\n   3811         raise InvalidIndexError(key)\n-&gt; 3812     raise KeyError(key) from err\n   3813 except TypeError:\n   3814     # If we have a listlike key, _check_indexing_error will raise\n   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n   3816     #  the TypeError.\n   3817     self._check_indexing_error(key)\n\nKeyError: 'loan_int_rate_decimal'\n\n\n\nNow we are going to find a threshold and identify teh best threshold\n\n# 5. Sweep over thresholds to find the one that maximizes average profit\n\nthresholds = np.linspace(0, 1, 101)\navg_profits = []\n\nfor t in thresholds:\n    # Predict default if probability &gt;= t\n    predicted_default = (y_prob_train &gt;= t).astype(int)\n      # If we predict default, we do NOT give the loan =&gt; profit = 0\n    # If we predict no default, we DO give the loan =&gt; actual profit depends on y_train_array\n    #    - If actual y=0 (no default), profit = profit_repaid\n    #    - If actual y=1 (default), profit = profit_default\n    give_loan = 1 - predicted_default  # 1 = give loan, 0 = no loan\n    # total_profit[i] = give_loan[i] * [ (1 - y[i])*profit_repaid[i] + y[i]*profit_default[i] ]\n    total_profit = give_loan * ((1 - y_train_array) * profit_repaid + y_train_array * profit_default)\n    \n    # Compute average profit per borrower\n    avg_profit = total_profit.mean()\n    avg_profits.append(avg_profit)\n\navg_profits = np.array(avg_profits)\n\n# 6. Identify the best threshold\n\nbest_idx = np.argmax(avg_profits) # index of the best threshold\nbest_threshold = thresholds[best_idx]\n\n\nprint(f\"Best Threshold: {best_threshold:.3f}\")\n\nBest Threshold: 1.000\n\n\n\n# 7. Plot profit vs. threshold\n\nplt.figure(figsize=(8, 5))\nplt.plot(thresholds, avg_profits, label='Profit per Borrower')\nplt.scatter(best_threshold, best_profit, color='red', zorder=10, label='Optimal Threshold')\nplt.title('Profit per Borrower (Training Set) vs. Threshold')\nplt.xlabel('Threshold (Probability of Default)')\nplt.ylabel('Average Profit per Borrower')\nplt.legend()\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Auditing Bias/index.html",
    "href": "posts/Auditing Bias/index.html",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "",
    "text": "The ACS PUMS dataset, collected by the U.S. Census Bureau, provides detailed demographic and employment data for thousands of individuals across the United States. In this blog post, we build a machine learning classifier to predict whether an individual is employed using demographic features such as age, education, and marital status. While race is excluded from the predictive features, it is retained as a group label to enable a comprehensive bias audit. By leveraging logistic regression with polynomial feature expansion and tuning model complexity via grid search, our classifier achieved an overall accuracy of approximately 82%. Our analysis not only demonstrates the predictive power of data-driven approaches in employment forecasting but also reveals disparities in error rates and predicted outcomes across racial groups, underscoring the critical need for fairness auditing before deploying such models in commercial or governmental settings."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#abstract",
    "href": "posts/Auditing Bias/index.html#abstract",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "",
    "text": "The ACS PUMS dataset, collected by the U.S. Census Bureau, provides detailed demographic and employment data for thousands of individuals across the United States. In this blog post, we build a machine learning classifier to predict whether an individual is employed using demographic features such as age, education, and marital status. While race is excluded from the predictive features, it is retained as a group label to enable a comprehensive bias audit. By leveraging logistic regression with polynomial feature expansion and tuning model complexity via grid search, our classifier achieved an overall accuracy of approximately 82%. Our analysis not only demonstrates the predictive power of data-driven approaches in employment forecasting but also reveals disparities in error rates and predicted outcomes across racial groups, underscoring the critical need for fairness auditing before deploying such models in commercial or governmental settings."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#goal-of-project",
    "href": "posts/Auditing Bias/index.html#goal-of-project",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Goal of Project",
    "text": "Goal of Project\nIn this project, we are going to build a machine learning classifier that predicts whether an individual is employed using demographic data from the ACS PUMS dataset. We will exclude race from the features used for prediction while retaining it as a group label to later audit the model for racial bias. This approach helps us explore whether the classifier inadvertently displays disparate performance across racial groups. In addition to this, we’ll tune the model’s complexity (using polynomial features with logistic regression) and perform a bias audit, examining overall and subgroup error rates, and statistical parity. Our findings will audit the bias of predictive models when applied to real-world census data."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#loading-data",
    "href": "posts/Auditing Bias/index.html#loading-data",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Loading Data",
    "text": "Loading Data\nUsing the folktables package, we will download the ACS PUMS data, which provides demographic and employment information collected by the U.S. Census. For our project, we select data for Michigan from the 2018 1-Year survey. In our code snippet below we will load the data:\n\nfrom folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"MI\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\nDownloading data for 2018 1-Year person survey for MI...\n\n\n\n\n\n\n\n\n\nRT\nSERIALNO\nDIVISION\nSPORDER\nPUMA\nREGION\nST\nADJINC\nPWGTP\nAGEP\n...\nPWGTP71\nPWGTP72\nPWGTP73\nPWGTP74\nPWGTP75\nPWGTP76\nPWGTP77\nPWGTP78\nPWGTP79\nPWGTP80\n\n\n\n\n0\nP\n2018GQ0000064\n3\n1\n2907\n2\n26\n1013097\n8\n60\n...\n9\n0\n12\n9\n11\n9\n0\n9\n10\n12\n\n\n1\nP\n2018GQ0000154\n3\n1\n1200\n2\n26\n1013097\n92\n20\n...\n92\n91\n93\n95\n93\n173\n91\n15\n172\n172\n\n\n2\nP\n2018GQ0000158\n3\n1\n2903\n2\n26\n1013097\n26\n54\n...\n26\n52\n3\n25\n25\n28\n28\n50\n51\n25\n\n\n3\nP\n2018GQ0000174\n3\n1\n1801\n2\n26\n1013097\n86\n20\n...\n85\n12\n87\n12\n87\n85\n157\n86\n86\n86\n\n\n4\nP\n2018GQ0000212\n3\n1\n2600\n2\n26\n1013097\n99\n33\n...\n98\n96\n98\n95\n174\n175\n96\n95\n179\n97\n\n\n\n\n5 rows × 286 columns"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#feature-selection",
    "href": "posts/Auditing Bias/index.html#feature-selection",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Feature Selection",
    "text": "Feature Selection\nBelow we are going to pick a list of possible feautres that would be used for modeling. We Exclude Target & Group Features: Remove ESR (employment status, which is our target) and RAC1P (race, used later for bias auditing) from the features list. We then subset the data: By focusing on a smaller set of relevant features, we simplify the model and focus on the factors that are most informative for predicting employment.\n\npossible_features = ['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nprint(acs_data[possible_features].head())\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n   AGEP  SCHL  MAR  RELP  DIS  ESP  CIT  MIG  MIL  ANC  NATIVITY  DEAR  DEYE  \\\n0    60  15.0    5    17    1  NaN    1  1.0  4.0    1         1     2     2   \n1    20  19.0    5    17    2  NaN    1  1.0  4.0    2         1     2     2   \n2    54  18.0    3    16    1  NaN    1  1.0  4.0    4         1     2     2   \n3    20  18.0    5    17    2  NaN    1  1.0  4.0    4         1     2     2   \n4    33  18.0    5    16    2  NaN    1  3.0  4.0    2         1     2     2   \n\n   DREM  SEX  RAC1P  ESR  \n0   1.0    1      2  6.0  \n1   2.0    2      1  6.0  \n2   1.0    1      1  6.0  \n3   2.0    1      1  6.0  \n4   2.0    1      1  6.0"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#chosing-our-problem",
    "href": "posts/Auditing Bias/index.html#chosing-our-problem",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Chosing our Problem",
    "text": "Chosing our Problem\nProblem Choice: In this project we will be predicting whether an individual is employed (target variable ESR) using demographic features—such as age, education, marital status, and others—while deliberately excluding race (RAC1P) from the predictors. However, we keep the race information aside as the group label. This allows us to later perform a bias audit by comparing model performance across different racial groups. This setup enables us to build a classifier that does not directly use race during training, while still allowing us to measure if the predictions favor one racial group over another helping us assess the bias.\nThe code below doesteh following steps\n\nDefines the Prediction Task: We use the BasicProblem class to specify which features to use, the target variable, and how to transform the target (converting ESR values into a binary label).\nWe then Extract the Data: The data is converted into a feature matrix, a label vector, and a group vector.\nAfter that we do our Train-Test Split: The data is split into training and test sets so that we can train our model and later evaluate its performance.\n\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# 3. Define the Prediction Task using BasicProblem\n# Here, we aim to predict employment status. The target_transform converts the raw ESR value\n# to a binary label (1 for employed if ESR == 1, else 0).\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',          # Use race as the group label for bias analysis later.\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1)\n)\n\n# Extract the feature matrix, binary labels, and group indicator from the DataFrame.\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\nprint(\"Features shape:\", features.shape)\n# 4. Train-Test Split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\nprint(\"Training set size:\", X_train.shape[0])\nprint(\"Test set size:\", X_test.shape[0])\n\n\nFeatures shape: (99419, 15)\nTraining set size: 79535\nTest set size: 19884"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#descriptive-analysis",
    "href": "posts/Auditing Bias/index.html#descriptive-analysis",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Descriptive Analysis",
    "text": "Descriptive Analysis\nOur code below converts the training data into a DataFrame to calculate some basic statistics. We find that there are 79,535 individuals in total. Overall, about 44.3% of these individuals are employed. When we break the data down by the race group (held in the “group” column), we see that Group 1 has 67,415 individuals, Group 2 has 6,881 individuals, and the remaining groups have much smaller numbers. Looking at employment rates within each group, about 45.5% of Group 1 are employed, while Group 2 has a lower rate at around 34.6%. The other groups show employment proportions between roughly 33% and 49%. This simple analysis helps us understand the size and distribution of our data, which is essential for building a fair and well-informed mode\n\n# 5. Descriptive Analysis\n# Convert training data into a DataFrame for easy descriptive analysis.\n\ndf = pd.DataFrame(X_train, columns=features_to_use)\ndf[\"group\"] = group_train  # This is the race indicator\ndf[\"label\"] = y_train      # 1 if employed, 0 otherwise\n\n# Explore the different statistics\ntotal_individuals = df.shape[0]\npositive_proportion = df[\"label\"].mean()\ngroup_counts = df[\"group\"].value_counts()\ngroup_positive = df.groupby(\"group\")[\"label\"].mean()\n\nprint(\"Total Individuals:\", total_individuals)\nprint(\"Overall Proportion Employed:\", positive_proportion)\nprint(\"Number of Individuals by Group:\\n\", group_counts)\nprint(\"Employment Proportion by Group:\\n\", group_positive)\n\nTotal Individuals: 79535\nOverall Proportion Employed: 0.44297479097252784\nNumber of Individuals by Group:\n group\n1    67415\n2     6881\n6     2061\n9     1922\n8      670\n3      467\n5       95\n7       24\nName: count, dtype: int64\nEmployment Proportion by Group:\n group\n1    0.454825\n2    0.345880\n3    0.419700\n5    0.400000\n6    0.492479\n7    0.458333\n8    0.444776\n9    0.328824\nName: label, dtype: float64\n\n\nWe also the “SEX” attribute and calculate employment rates for each race-sex combination. For example, in Group 1, males are employed at about 45.6% and females at 45.4%, while in Group 2, rates are around 34.4% for males and 34.8% for females. This helps identify any intersectional disparities.\n\n# Intersectional analysis: using an additional sensitive attribute (e.g., SEX)\n# Assuming the original acs_data contains the SEX column:\ndf[\"sex\"] = acs_data[\"SEX\"]\nintersection = df.groupby([\"group\", \"sex\"])[\"label\"].mean().reset_index()\nprint(\"Intersectional Employment Proportions:\\n\", intersection)\n\nIntersectional Employment Proportions:\n     group  sex     label\n0       1    1  0.455874\n1       1    2  0.453794\n2       2    1  0.344051\n3       2    2  0.347688\n4       3    1  0.384615\n5       3    2  0.451220\n6       5    1  0.428571\n7       5    2  0.369565\n8       6    1  0.486829\n9       6    2  0.498069\n10      7    1  0.416667\n11      7    2  0.500000\n12      8    1  0.448753\n13      8    2  0.440129\n14      9    1  0.339112\n15      9    2  0.319319\n\n\nOur Graph below plots the proprtions of employment amongst all the different races. This bar chart shows the proportion of employed individuals broken down by race group and sex. Each pair of bars compares males (blue) and females (orange) within the same race group. By comparing the heights of the bars, we can quickly see people of white have higher employment rates compared to people of black race who contrarily have this graph aslo shows us whether there are notable differences between males and females in each group.\n\n# Map numeric group codes to more descriptive labels\ngroup_map = {\n    1: \"White Alone\",\n    2: \"Black/African American\",\n    3: \"American Indian alone\",\n    5: \"Other Group 5\",\n    6: \"Other Group 6\",\n    7: \"Other Group 7\",\n    8: \"Some Other Race alone\",\n    9: \"Two or More Races\"\n}\n\n# Map numeric sex codes to more descriptive labels\nsex_map = {\n    1: \"Male\",\n    2: \"Female\"\n}\n\n# Apply these mappings to the DataFrame used for plotting\nintersection[\"group_label\"] = intersection[\"group\"].map(group_map)\nintersection[\"sex_label\"] = intersection[\"sex\"].map(sex_map)\n\n# Plot the results using a bar chart\nsns.barplot(data=intersection, x=\"group_label\", y=\"label\", hue=\"sex_label\")\nplt.xlabel(\"Race Group\")\nplt.ylabel(\"Proportion Employed\")\nplt.title(\"Employment Proportion by Race Group and Sex\")\nplt.xticks(rotation=45)  # Rotate x labels if they're too long\nplt.legend(title=\"Sex\")\nplt.show()"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#training-our-model",
    "href": "posts/Auditing Bias/index.html#training-our-model",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Training our model",
    "text": "Training our model\nBelow, we begin training our model to predict whether an individual is employed based on their demographic features (excluding race). We set up a pipeline that performs three main steps:\n\nPolynomialFeatures: Expands the original features to include interaction terms and higher-order terms, allowing the model to capture more complex relationships.\nStandardScaler: Normalizes these expanded features so that no single feature dominates because of its scale.\nLogisticRegression: Fits a logistic regression classifier to predict employment (employed vs. not employed).\n\nWe then use a parameter grid (param_grid) to tune two key aspects:\n\npolynomialfeatures__degree controls how many polynomial and interaction terms we create (from no expansion at degree 1, to quadratic expansion at degree 2).\nlogisticregression__C controls the regularization strength of the logistic regression model, balancing the trade-off between overfitting and underfitting.\n\nThis setup, combined with grid search, systematically tests different combinations of polynomial degrees and regularization strengths, helping us figure out the best model configuration for our prediction task.\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# Create a pipeline with PolynomialFeatures, StandardScaler, and LogisticRegression.\n# PolynomialFeatures adds interaction terms and higher order terms, increasing model complexity.\npipeline = make_pipeline(\n    PolynomialFeatures(), \n    StandardScaler(), \n    LogisticRegression(max_iter=1000)\n)\n# Define a grid of parameters to tune:\n# - polynomialfeatures__degree: The degree of the polynomial features (1 = no expansion, 2 = quadratic, etc.)\n# - logisticregression__C: Regularization strength for logistic regression.\nparam_grid = {\n    'polynomialfeatures__degree': [1, 2],  # Limit the degree to reduce feature explosion\n    'logisticregression__C': [0.1, 1, 10]    # A small set of regularization values\n}\n\nWe use GridSearchCV to perform a 3-fold cross-validation test each combination of polynomial degree and regularization strength. The training set is split into 3 parts, and each model configuration is evaluated on each part. The best parameters are then chosen based on the highest average accuracy, and the final model is trained on the entire training set with those parameters taht we find.\n\n\n# Perform grid search with 3-fold cross-validation to speed up computation.\ngrid = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy')\ngrid.fit(X_train, y_train)\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('polynomialfeatures',\n                                        PolynomialFeatures()),\n                                       ('standardscaler', StandardScaler()),\n                                       ('logisticregression',\n                                        LogisticRegression(max_iter=1000))]),\n             param_grid={'logisticregression__C': [0.1, 1, 10],\n                         'polynomialfeatures__degree': [1, 2]},\n             scoring='accuracy')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV?Documentation for GridSearchCViFittedGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('polynomialfeatures',\n                                        PolynomialFeatures()),\n                                       ('standardscaler', StandardScaler()),\n                                       ('logisticregression',\n                                        LogisticRegression(max_iter=1000))]),\n             param_grid={'logisticregression__C': [0.1, 1, 10],\n                         'polynomialfeatures__degree': [1, 2]},\n             scoring='accuracy') best_estimator_: PipelinePipeline(steps=[('polynomialfeatures', PolynomialFeatures()),\n                ('standardscaler', StandardScaler()),\n                ('logisticregression',\n                 LogisticRegression(C=10, max_iter=1000))]) PolynomialFeatures?Documentation for PolynomialFeaturesPolynomialFeatures() StandardScaler?Documentation for StandardScalerStandardScaler() LogisticRegression?Documentation for LogisticRegressionLogisticRegression(C=10, max_iter=1000)"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#testing-our-accuracy",
    "href": "posts/Auditing Bias/index.html#testing-our-accuracy",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Testing our accuracy",
    "text": "Testing our accuracy\nIn this section, we test and evaluate our classifier. From our grid search, we found that the best parameters are a polynomial degree of 2 and a regularization strength (C) of 10. This helped us achieved a cross-validation accuracy of about 82.2%, and when we evaluated the model on the test set, we obtained an accuracy of roughly 82.5%. This means that our classifier correctly predicts whether an individual is employed for 82% of the cases in the test set.\n\n\n# Output the best parameters and cross-validation accuracy.\nprint(\"Best Parameters:\", grid.best_params_)\nprint(\"Best Cross-Validation Accuracy:\", grid.best_score_)\n\n# Evaluate the best model on the test set.\ny_pred = grid.predict(X_test)\ntest_accuracy = (y_pred == y_test).mean()\nprint(\"Test Set Accuracy:\", test_accuracy)\n\nBest Parameters: {'logisticregression__C': 10, 'polynomialfeatures__degree': 2}\nBest Cross-Validation Accuracy: 0.8223675160030627\nTest Set Accuracy: 0.8245825789579562"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#auditing-our-model-overall-measures",
    "href": "posts/Auditing Bias/index.html#auditing-our-model-overall-measures",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Auditing Our Model (Overall Measures)",
    "text": "Auditing Our Model (Overall Measures)\nHere we evaluate our model’s performance on the test data. We calculate the confusion matrix, which includes true negatives (TN), false positives (FP), false negatives (FN), and true positives (TP). The model’s overall accuracy is about 82.5%. Its precision (PPV) shows that around 78% of predicted employed cases were correct. The false negative rate (FNR) indicates that about 15.4% of employed individuals were missed, while the false positive rate (FPR) reveals that roughly 19.3% of non-employed individuals were incorrectly classified as employed. These metrics show us how effective our model is on unseen data.\n\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\nfrom sklearn.calibration import calibration_curve\n\n# --------------------------\n# 1. Overall Measures\n# --------------------------\n# Compute overall confusion matrix and metrics\n# y_test: true labels; y_pred: binary predictions from your best model\ncm = confusion_matrix(y_test, y_pred)\ntn, fp, fn, tp = cm.ravel()\n\noverall_accuracy = accuracy_score(y_test, y_pred)\noverall_ppv = precision_score(y_test, y_pred)  # PPV = precision\noverall_fnr = fn / (tp + fn) if (tp + fn) &gt; 0 else np.nan  # false negative rate\noverall_fpr = fp / (tn + fp) if (tn + fp) &gt; 0 else np.nan  # false positive rate\n\nprint(\"Overall Accuracy:\", overall_accuracy)\nprint(\"Overall PPV (Precision):\", overall_ppv)\nprint(\"Overall False Negative Rate:\", overall_fnr)\nprint(\"Overall False Positive Rate:\", overall_fpr)\n\nOverall Accuracy: 0.8245825789579562\nOverall PPV (Precision): 0.7794621534627765\nOverall FNR: 0.15375944087476046\nOverall FPR: 0.19286298011441025"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#auditing-our-model-testing-by-group-measures",
    "href": "posts/Auditing Bias/index.html#auditing-our-model-testing-by-group-measures",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Auditing our model (testing by group measures)",
    "text": "Auditing our model (testing by group measures)\nOur group metrics reveal how well our model performs across different subgroups. In our by-group analysis, Group 1 (White individuals) has an accuracy of 82.7% and a precision of 78.9%, with a false negative rate of 15.2% and a false positive rate of 19.2%. Group 2 (Black individuals) shows slightly lower performance, with an accuracy of 81.3% and a precision of 70.9%, alongside a higher false negative rate (17.2%). These differences suggest that our model may be less reliable for Black individuals, which is an important insight for our project focused on detecting and addressing racial bias in employment predictions.\n\n# By-Group Measures\n\n# Get the unique groups \ngroups = np.unique(group_test)\ngroup_metrics = []\n\n# Loop over each group to calculate the performance metrics\nfor g in groups:\n    idx = (group_test == g)\n    # Extract the true labels and predictions for this group\n    y_true_g = y_test[idx]\n    y_pred_g = y_pred[idx]\n\n    # here we check if the group contains both classes (0 and 1) for a reliable confusion matrix\n    if len(np.unique(y_true_g)) &lt; 2:  # Handle cases with only one class\n        tn_g = fp_g = fn_g = tp_g = np.nan\n    else: # Compute the confusion matrix for the current group\n        cm_g = confusion_matrix(y_true_g, y_pred_g)\n        try: # Unpack the confusion matrix into true negatives, false positives, false negatives, and true positives\n            tn_g, fp_g, fn_g, tp_g = cm_g.ravel()\n        except ValueError: # If there's an error (e.g., due to an unexpected shape), assign NaN\n            tn_g = fp_g = fn_g = tp_g = np.nan\n\n    # Calculate accuracy: proportion of correct predictions in this group\n    acc = accuracy_score(y_true_g, y_pred_g)\n    # Calculate precision (PPV): proportion of predicted positives that are actually positive\n    ppv = precision_score(y_true_g, y_pred_g, zero_division=0)\n    # Calculate false negative rate (FNR): proportion of actual positives that were missed\n    fnr = fn_g / (tp_g + fn_g) if (tp_g + fn_g) &gt; 0 else np.nan\n    # Calculate false positive rate (FPR): proportion of actual negatives that were incorrectly predicted as positive\n    fpr = fp_g / (tn_g + fp_g) if (tn_g + fp_g) &gt; 0 else np.nan\n\n    print(f\"\\nGroup {g}:\")\n    print(f\"  Count: {np.sum(idx)}\")\n    print(f\"  Accuracy: {acc:.3f}\")\n    print(f\"  PPV (Precision): {ppv:.3f}\")\n    print(f\"  FNR (False Negative Rate): {fnr:.3f}\")\n    print(f\"  FPR (False Positive Rate): {fpr:.3f}\")\n\n    group_metrics.append({\n        'group': g,\n        'count': np.sum(idx),\n        'accuracy': acc,\n        'ppv': ppv,\n        'fnr': fnr,\n        'fpr': fpr,\n    })\n\n\nGroup 1:\n  Count: 16815\n  Accuracy: 0.827\n  PPV (Precision): 0.789\n  FNR (False Negative Rate): 0.152\n  FPR (False Positive Rate): 0.192\n\nGroup 2:\n  Count: 1813\n  Accuracy: 0.813\n  PPV (Precision): 0.709\n  FNR (False Negative Rate): 0.172\n  FPR (False Positive Rate): 0.195\n\nGroup 3:\n  Count: 126\n  Accuracy: 0.754\n  PPV (Precision): 0.655\n  FNR (False Negative Rate): 0.250\n  FPR (False Positive Rate): 0.244\n\nGroup 5:\n  Count: 21\n  Accuracy: 0.714\n  PPV (Precision): 0.556\n  FNR (False Negative Rate): 0.286\n  FPR (False Positive Rate): 0.286\n\nGroup 6:\n  Count: 474\n  Accuracy: 0.800\n  PPV (Precision): 0.731\n  FNR (False Negative Rate): 0.111\n  FPR (False Positive Rate): 0.276\n\nGroup 7:\n  Count: 3\n  Accuracy: 0.667\n  PPV (Precision): 0.000\n  FNR (False Negative Rate): nan\n  FPR (False Positive Rate): nan\n\nGroup 8:\n  Count: 162\n  Accuracy: 0.809\n  PPV (Precision): 0.815\n  FNR (False Negative Rate): 0.195\n  FPR (False Positive Rate): 0.188\n\nGroup 9:\n  Count: 470\n  Accuracy: 0.847\n  PPV (Precision): 0.726\n  FNR (False Negative Rate): 0.185\n  FPR (False Positive Rate): 0.139"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#calculating-bias-measures",
    "href": "posts/Auditing Bias/index.html#calculating-bias-measures",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Calculating Bias Measures",
    "text": "Calculating Bias Measures\nCalibration Curve Analysis:\n\nWe can see in our calibration curve, which shows how well our model’s predicted probabilities align with the actual outcomes. The blue line (our model’s predictions) is very close to the orange diagonal (perfect calibration), indicating that when our model predicts a probability ( p ) of being employed, roughly ( p% ) of those individuals are employed. From our graph, we can say that our model is well-calibrated overall, which means it does a good job matching predicted probabilities to real-world outcomes. However, calibration alone does not guarantee fairness, so we still need to check subgroup performance to ensure the model treats different groups equitably.\n\n\n# So we need to first calculate our y_prob: predicted probability for the positive class\ny_prob = grid.predict_proba(X_test)[:, 1]\n\n# Overall calibration curve\nprob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\nplt.figure(figsize=(6, 6))\nplt.plot(prob_pred, prob_true, label=\"Overall\")\nplt.plot([0, 1], [0, 1], label=\"Perfect Calibration\")\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve: Overall\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCalibration Curve by Group:\n\nOur graph below shows our calibration curve broken down by race group. Each colored line corresponds to a different group’s predicted probabilities versus their actual outcomes.We can see in our graph that Group 1 (White) appears relatively close to the diagonal for most probability ranges, suggesting good calibration. Group 2 (Black/African American) shows more fluctuation, especially in the mid-probability range, indicating that the model’s predicted probabilities for this group deviate more from perfect calibration. Groups with fewer samples (such as Groups 5, 7, and 9) have jagged lines, which likely stems from limited data. Looking at these deviations is important for understanding and addressing potential fairness issues across all groups.\n\n\ngroups = np.unique(group_test)\nplt.figure(figsize=(6, 6))\nfor g in groups:\n    idx = (group_test == g)\n    # If there are very few samples for this group, skip\n    if np.sum(idx) &lt; 10:\n        continue\n    prob_true_g, prob_pred_g = calibration_curve(y_test[idx], y_prob[idx], n_bins=10)\n    plt.plot(prob_pred_g, prob_true_g,  label=f\"Group {g}\")\nplt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\nplt.xlabel(\"Predicted Probability\")\nplt.ylabel(\"Fraction of Positives\")\nplt.title(\"Calibration Curve by Group\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nError Rate Balance\n\nOur error rate balance shows that overall, our model misses about 15.4% of employed individuals (FNR) and incorrectly predicts employment for 19.3% of unemployed individuals (FPR). For Group 1 (White individuals), the rates are very similar—15.2% FNR and 19.2% FPR. In contrast, Group 2 (Black individuals) has slightly higher error rates, with an FNR of 17.2% and an FPR of 19.5%. Other groups, particularly those with fewer samples, exhibit more variability: for example, Group 3 shows a 25% FNR and 24% FPR, while Group 6 (Asian individuals) has a low FNR (11.1%) but a higher FPR (27.6%). These differences indicate that our model does not achieve perfect error rate balance across all groups, highlighting areas where bias may be present in predicting employment status.\n\n\ndef get_fnr_fpr(y_true, y_pred):\n    \"\"\"Compute FNR and FPR for given true and predicted labels.\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    tn, fp, fn, tp = cm.ravel()\n    fnr = fn / (fn + tp) if (fn + tp) &gt; 0 else np.nan\n    fpr = fp / (fp + tn) if (fp + tn) &gt; 0 else np.nan\n    return fnr, fpr\n\n# Overall FNR/FPR\noverall_fnr, overall_fpr = get_fnr_fpr(y_test, grid.predict(X_test))\nprint(f\"Overall FNR: {overall_fnr:.3f}\")\nprint(f\"Overall FPR: {overall_fpr:.3f}\")\n\n# By-group FNR/FPR\nfor g in groups:\n    idx = (group_test == g)\n    y_true_g = y_test[idx]\n    y_pred_g = grid.predict(X_test[idx])\n    fnr_g, fpr_g = get_fnr_fpr(y_true_g, y_pred_g)\n    print(f\"Group {g} FNR: {fnr_g:.3f}, FPR: {fpr_g:.3f}\")\n\nOverall FNR: 0.154\nOverall FPR: 0.193\nGroup 1 FNR: 0.152, FPR: 0.192\nGroup 2 FNR: 0.172, FPR: 0.195\nGroup 3 FNR: 0.250, FPR: 0.244\nGroup 5 FNR: 0.286, FPR: 0.286\nGroup 6 FNR: 0.111, FPR: 0.276\nGroup 7 FNR: nan, FPR: 0.333\nGroup 8 FNR: 0.195, FPR: 0.188\nGroup 9 FNR: 0.185, FPR: 0.139\n\n\nStatistical Parity Test\n\nOur statistical parity analysis shows that, overall, our model predicts employment for about 48.4% of individuals. However, when we break it down by race, we see notable differences that raise fairness concerns. For example, Group 1 (White individuals) has a predicted employment rate of 49.3%, whereas Group 2 (Black individuals) is lower at 42.6%. Other groups vary as well—Group 6 (Asian individuals) has a higher rate of 55.7%, while Groups 7 and 9 have much lower rates (33.3% and 34.9%, respectively). These disparities in predicted positive rates suggest that our model may favor some groups over others, which is a critical issue in auditing fairness.\n\n\ny_pred = grid.predict(X_test)\n\n# Overall predicted positive rate\noverall_positive_rate = np.mean(y_pred)\nprint(f\"\\nOverall Predicted Positive Rate: {overall_positive_rate:.3f}\")\n\n# By-group predicted positive rate\nfor g in groups:\n    idx = (group_test == g)\n    group_positive_rate = np.mean(y_pred[idx])\n    print(f\"Group {g} Predicted Positive Rate: {group_positive_rate:.3f}\")\n\n\nOverall Predicted Positive Rate: 0.484\nGroup 1 Predicted Positive Rate: 0.493\nGroup 2 Predicted Positive Rate: 0.426\nGroup 3 Predicted Positive Rate: 0.437\nGroup 5 Predicted Positive Rate: 0.429\nGroup 6 Predicted Positive Rate: 0.557\nGroup 7 Predicted Positive Rate: 0.333\nGroup 8 Predicted Positive Rate: 0.500\nGroup 9 Predicted Positive Rate: 0.349\n\n\nFeasible FNR and FPR rates interpretation\nIn our graph each colored line shows all (FNR, FPR) combinations that would achieve a fixed positive predictive value (PPV) for a given group, based on that group’s prevalence. The circular markers indicate our model’s actual (FNR, FPR).\n\nGroup 1 (White): Its feasible line illustrates how changing the threshold could shift its false negative rate (FNR) and false positive rate (FPR).\nGroup 2 (Black): Has its own feasible line, which typically differs because of a higher prevalence (more individuals in the group are actually employed) and/or different PPV requirements.\n\nIf we tried to make FPR the same for both groups (say, moving Group 2’s marker to Group 1’s FPR), we would generally need to increase Group 2’s FNR. That means more Black individuals who are actually employed would be missed by the model—an important trade-off for fairness. Essentially, achieving equal FPR across groups often comes at the cost of raising FNR for at least one group, which can exacerbate disparities in missed positives. This plot helps us visualize how much we’d have to adjust each group’s threshold to meet that fairness goal, and how it could impact different groups in our dataset.\nIn the figure below, each colored line shows the combinations of false negative rate (FNR) and false positive rate (FPR) that would achieve the same positive predictive value (PPV) for a particular group, given its prevalence. The colored dots represent our model’s actual (FNR, FPR) for each group. Notably, some groups’ actual points lie close to their feasible lines, meaning only minor threshold adjustments would be required to change their FPR or FNR. However, for groups where the actual point is far from the feasible line (e.g., Group 2), forcing a lower FPR would require a significant increase in FNR. In practical terms, that means we would miss many more truly employed individuals from that group. This highlights the inherent trade-off between equalizing FPR across groups and maintaining a reasonable FNR, especially when group prevalences differ.\n\ncolor_map = {\n    1: \"blue\",\n    2: \"green\",\n    3: \"red\",\n    5: \"purple\",\n    6: \"orange\",\n    7: \"brown\",\n    8: \"pink\",\n    9: \"gray\"\n}\n\nfpr_lines = {}\nfpr_actual = {}\nfor g in groups:\n    idx = (group_test == g)\n    # Prevalence: proportion of true positives in group g\n    p = np.mean(y_test[idx])\n    fnr_range = np.linspace(0, 1, 100)\n    PPV_desired = overall_ppv  # fixed desired PPV\n    # Compute feasible FPR using Eq. (2.6)\n    fpr_line = np.where((1 - p) &gt; 0, ((1 - fnr_range) * p * (1/PPV_desired - 1)) / (1 - p), np.nan)\n    fpr_lines[g] = (fnr_range, fpr_line)\n    \n    # Get actual FNR and FPR from group_metrics\n    for metric in group_metrics:\n        if metric['group'] == g:\n            fpr_actual[g] = (metric['fnr'], metric['fpr'])\n            break\n\n# Plot the feasible FNR-FPR lines and actual points with consistent colors\nplt.figure(figsize=(8, 6))\nfor g, (fnr_range, fpr_line) in fpr_lines.items():\n    plt.plot(fnr_range, fpr_line, linestyle='--', linewidth=2,\n             label=f\"Feasible for Group {g}\", color=color_map.get(g, \"black\"))\n    if g in fpr_actual:\n        actual_fnr, actual_fpr = fpr_actual[g]\n        plt.plot(actual_fnr, actual_fpr, \"o\", markersize=10, \n                 markeredgecolor='black', markeredgewidth=1.5,\n                 color=color_map.get(g, \"black\"), \n                 label=f\"Actual Group {g}\")\n        \nplt.xlabel(\"False Negative Rate (FNR)\")\nplt.ylabel(\"False Positive Rate (FPR)\")\nplt.title(\"Feasible FNR vs. FPR Rates (Chouldechova 2017 Eq. 2.6)\")\nplt.legend()\nplt.xlim([0, 1])\nplt.ylim([0, 0.3])  # Limit the y-axis to 0.3 for better visibility\nplt.show()"
  },
  {
    "objectID": "posts/Auditing Bias/index.html#interpreting-fnr-changes-to-equalize-fpr",
    "href": "posts/Auditing Bias/index.html#interpreting-fnr-changes-to-equalize-fpr",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Interpreting FNR Changes to Equalize FPR:",
    "text": "Interpreting FNR Changes to Equalize FPR:\nFrom the feasible lines, we see how each group’s false negative rate (FNR) would have to shift if we wanted all groups to share the same false positive rate (FPR). For example, if Group 2’s actual FPR is higher than Group 1’s, lowering it to match Group 1’s level would require moving Group 2’s marker along its feasible line toward a lower FPR. This shift, however, pushes the marker toward a much higher FNR—often much higher than the group’s current FNR. In other words, this means that, to reduce Group 2’s false positives, we would end up missing many more truly employed individuals (i.e., a spike in FNR). Each group’s line tells a similar story: matching another group’s FPR typically demands a noticeable trade-off in FNR, underscoring the difficulty of balancing error rates across groups when their underlying prevalence differs."
  },
  {
    "objectID": "posts/Auditing Bias/index.html#concluding-discussion",
    "href": "posts/Auditing Bias/index.html#concluding-discussion",
    "title": "Auditing Bias in Machine Learning Models",
    "section": "Concluding Discussion",
    "text": "Concluding Discussion\nOur project aimed to predict whether an individual is employed using demographic data from the ACS PUMS dataset. Systems like this could benefit various stakeholders. For example, companies in human resources or financial services might use such a model to assess employment status quickly and efficiently. Government agencies could also deploy this system to better understand labor market trends or to guide social policy. Essentially, any organization that needs insights into employment patterns could find value in our approach.\nHowever, our bias audit revealed important fairness challenges. Although the model is overall well-calibrated—meaning its predicted probabilities match real-world outcomes—it shows differences in error rates across racial groups. Group 1 (White individuals) and Group 2 (Black individuals) have similar overall accuracy, but Black individuals experience a higher false negative rate, indicating that more truly employed individuals are being missed by the model. This disparity in error rates is critical; deploying such a model at scale might inadvertently disadvantage Black individuals, leading to biased outcomes in decision-making processes.\nBeyond bias in error rates, there are other potential concerns. Even if calibration is good, uneven predicted positive rates suggest that the model may favor one group over another, undermining fairness. Moreover, issues such as data quality, changes over time, and the potential misuse of the model in sensitive contexts (e.g., employment or credit scoring) could further compound these problems. To address these concerns, it is important to continue refining the model, incorporate more diverse and representative data, and establish robust monitoring and adjustment procedures when the model is deployed in real-world settings."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html",
    "href": "posts/limits of approach of quantitative bias/index.html",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In today’s day and age where data-driven decisions increasingly affect everyday lives—from credit scoring to hiring practices—quantitative methods have become central to assessing discrimination and bias. In his 2022 speech, Arvind Narayanan argues that “currently quantitative methods are primarily used to justify the status quo” Narayanan (2022). He challenges the prevalent view that numbers provide an objective measure of fairness, suggesting instead that quantitative approaches may inadvertently reinforce existing social hierarchies. This essay examines Narayanan’s position, discusses the dual nature of quantitative methods in both uncovering and perpetuating discrimination, and reviews a case study where quantitative techniques have been beneficial in illuminating fairness issues in decision-making processes."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#introduction",
    "href": "posts/limits of approach of quantitative bias/index.html#introduction",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "In today’s day and age where data-driven decisions increasingly affect everyday lives—from credit scoring to hiring practices—quantitative methods have become central to assessing discrimination and bias. In his 2022 speech, Arvind Narayanan argues that “currently quantitative methods are primarily used to justify the status quo” Narayanan (2022). He challenges the prevalent view that numbers provide an objective measure of fairness, suggesting instead that quantitative approaches may inadvertently reinforce existing social hierarchies. This essay examines Narayanan’s position, discusses the dual nature of quantitative methods in both uncovering and perpetuating discrimination, and reviews a case study where quantitative techniques have been beneficial in illuminating fairness issues in decision-making processes."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#narayanans-position-on-quantitative-methods",
    "href": "posts/limits of approach of quantitative bias/index.html#narayanans-position-on-quantitative-methods",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Narayanan’s Position on Quantitative Methods",
    "text": "Narayanan’s Position on Quantitative Methods\nNarayanan’s speech critiques the heavy reliance on quantitative analyses in the study of discrimination. He argues that quantitative methods assume no discrimination unless proven otherwise, placing the burden of proof on those claiming bias. He asserts that this default assumption upholds existing power structures rather than challenging them. When disparities appear in metrics like false-positive rates in criminal risk assessments, they are often explained as statistical noise rather than evidence of systemic injustice Narayanan (2022).\nFurthermore, Narayanan emphasizes that quantitative methods, by their very nature, reduce complex social phenomena to just numbers. By doing this, they risk overlooking the real experiences of those affected by discrimination. While significance tests and confusion matrices show differences in outcomes between racial or gender groups, these metrics alone cannot fully capture the complexity of systemic oppression. This critique challenges scholars and practitioners to consider whether the technical accuracy of quantitative methods always aligns with their social impact."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#the-benefits-of-quantitative-methods",
    "href": "posts/limits of approach of quantitative bias/index.html#the-benefits-of-quantitative-methods",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "The Benefits of Quantitative Methods",
    "text": "The Benefits of Quantitative Methods\nDespite Narayanan’s reservations, quantitative methods have undeniable strengths that have contributed to significant advancements in understanding discrimination. Some of their benefits include:\n\nPrecision and Replicability: Quantitative techniques, such as formal mathematical definitions of bias and fairness, provide a framework for rigorously measuring disparities. For example, fairness metrics like statistical parity, equality of opportunity, and calibration enable researchers to compare outcomes across groups in a replicable manner Barocas, Hardt, and Narayanan (2023). These methods are especially useful in contexts where decisions must be justified to regulatory bodies or stakeholders.\nScalability: Large datasets from various sectors (e.g., criminal justice, hiring, lending) allow quantitative methods to detect subtle patterns of bias that might escape qualitative analyses. Collecting data systematically and analyzing it statistically can uncover trends—such as differences in false-positive rates between demographic groups—that highlight broader systemic issues. These findings can then guide policy decisions to address such disparities.\nPolicy Relevance: Quantitative analyses have played a critical role in policy-making. A notable example is the ProPublica investigation into criminal risk prediction tools, which used statistical tests to show that Black defendants were more likely to be misclassified as high risk compared to white defendants. Although Narayanan critiques the limitations of such studies, it is undeniable that they have driven significant public and policy debates about algorithmic fairness Narayanan (2022). Quantitative evidence, when appropriately contextualized, can help mobilize reform and promote accountability."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#an-example-of-a-beneficial-quantitative-study",
    "href": "posts/limits of approach of quantitative bias/index.html#an-example-of-a-beneficial-quantitative-study",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "An Example of a Beneficial Quantitative Study",
    "text": "An Example of a Beneficial Quantitative Study\nOne beneficial example of using quantitative methods in the study of fairness is the analysis of credit-scoring algorithms. In the study discussed by Barocas, Hardt, and Narayanan Barocas, Hardt, and Narayanan (2023), researchers used detailed statistical models to assess whether machine learning systems used in credit decisions perpetuated racial or gender disparities. By analyzing large datasets and employing fairness criteria, they were able to identify that certain features—even when not explicitly related to race or gender—acted as proxies for these protected attributes. The study not only highlighted the existence of bias but also suggested modifications to the algorithms that could lead to fairer outcomes.\nThe strength of this quantitative approach lies in its ability to pinpoint exactly which elements of the data contribute to discriminatory outcomes. For example, confusion matrices and false-positive rates provided clear metrics by which the performance of the credit-scoring model could be judged across different demographic groups. This analysis allowed for the identification of unintended consequences—such as systematically lower credit scores for minority applicants—that might have otherwise been overlooked. Thus, when used carefully and coupled with a deep understanding of the social context, quantitative methods can offer actionable insights that promote more equitable decision-making."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#the-drawbacks-and-limitations-of-quantitative-approaches",
    "href": "posts/limits of approach of quantitative bias/index.html#the-drawbacks-and-limitations-of-quantitative-approaches",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "The Drawbacks and Limitations of Quantitative Approaches",
    "text": "The Drawbacks and Limitations of Quantitative Approaches\nWhile the benefits are compelling, the drawbacks of relying solely on quantitative methods are equally significant, as Narayanan and other scholars have pointed out:\n\nReductionism and Oversimplification: Quantitative methods often require reducing complex human experiences and societal structures to numerical values. This abstraction can lead to a failure to account for the context and intersectionality inherent in discrimination. For instance, while statistical tests might show that women are underrepresented in leadership roles, they cannot capture the myriad ways in which gender intersects with race, socioeconomic status, and other factors to produce these outcomes D’Ignazio and Klein (2023).\nThe Null Hypothesis Problem: As Narayanan argues, the conventional use of the null hypothesis—assuming no discrimination until evidence suggests otherwise—can obscure genuine disparities. By demanding rigorous statistical proof before acknowledging discrimination, quantitative methods may inadvertently uphold the status quo. This methodological choice means that even when quantitative data reveal significant disparities, the interpretation of these results may lean towards rationalizing existing practices rather than questioning them.\nIgnoring Qualitative Nuance: Quantitative studies may fail to capture the lived experiences of those affected by discrimination. For example, while a machine learning model might quantify the disparity in false-positive rates, it cannot communicate the real-world impact of these errors on individuals’ lives. As critics like Buolamwini and Gebru Buolamwini and Gebru (2018) have argued, integrating qualitative insights is essential for understanding the human cost of algorithmic bias. Qualitative methods—through interviews, ethnographic research, and case studies—provide context that enriches the quantitative findings.\nReinforcing Existing Biases: When historical data, which often reflect long-standing societal prejudices, are used to train machine learning models, the models can perpetuate these biases. This phenomenon is sometimes referred to as “feedback loops,” where biased decisions lead to further bias in future data collection. Cathy O’Neil, in Weapons of Math Destruction, argues that such opaque, unaccountable algorithms can reinforce and even amplify societal inequalities O’Neil (2016). In such cases, quantitative methods may end up reinforcing the very inequalities they are meant to expose, unless corrective interventions are thoughtfully implemented."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#balancing-quantitative-and-qualitative-approaches",
    "href": "posts/limits of approach of quantitative bias/index.html#balancing-quantitative-and-qualitative-approaches",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Balancing Quantitative and Qualitative Approaches",
    "text": "Balancing Quantitative and Qualitative Approaches\nThe critique posed by Narayanan calls for a balanced approach that integrates both quantitative and qualitative methods. Quantitative analysis provides the rigorous statistical evidence needed to identify and measure disparities, while qualitative research adds depth and context to understand the root causes and real-world impacts of these disparities. For instance, while the quantitative study of credit-scoring algorithms identified proxy variables that contributed to discriminatory outcomes, qualitative interviews with affected applicants could reveal how these algorithmic decisions influence their access to financial services and overall economic mobility.\nSeveral scholars have advocated for mixed-methods research as a way to bridge the gap between numerical data and human experience. Selbst et al. Selbst et al. (2019) argue that incorporating qualitative insights into quantitative frameworks not only enriches the analysis but also helps to avoid the pitfalls of reductionism. Such approaches are particularly valuable in sensitive areas like discrimination, where numbers alone cannot capture the full spectrum of injustice."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#my-position-on-narayanans-claim",
    "href": "posts/limits of approach of quantitative bias/index.html#my-position-on-narayanans-claim",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "My Position on Narayanan’s Claim",
    "text": "My Position on Narayanan’s Claim\nAfter weighing the evidence and arguments, I find merit in Narayanan’s claim that quantitative methods, when used in isolation, risk justifying the status quo. It is clear that the methodological choices—such as adopting the null hypothesis of no discrimination—can skew interpretations and obscure the broader social dynamics at play. However, I would argue that quantitative methods are not inherently harmful; rather, their impact depends on how they are applied and interpreted.\nWhen used with qualitative research, quantitative methods become a powerful tool for highlighting inequities and informing policy. For example, the credit-scoring study discussed earlier shows that with careful design and a willingness to interrogate the data critically, quantitative analyses can lead to meaningful reforms. Therefore, I agree with Narayanan’s caution against a purely quantitative lens but also advocate for a balanced methodology that leverages the strengths of both approaches.\nIn practice, decision-makers and researchers should be transparent about the limitations of their quantitative analyses. They should complement statistical findings with qualitative research to ensure that the numbers do not become an end in themselves but serve as a means to a more comprehensive understanding of discrimination. This integrative approach can help shift the focus from merely justifying existing practices to actively challenging and transforming them."
  },
  {
    "objectID": "posts/limits of approach of quantitative bias/index.html#conclusion",
    "href": "posts/limits of approach of quantitative bias/index.html#conclusion",
    "title": "Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Conclusion",
    "text": "Conclusion\nQuantitative methods have transformed the study of discrimination by offering precise, scalable, and policy-focused tools for analyzing disparities. However, as Narayanan (2022) points out, these methods have significant limitations, including the risk of reinforcing the status quo when used without proper context. For example, credit-scoring algorithms show that while quantitative techniques can detect and reduce bias, they must be paired with qualitative insights to fully address the complexities of discrimination.\nUltimately, the debate over fairness in quantitative methods is not about choosing between data and narratives. Instead, it requires integrating both—valuing statistical rigor while remaining mindful of the social, historical, and human factors that shape discrimination. By doing so, researchers and policymakers can use quantitative methods to promote meaningful change rather than unintentionally upholding existing inequities."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing Perceptron",
    "section": "",
    "text": "%load_ext autoreload\n%autoreload 2\nfrom perceptron import Perceptron, PerceptronOptimizer"
  },
  {
    "objectID": "posts/perceptron/index.html#abstract",
    "href": "posts/perceptron/index.html#abstract",
    "title": "Implementing Perceptron",
    "section": "Abstract",
    "text": "Abstract\nIn this blog post, I implement the perceptron algorithm using PyTorch. I experiment with linear boundaries that separate two classes in a dataset, as well as cases where a clear separation does not exist. My implementation includes a LinearModel class for basic operations (computing scores and predictions) and a Perceptron subclass that defines the loss (misclassification rate) and gradient update. The algorithm updates its weight vector whenever a data point is misclassified. Additionally, I use visualization functions to display the training data and, for 2D cases, both the decision boundary and the evolution of the loss over time."
  },
  {
    "objectID": "posts/perceptron/index.html#train-using-the-perceptron-optimizer-minimal-training-loop",
    "href": "posts/perceptron/index.html#train-using-the-perceptron-optimizer-minimal-training-loop",
    "title": "Implementing Perceptron",
    "section": "Train Using the Perceptron Optimizer (Minimal Training Loop)",
    "text": "Train Using the Perceptron Optimizer (Minimal Training Loop)"
  },
  {
    "objectID": "posts/perceptron/index.html#walk-through-of-grad-function",
    "href": "posts/perceptron/index.html#walk-through-of-grad-function",
    "title": "Implementing Perceptron",
    "section": "Walk through of Grad function",
    "text": "Walk through of Grad function\nThe grad() function in my implementation is responsible for computing the weight update for a single example. So what i did was:\n\nFirst, I made sure the data point X is in the right shape. If it came in as a row (1, p), I turned it into a 1D vector (p,).\nThen I checked if the label y is in {-1, 1}. If it wasnt, I converted it using y_mod = 2 * y - 1.\nI calculated the score by taking the dot product of the weights and the input (s = w • x). This score tells us how far the point is from the decision boundary.\nIf s * y_mod is less than 0, that means the point is misclassified, so I returned the gradient as -y_mod * X. This will move the weights in the right direction.\nIf the point was classified correctly, I just returned a zero vector (so there would be no update needed).\n\nThis matches the perceptron update rule taht if a point is wrong, we update the weights with (2y - 1) * x. Lets now perfom some experiments to check our implementation. Before that, teh code below does a simple check to see if our implementation is working by creating 2d data and perfroms the training and prints our loss after our update step.\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom perceptron import Perceptron, PerceptronOptimizer, perceptron_data, plot_perceptron_data\n\n# Generate the 2D data.\nX, y = perceptron_data(n_points=300, noise=0.2)\n\n# Plot the training data.\nfig, ax = plt.subplots(figsize=(6, 6))\nplot_perceptron_data(X, y, ax)\nax.set_title(\"Perceptron Training Data\")\nplt.show()\n\n# Create a perceptron and an optimizer.\np = Perceptron()\nopt = PerceptronOptimizer(p)\nloss = opt.step(X[0:1], y[0])  # update using a single data point.\nprint(\"Loss after one step:\", loss.item())\n\n\n\n\n\n\n\n\nLoss after one step: 1.0"
  },
  {
    "objectID": "posts/perceptron/index.html#runtime-complexity-of-a-single-iteration",
    "href": "posts/perceptron/index.html#runtime-complexity-of-a-single-iteration",
    "title": "Implementing Perceptron",
    "section": "Runtime Complexity of a Single Iteration",
    "text": "Runtime Complexity of a Single Iteration\nFor each update (i.e., for each single data point):\nDot Product: The calculation torch.dot(self.w, X) involves summing over the products of corresponding elements of two vectors. If there are p features, this operation is O(p).\nGradient Computation and Weight Update: Similarly, multiplying the feature vector by the scalar label and subtracting this from the weight vector both take O(p) time.\nThus, the runtime complexity for a single iteration (or update) of the perceptron algorithm is O(p), where p is the number of features in your dataset. In the context of the full training loop, if you compute the loss over n points, that operation is O(n×p), but each update step remains O(p).\n\nimport torch\nimport matplotlib.pyplot as plt\nfrom perceptron import Perceptron, PerceptronOptimizer, perceptron_data, plot_perceptron_data\n\n# Generate the 2D data.\nX, y = perceptron_data(n_points=300, noise=0.2)\n\n# Plot the training data.\nfig, ax = plt.subplots(figsize=(6, 6))\nplot_perceptron_data(X, y, ax)\nax.set_title(\"Perceptron Training Data\")\nplt.show()\n\n# Optionally, test a training step:\np = Perceptron()\nopt = PerceptronOptimizer(p)\nloss = opt.step(X[0:1], y[0])  # update using a single data point.\nprint(\"Loss after one step:\", loss.item())\n\n\n\n\n\n\n\n\nLoss after one step: 1.0"
  },
  {
    "objectID": "posts/perceptron/index.html#experiment-1-perceptron-training-on-2d-data",
    "href": "posts/perceptron/index.html#experiment-1-perceptron-training-on-2d-data",
    "title": "Implementing Perceptron",
    "section": "Experiment 1: Perceptron Training on 2D data",
    "text": "Experiment 1: Perceptron Training on 2D data\nIn the experiment below, I use the perceptron algorithm to classify 2D data that is linearly separable.\n\nWe first generate 2D data using perceptron_data(), which creates two clearly separated classes and adds a bias column.\nWe next initialize a Perceptron model and an optimizer. For each iteration we calculate the current loss (misclassification rate), randomly select a single data point and if the model misclassifies it, we update the weights using the Perceptron rule.\nWe repeat this until the model achieves zero loss or we reach the maximum number of iterations.\n\n\n# Experiment 1: 2D Data (Linearly Separable\nX2, y2 = perceptron_data(n_points=300, noise=0.2)  #generate 2d data \np2 = Perceptron()\nopt2 = PerceptronOptimizer(p2)\nloss_vec = []\nn = X2.size(0)\nmax_iter = 1000\nfor _ in range(max_iter):\n    loss = p2.loss(X2, y2)\n    loss_vec.append(loss.item())\n    if loss.item() == 0: break\n    i = torch.randint(n, (1,))\n    opt2.step(X2[[i], :], y2[i])\n \n\n#Plot decision boundary on 2D data:\nw = p2.w  # final weight vector: [w1, w2, bias]\nx1_vals = torch.linspace(X2[:,0].min()-0.5, X2[:,0].max()+0.5, 100)\nx2_vals = -(w[0]*x1_vals + w[2]) / w[1]  # boundary: w1*x1 + w2*x2 + bias = 0\nplt.figure(figsize=(6,6))\nplt.scatter(X2[:,0], X2[:,1], c=y2)\nplt.plot(x1_vals, x2_vals,  linewidth=2)\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Decision Boundary on 2D Data\")\nplt.show()\n\n\n\n\n\n\n\n\nVisualizing the loss evolution\n\n#plot loss evolution\nplt.figure(figsize=(6,4))\nplt.plot(loss_vec, marker='o', color='slategrey')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Evolution (2D Data)\")\nplt.show()\n\n\n\n\n\n\n\n\nAfter training we plot the loss vs. iteration to show how the loss decreases as the model learns adn we also plot the final decision boundary over the 2D data. The red and blue dots show the two classes and the straight line separates them. This shows that the perceptron can successfully find a separating line when the data is linearly separable."
  },
  {
    "objectID": "posts/perceptron/index.html#runtime-complexity-of-a-single-perceptron-iteration",
    "href": "posts/perceptron/index.html#runtime-complexity-of-a-single-perceptron-iteration",
    "title": "Implementing Perceptron",
    "section": "Runtime Complexity of a Single Perceptron Iteration",
    "text": "Runtime Complexity of a Single Perceptron Iteration\nIn each iteration of perceptron training, we:\n\nCompute a dot product between the weight vector w and a single input example x. If x has p features, this takes O(p) time.\nIf the example is misclassified, we compute the gradient and update the weights — this also takes O(p) time.\n\nSo overall, a single iteration takes O(p) time. It doesn’t depend on the total number of data points n, because we only update using one randomly selected point per iteration.\nLooking at the Loss Evolution graph above, we can also see that the model usually improves quickly (loss drops) and then flattens out. Even if a spike happens, it corrects quickly with just a couple more iterations. This shows that each iteration is efficient and contributes quickly to learning — especially when the data is linearly separable like in our 2D example."
  },
  {
    "objectID": "posts/perceptron/index.html#experiment-2-perceptron-training-on-high-dimensional-data-3-features",
    "href": "posts/perceptron/index.html#experiment-2-perceptron-training-on-high-dimensional-data-3-features",
    "title": "Implementing Perceptron",
    "section": "Experiment 2: Perceptron Training on High-Dimensional Data (3 Features)",
    "text": "Experiment 2: Perceptron Training on High-Dimensional Data (3 Features)\nIn this experiment, I train the perceptron algorithm on a dataset with 5 features instead of just 2.\n\nWe generate high-dimensional data using high_dim_data(), which creates 5-dimensional data points (4 random features plus a constant column for bias). The two classes are linearly separable, but harder to visualize.\nWe initialize a Perceptron model and an optimizer. As before, in each iteration we:\n\ncalculate the current loss,\nrandomly select a data point, and\napply the perceptron update rule if that point is misclassified.\n\nWe keep updating until the loss reaches zero or we hit the maximum number of iterations.\n\n\n\n#  Experiment 2: High-Dimensional Data (5 Features) \ndef high_dim_data(n_points=300, noise=0.2, p_dims=5):\n    # Generate data with p_dims-1 features and append constant column.\n    y = torch.arange(n_points) &gt;= n_points//2\n    X = y[:, None].float() + torch.normal(0.0, noise, size=(n_points, p_dims-1))\n    X = torch.cat((X, torch.ones((n_points, 1))), 1)\n    y = 2 * y.float() - 1  # convert targets to {-1, 1}\n    return X, y\n\nX5, y5 = high_dim_data(n_points=300, noise=0.2, p_dims=5)\np5 = Perceptron()\nopt5 = PerceptronOptimizer(p5)\nloss_vec_hd = []\nn_hd = X5.size(0)\nfor _ in range(max_iter):\n    loss = p5.loss(X5, y5)\n    loss_vec_hd.append(loss.item())\n    if loss.item() == 0: break\n    i = torch.randint(n_hd, (1,))\n    opt5.step(X5[[i], :], y5[i])\n    \n# Plot loss evolution for high-dimensional data:\nplt.figure(figsize=(6,4))\nplt.plot(loss_vec_hd, marker='o', color='teal')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Evolution (High-Dimensional Data)\")\nplt.show()\n\n\n\n\n\n\n\n\nSince the data lives in more than 2 dimensions, we cannot directly plot the decision boundary. But we still visualize the loss over time to see how the model is learning and the graph above shows that the perceptron is still able to learn from high-dimensional data. The loss decreases quickly, though it fluctuates a bit more than in the 2D case — this is makes sense, because higher dimensions can make it harder to find a good separating boundary."
  },
  {
    "objectID": "posts/perceptron/index.html#runtime-complexity-of-a-single-perceptron-iteration-high-dimensional-case",
    "href": "posts/perceptron/index.html#runtime-complexity-of-a-single-perceptron-iteration-high-dimensional-case",
    "title": "Implementing Perceptron",
    "section": "Runtime Complexity of a Single Perceptron Iteration (High-Dimensional Case)",
    "text": "Runtime Complexity of a Single Perceptron Iteration (High-Dimensional Case)\nEven though the data has more features now, the complexity of each iteration stays the same:\n\nWe compute the dot product w · x where x has p features → this takes O(p) time.\nIf the point is misclassified, the gradient computation and weight update also take O(p) time.\n\nSo a single update step still runs in O(p) time. The number of data points n does not affect the time for each step because we’re only using one data point per iteration.\nFrom the Loss Evolution graph above, we can see that perceptron learns effectively. The drops in loss happen fast, confirming that even in high dimensions, each iteration helps the model improve quickly — as long as the data is linearly separable.\nExperiment 3: Perceptron on 2D Data that is not linearly Separable\n\n# Generate nonseparable 2D data by increasing noise.\nX_ns, y_ns = perceptron_data(n_points=300, noise=0.6)\np_ns = Perceptron()\nopt_ns = PerceptronOptimizer(p_ns)\nloss_vec = []\nn = X_ns.size(0)\nmax_iter = 1000\n\n# Training loop: update on a random point each iteration.\nfor _ in range(max_iter):\n    loss = p_ns.loss(X_ns, y_ns).item()\n    loss_vec.append(loss)\n    # If by chance loss becomes 0, we could break—but for nonseparable data, it won't.\n    i = torch.randint(n, (1,))\n    opt_ns.step(X_ns[[i], :], y_ns[i])\n\n\n# Plot the data and the final decision boundary.\nw = p_ns.w  # final weight vector [w1, w2, bias]\nx_vals = torch.linspace(X_ns[:,0].min()-0.5, X_ns[:,0].max()+0.5, 100)\ny_vals = -(w[0]*x_vals + w[2]) / w[1]  # decision boundary: w1*x + w2*y + bias = 0\nplt.figure(figsize=(6,6))\nax = plt.gca()\nplot_perceptron_data(X_ns, y_ns, ax)\nax.plot(x_vals, y_vals, linewidth=2)\nax.set_xlabel(\"x1\")\nax.set_ylabel(\"x2\")\nax.set_title(\"Final Decision Boundary (Nonseparable Data)\")\nplt.show()\n\n# Plot evolution of loss.\nplt.figure(figsize=(6,4))\nplt.plot(loss_vec, marker='o', markersize=3, color='slategrey')\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"Loss Evolution (Nonseparable Data)\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn our experiment above , I tested the perceptron algorithm on 2D data that is not linearly separable by increasing the noise when generating data with perceptron_data().\n\nWe use a noise level of 0.6 which causes the two classes to overlap significantly. This makes it impossible to draw a perfect straight line that separates all the points.\nWe then train the perceptron for up to 1000 iterations. Like before, we calculate the loss at each step and perform updates on a randomly chosen data point.\nAfter training, we plot the loss over time and the final decision boundary. Since the data is not linearly separable, the loss never reaches zero. The model keeps fluctuating as it tries to fit the noisy data.\n\nThe plot of the final decision boundary shows that the perceptron still finds a line that roughly splits the two classes, but some red and blue points are still on the wrong side. The loss curve also shows this as it goes around up and down and never stays flat like it did in the previous experiments. This shows a key limitation of the basic perceptron that it only works well when the data is linearly separable"
  },
  {
    "objectID": "posts/perceptron/index.html#conclusion",
    "href": "posts/perceptron/index.html#conclusion",
    "title": "Implementing Perceptron",
    "section": "Conclusion",
    "text": "Conclusion\nIn this blog post, I learnt to build implement the Perceptron algorithm using PyTorch. I started with linearly separable data and showed that the perceptron can successfully learn a straight-line boundary that separates the two classes. Then, I tried the algorithm on higher-dimensional data and saw that it still learns well. Finally, I tested it on noisy, nonseparable data and noticed that the perceptron could not find a perfect boundary, and the loss never reached zero. These experiments show that the perceptron works best when the data is linearly separable. It’s fast and simple, but has limitations when data is noisy or can’t be split with a straight line."
  }
]